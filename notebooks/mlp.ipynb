{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline \n",
    "\n",
    "See MIT License in \n",
    "https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 [Online.](https://arxiv.org/abs/1611.06455 \"Wang et al. (2017)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting\n",
    "\n",
    "Left align the tables in this Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Additional datasets are available in the UEA & UCT Time Series Classification Repository at http://www.timeseriesclassification.com/. \n",
    "\n",
    "Save additional datasets in deepscent/data. E.g. deepscent/data/Adiac/Adiac_TRAIN.txt\n",
    "\n",
    "### GunPoint dataset\n",
    "The GunPoint dataset is from the [UEA & UCR Time Series \n",
    "Classification Repository](http://www.timeseriesclassification.com/description.php?Dataset=GunPoint \n",
    "\"GunPoint description\"). The data is from one female and one male either drawing and pointing a gun at a target or pointing their finger at a target. The location of their hand was tracked. This data is the time series of the x-axis location.\n",
    "\n",
    "|Training set size |Test set size |Number of classes \n",
    "|:-----------      |:--------     |:----------     \n",
    "|50 |150  |2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP\n",
    "Expected running time for various datasets when using the original training/test split and the original batch_size. Running on a single NVIDIA GeForce GTX 1080 Ti Graphics Card -\n",
    "\n",
    "Dataset   | Training time\n",
    ":-------  | :--------\n",
    "Adiac     | 18 minutes\n",
    "GunPoint  | 11 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "flist = ['private_dog0']  # List dataset directory names. WormsTwoClass Lightning2 Earthquakes GunPoint \n",
    "batch_size = 32 # Set to -1 to use Wang et al settings\n",
    "nb_epochs = 1500\n",
    "k = 10 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 1 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "seed = 35 # seed for initialising random weights in the NN.\n",
    "k_fold_seed = 87\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "\n",
    "# Directories\n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now())\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "# Input directory\n",
    "if 'private' in flist[0]:\n",
    "    fdir = '../data/private_data/private_events_dev' \n",
    "else:\n",
    "    fdir = '../data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "#from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "\n",
    "np.random.seed(22)#813306)\n",
    "      \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = flist[0]\n",
    "data_a = 0\n",
    "data_b = 2\n",
    "x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "plt.plot(x_train[data_a], label='category'+str(y_train[data_a]))\n",
    "plt.plot(x_train[data_b], label='category'+str(y_train[data_b]))\n",
    "plt.legend(loc='upper left', frameon=False)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    print('Running dataset', fname)\n",
    "    if batch_size == -1:\n",
    "        batch = int(min(x_train.shape[0]/10, 16)) # Wang et al. setting.\n",
    "    else:\n",
    "        batch=batch_size \n",
    "    \n",
    "    nb_classes =len(np.unique(y_test))\n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "    \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "     \n",
    "    # Build model\n",
    "    x = Input(x_train.shape[1:])\n",
    "    y= Dropout(0.1,name='Drop010')(x)\n",
    "    y = Dense(500, activation='relu', name='Dense010')(x)\n",
    "    y = Dropout(0.2,name='Drop020')(y)\n",
    "    y = Dense(500, activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(0.2,name='Drop030')(y)\n",
    "    y = Dense(500, activation = 'relu', name='Dense030')(y)\n",
    "    y = Dropout(0.3,name='Drop040')(y)\n",
    "    out = Dense(nb_classes, activation='softmax', name='Out')(y)\n",
    "    model = Model(x, out)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adadelta(rho=0.95, epsilon=1e-8)  \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "     \n",
    "    reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                      patience=200, min_lr=0.1)\n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        callbacks = [reduce_lr, keras.callbacks.TensorBoard(log_dir=tensorboard_dir+'/'+fname+'_'+label, histogram_freq=1)]\n",
    "\n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    \n",
    "    log = pd.DataFrame(hist.history)   \n",
    "    # Print results. Print the testing results which has the lowest training loss.\n",
    "    print('Training complete on', fname)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training time ', end-start, 'seconds, which is about', duration_minutes, 'minutes.')    \n",
    "    print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "    idx = log['loss'].idxmin()\n",
    "    loss = log.loc[log['loss'].idxmin]['loss']\n",
    "    val_acc = log.loc[log['loss'].idxmin]['val_acc']\n",
    "    print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(idx+1), ')')\n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(idx)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(idx)+','+ duration_minutes \n",
    "    # Save summary file and log file.\n",
    "    pathlib.Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    with open(logs_dir+'/'+fname+'/summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    return summary, model\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "\n",
    "\n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)\n",
    "\n",
    "\n",
    "#with tf.Graph().as_default():\n",
    "#    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completed at\n",
    "'{:%Y-%m-%dT%H:%M}'.format(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recorded results\n",
    "## Adiac\n",
    "\n",
    "|Run |Loss |Accuracy | Comment\n",
    "|:---|:--- |:---     |:----------\n",
    "|1   | 0.005222544357037315  | 0.6930946294608933 |Adadelta - default values\n",
    "|2   | 0.0018498263113997382 | 0.7109974427601261 |\n",
    "|3   | 0.001620212956988372  | 0.7212276216358176 |Adadelta parameters match Wang (2017)\n",
    "|4   | 0.0023210097823697976 | 0.7289002559069172  |\n",
    "|5   | 0.001681140968559283   | 0.7212276216358176  |\n",
    "\n",
    "## GunPoint\n",
    "\n",
    "|Run |Loss |Accuracy | Comment     | Timestamp\n",
    "|:---|:--- |:---     |:----------  |:--------\n",
    "|1   | 1.466274483163943e-07  | 0.9400000035762787 |Adadelta parameters match Wang (2017)\n",
    "|2   | 1.370907000364241e-07  | 0.9333333363135655  |\n",
    "|3   | 1.3589860969887013e-07 | 0.9333333373069763  |\n",
    "|4   | 1.3351442333942033e-07   | 0.9333333363135655  |\n",
    "|5   | 1.4781954007503373e-07   | 0.9333333363135655  |\n",
    "|6   | 1.4781953865394827e-07   | 0.940000002582868  |\n",
    "|7   | 1.4305116877721957e-07   | 0.940000002582868  |\n",
    "|8   | 1.239776764805356e-07   | 0.946666669845581  |\n",
    "|9   | 1.311302369799705e-07   | 0.9333333363135655  | Changed random seed from 813306 to 22\n",
    "|10  | 1.466274483163943e-07   | 0.9333333363135655  | Added kernel_initializer=RandomUniform(seed=seed) with seed=35\n",
    "|11   | 7.161587154865266   | 0.49333334614833196  | Changed from Adadelta optimiser to Adam(lr=0.1, epsilon=1e-8)\n",
    "|12   | 1.2874604919943523e-07   | 0.9266666700442632  |Adam(default params)| 2018-11-18T12:41   \n",
    "|13   | 1.5139580966661015e-07   | 0.9333333363135655  | . | 2018-11-18T15:28  \n",
    "|14   | 1.3470651367697427e-07   | 0.9333333363135655  |  .| 2018-11-23T12:37   |\n",
    "|15  |2.695351213333197e-06  |0.9399999976158142 |Batch size 128 - 1mins  | 2018-12-02T13:07 |\n",
    "|16  |2.858659172488842e-06  |0.9466666642824809 |Batch size 128 - 1mins  | 2018-12-02T13:11 |\n",
    "|17  |2.6953459837386617e-06  |0.9466666642824809 |Sequential instead of Model API. Batch size 64 - 2mins  | 2018-12-02T19:21|\n",
    "\n",
    "### K-fold cross validation\n",
    "|Run |Loss |Accuracy | Comment     | Timestamp\n",
    "|:---|:--- |:---     |:----------  |:--------\n",
    "|1.0  | 2.058348101741103e-07   | 1.0  |  .| 2018-11-23T12:51   |\n",
    "|1.1   | 2.7934738824342274e-07   | 0.9400000035762787  |  .| 2018-11-23T12:51   |\n",
    "|1.2   | 2.1417950222257786e-07   | 0.9200000047683716  |  .| 2018-11-23T12:51   |\n",
    "|1.3   | 1.9788750762472774e-07   | 0.9800000011920929  |  .| 2018-11-23T12:51   |\n",
    "\n",
    "|Run |Loss |Accuracy | Dataset     | Timestamp | Duration\n",
    "|:---|:--- |:---     |:----------  |:--------  |:-------------\n",
    "|0  |2.5153182861004095e-07  |1.0 |GunPoint  |2018-11-23T13:45  |7mins  |\n",
    "|1  |2.415976368297379e-07  |0.9600000023841858 |GunPoint  |2018-11-23T13:45  |7mins  |\n",
    "|2  |1.8239028776179111e-07  |0.9400000035762787 |GunPoint  |2018-11-23T13:45  |7mins  |\n",
    "|3  |2.467634772074234e-07  |0.9800000011920929 |GunPoint  |2018-11-23T13:45  |7mins  |\n",
    "\n",
    "|Run |Loss |Accuracy | Dataset     | Timestamp | Duration\n",
    "|:---|:--- |:---     |:----------  |:--------  |:-------------\n",
    "|0  |2.079541167808606e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|1  |2.076229498647485e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|2  |2.1060326010532057e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|3  |1.6821761650792017e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|4  |2.0232486279938812e-07  |0.95 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|5  |2.457037595604561e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|6  |1.8775471220225073e-07  |0.9 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|7  |2.1126546982941364e-07  |0.9 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|8  |1.8775469325444444e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "|9  |2.4570393135389974e-07  |1.0 |GunPoint  |2018-11-23T14:38  |8mins  |\n",
    "\n",
    "\n",
    "\n",
    "#### GunPoint 2018-12-02T13:15\n",
    "Batch size = 64\n",
    "\n",
    "|Run |Loss |Accuracy | Epoch index     | Duration\n",
    "|:---|:--- |:---     |:----------      |:-------------\n",
    "|0  |1.5788874268057246e-06  |1.0 |4637 |2mins  |\n",
    "|1  |1.2871531074173011e-06  |1.0 |3602 |2mins  |\n",
    "|2  |1.1083247272836161e-06  |1.0 |4959 |2mins  |\n",
    "|3  |1.523910724346125e-06  |0.949999988079071 |4749 |2mins  |\n",
    "|4  |2.1378607546769975e-06  |0.949999988079071 |3957 |2mins  |\n",
    "|5  |1.4994056679521842e-06  |1.0 |3945 |2mins  |\n",
    "|6  |1.6245767508533188e-06  |0.949999988079071 |4641 |2mins  |\n",
    "|7  |1.655717910075004e-06  |0.8999999761581421 |4676 |2mins  |\n",
    "|8  |1.1629627541272687e-06  |1.0 |4574 |2mins  |\n",
    "|9  |1.6093625062138825e-06  |1.0 |3838 |2mins  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_log = pd.read_csv(logs_dir+'/'+fname+'/history_0.csv')\n",
    "read_loss = read_log.loc[read_log['loss'].idxmin]['loss']\n",
    "read_val_acc = read_log.loc[read_log['loss'].idxmin]['val_acc']\n",
    "print('Read from file', logs_dir+'/'+fname+'/history_0.csv', '- loss and val_acc are')\n",
    "print(read_loss, read_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../logs/2019-03-16T09:23/private_dog0_correct/summary.csv'\n",
    "data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "accuracy = data.iloc[:,2]\n",
    "print(data.describe())\n",
    "print('Accuracy mean and 95% confidence level is', accuracy.mean(), accuracy.std()*1.96)\n",
    "print('95% confidence interval is', accuracy.quantile(0.0025), 'to', accuracy.quantile(0.975))\n",
    "plt.figure(0)\n",
    "data.boxplot(column='loss')\n",
    "plt.figure(1)\n",
    "data.boxplot(column='val_acc')\n",
    "data.hist(column='val_acc')\n",
    "print('Rows with val_acc<.94 are')\n",
    "data[data.val_acc<0.94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix'):\n",
    "    # Plot the normalised confusion matrix\n",
    "    # Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    # 'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use trained model (after all epochs) to make predictions\n",
    "x_input = x_test\n",
    "y_input = y_test\n",
    "y_input = y_input - y_input.min()\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_input = (x_input - x_train_mean)/(x_train_std)\n",
    "nb_classes = len(np.unique(y_input))\n",
    "y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "# Calculate model prediction\n",
    "y_probs = model.predict_on_batch(x_input)\n",
    "y_class = y_probs.argmax(axis=1)\n",
    "cm = confusion_matrix(y_input, y_probs.argmax(axis=1), labels=[1,0])\n",
    "acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "print('Pred', y_class[:20])\n",
    "print('True', y_input[:20].astype(int))\n",
    "print(cm)\n",
    "print('Calculated accuracy:',acc_calc) \n",
    "print('Normalised confusion matrix:\\n', cm_norm)\n",
    "title = 'Normalised confusion matrix'\n",
    "plot_confusion_matrix(cm_norm, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K.clear_session()\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
