{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DevNet\n",
    "Train and test the models used in developing Deepscent.\n",
    "\n",
    "\n",
    "MIT license to use [software by Zhiguang Wang](https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline/blob/master/README.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model with hyperparameters as per Wang et al. (2017): \n",
    "# MLP, FCN, ResNet\n",
    "# or\n",
    "# select a model with hyperparameters tuned to optimise performance on the ACI detection dogs dataset:\n",
    "# MLP_tuned, FCN_tuned, ResNet_tuned, CNN\n",
    "model_type = 'MLP_tuned' # MLP, MLP_tuned, FCN, FCN_tuned, CNN, ResNet, ResNet_tuned\n",
    "\n",
    "# Provide the dataset directory name. Select 'GunPoint' to use a dataset from the UCR TSC Archive.\n",
    "flist = ['private_dog0_correct_plus'] # private_balanced, GunPoint\n",
    "\n",
    "batch_size = 32 \n",
    "k = 2 # k-fold cross validation: number of folds. If k=1, the original test-train split is used.\n",
    "m = 1 # k-fold cross validation: number of repetitions (if k>1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_dict = {'MLP':5000, 'FCN':2000, 'ResNet':1500, 'MLP_tuned':500, 'FCN_tuned':1000, 'CNN':1000, 'ResNet_tuned':1000}\n",
    "nb_epochs = epochs_dict[model_type]\n",
    "\n",
    "do_end_test = True    # For each fold, evaluate model on the end_test set too.\n",
    "truncate_data = False # Truncate pressure samples to first n data points\n",
    "filter_data = False # Filter out noise below a threshold\n",
    "\n",
    "data_augmentation = False\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "k_fold_seed = 765432\n",
    "\n",
    "# Output directories\n",
    "logs_dir = '../../logs'\n",
    "tensorboard_dir = '../../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\")))\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "# Input directory\n",
    "if 'private' in flist[0]:\n",
    "    fdir = '../../data/private_data/private_events_dev2' \n",
    "else:\n",
    "    fdir = '../../data' \n",
    "    \n",
    "if not 'correct_plus' in flist[0]:\n",
    "    do_end_test = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix', name=''):\n",
    "    ''' Plot the normalised confusion matrix\n",
    "    Parameters\n",
    "    cm : array - normalised confusion matrix\n",
    "    Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    '''\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    sns.set_style('dark')\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(format=FuncFormatter('{0:.0%}'.format))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.clim(0, 1)\n",
    "    fmt = '.0%'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    file_name = 'cm_devnet_'+name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "def plot_roc(y_true, y_probs, name): \n",
    "    ''' Plot ROC and return AUC\n",
    "    Parameters\n",
    "    y_true : vector of true class labels\n",
    "    y_probs : vector of predicted probabilities\n",
    "    Returns\n",
    "    auc : float\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "    auc = roc_auc_score(y_true, y_probs)\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    file_name = 'roc_devnet_'+name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    return auc\n",
    "\n",
    "\n",
    "def filter_out(x, threshold):\n",
    "    ''' Filter out any data points in x that are below the threshold by setting them to zero.\n",
    "    Return the modified data, x '''\n",
    "    if x < threshold:\n",
    "        return 0\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def preprocess(X):\n",
    "    ''' Apply preprocessing to the input data X'''\n",
    "    if filter_data:\n",
    "        threshold = 0.1\n",
    "        X = np.piecewise(X, [X < threshold, X >= threshold], [lambda X: 0, lambda X: X])\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    if truncate_data:\n",
    "        X = X[:,:300]\n",
    "    X = preprocess(X)\n",
    "    return X, Y\n",
    "   \n",
    "\n",
    "def reshape(x, model_type):\n",
    "    ''' Reshape data into input format for the selected DNN '''\n",
    "    if model_type == 'ResNet':\n",
    "        return reshape_2d(x)\n",
    "    elif model_type == 'FCN' or model_type == 'FCN_tuned' or model_type == 'CNN' or model_type == 'ResNet_tuned':\n",
    "        return reshape_1d(x)\n",
    "    elif model_type == 'MLP' or model_type == 'MLP_tuned':\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unrecognised model type')\n",
    "    return x\n",
    "\n",
    "\n",
    "def augment_data(x, y):\n",
    "    ''' Return n times as many data samples, x, and labels, y. The augmented data is generated \n",
    "    by applying a shift to each row of x and appending these new rows to x '''\n",
    "    m = x.shape[1]\n",
    "    x_new = x\n",
    "    y_new = y\n",
    "    for shift in range(-50, 60, 10):\n",
    "        x_aug = np.zeros_like(x)\n",
    "        if shift < 0:\n",
    "            x_aug[:,:m+shift] = x[:,-shift:]\n",
    "        elif shift > 0:\n",
    "            x_aug[:,shift:] = x[:,:m-shift]\n",
    "        elif shift == 0:\n",
    "            continue\n",
    "        x_new = np.concatenate((x_new, x_aug), axis=0)\n",
    "        y_new = np.concatenate((y_new, y), axis=0)\n",
    "    return x_new, y_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DNN\n",
    "Build a binary classifier. Model types: MLP, FCN, ResNet, CNN.\n",
    "## ResNet\n",
    "ResNet with hyperparameters as per Wang et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_2d(x):\n",
    "    ''' Reshape data into input format for ResNet '''\n",
    "    x = x.reshape(x.shape + (1,1,))\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_resnet(input_shape, nb_classes):\n",
    "    ''' Build ResNet DNN and return input and output tensors '''\n",
    "    # Parameters\n",
    "    k = 1 # kernel multiplier\n",
    "    n_feature_maps = 128\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8*k, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1*k, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8*k, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "         \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1*k, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8*k, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1*k, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet tuned\n",
    "ResNet with hyperparameters tuned to optimise performance on the ACI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet_tuned(input_shape, num_features0, num_features1, filter_size, pooling_size, dropout):\n",
    "    ''' Return ResNet model '''\n",
    "    nb_classes = 2\n",
    "    print(input_shape, num_features0, num_features1, filter_size, pooling_size, dropout)\n",
    "    \n",
    "    # Preparation block\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(x)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = keras.layers.MaxPooling1D(pooling_size)(conv)\n",
    "    \n",
    "    # First block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Second block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Third block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Output block\n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv)\n",
    "    y = Dropout(dropout, name='Dropout')(full)\n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN\n",
    "With hyperparameters as per Wang et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_1d(x):\n",
    "    ''' Reshape data into input format for FCN or CNN'''\n",
    "    x = x.reshape(x.shape + (1,))\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def build_fcn(input_shape, nb_classes):\n",
    "    ''' Build Fully Convolutional Network (FCN) and return input and output tensors '''\n",
    "    # Parameters\n",
    "    k = 1 # kernel multiplier\n",
    "    n_feature_maps = 128\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    #conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps, 8*k, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    conv_z = Activation('relu')(conv_z)\n",
    "    \n",
    "    #print ('build conv_za')\n",
    "    #conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_z)\n",
    "    #conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    #conv_z = Activation('relu')(conv_z)\n",
    "    \n",
    "    #print ('build conv_zb')\n",
    "    #conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_z)\n",
    "    #conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    #conv_z = Activation('relu')(conv_z)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv_z)\n",
    "    #full = Dense(128, activation='relu')(full)\n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN tuned\n",
    "With hyperparameters tuned to optimise performance on the ACI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn_tuned(input_shape, nb_classes):\n",
    "    ''' Build Fully Convolutional Network (FCN) and return input and output tensors '''\n",
    "    # Parameters\n",
    "    feature_maps_a = 32\n",
    "    feature_maps_b = 64\n",
    "    feature_maps_c = 32\n",
    "    filter_a = 4\n",
    "    filter_b = 4\n",
    "    filter_c = 4\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    conv_x = keras.layers.Conv1D(feature_maps_a, filter_a, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(feature_maps_b, filter_b, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(feature_maps_c, filter_c, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    conv_z = Activation('relu')(conv_z)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv_z)\n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "With hyperparameters tuned to optimise performance on the ACI dataset.\n",
    "\n",
    "Using the CNN architecture of\n",
    "[Ackermann, Nils, 2018, Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_harus(input_shape, nb_classes):\n",
    "    ''' Build a CNN and return input and output tensors '''\n",
    "    # Parameters\n",
    "    n_features_a = 128 # Ackermann 100 \n",
    "    n_features_b = 128 # Ackermann 160\n",
    "    filter_size = 16   # Ackermann 10\n",
    "    pooling_size = 16   # Ackermann 3\n",
    "    dropout = 0.7      # Ackermann 0.5\n",
    "    has_dense_layer = True\n",
    "    \n",
    "    print ('build CNN HARUS')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    conv_x = keras.layers.Conv1D(n_features_a, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_a, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.MaxPooling1D(pooling_size)(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_b, filter_size, activation='relu')(conv_x)\n",
    "    \n",
    "    if True:\n",
    "        conv_x = keras.layers.Conv1D(n_features_b, filter_size, activation='relu')(conv_x)\n",
    "    else:\n",
    "        conv_x = keras.layers.Conv1D(n_features_b, filter_size)(conv_x)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = Activation('relu')(conv_x)\n",
    "        \n",
    "    if has_dense_layer: # End with a fully connected layer\n",
    "        full = keras.layers.GlobalAveragePooling1D()(conv_x)\n",
    "        full = Dropout(dropout,name='Dropout')(full)\n",
    "        out = Dense(1, activation='sigmoid')(full)\n",
    "    else:\n",
    "        conv_x = keras.layers.Conv1D(16, filter_size, activation='relu')(conv_x)\n",
    "        conv_x = Dropout(dropout,name='Dropout')(conv_x)\n",
    "        conv_x = keras.layers.Conv1D(1, filter_size, activation='relu')(conv_x)\n",
    "        conv_x = keras.layers.GlobalAveragePooling1D()(conv_x)\n",
    "        out = Activation(activation='sigmoid')(conv_x)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "With hyperparameters as per Wang et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, nb_classes):\n",
    "    num = 500\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dropout(0.1,name='WDrop010')(x)\n",
    "    y = Dense(num, activation='relu', name='WDense010')(y)\n",
    "    y = Dropout(0.2,name='WDrop020')(y)\n",
    "    y = Dense(num, activation='relu', name='WDense020')(y)\n",
    "    y = Dropout(0.2,name='WDrop021')(y)\n",
    "    y = Dense(num, activation='relu', name='WDense021')(y)\n",
    "    y = Dropout(0.3,name='WDrop031')(y)\n",
    "    out = Dense(1, activation='sigmoid', name='WDense080')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP tuned\n",
    "With hyperparameters tuned to optimise performance on the ACI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_tuned(input_shape, nb_classes):\n",
    "    ''' Build a Multilayer Perceptron (MLP) and return input and output tensors '''\n",
    "    drop = 0.2\n",
    "    num = 16\n",
    "    l2 = 0.1\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dropout(drop,name='DropInput')(x)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense010')(y)\n",
    "    y = Dropout(drop,name='Drop010')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(drop,name='Drop020')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense030')(y)\n",
    "    y = Dropout(drop,name='Drop030')(y)\n",
    "    out = Dense(1, activation='sigmoid', name='DenseOutput')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    ''' Build and train a DNN. Return summary info and a trained model '''\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "    if nb_classes != 2:\n",
    "        raise 'Number of classes must be 2 to use this binary classifier'\n",
    "    \n",
    "    if data_augmentation:\n",
    "        x_train, y_train = augment_data(x_train, y_train)\n",
    "     \n",
    "    Y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    Y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "     \n",
    "    x_train = reshape(x_train, model_type)\n",
    "    x_test = reshape(x_test, model_type)\n",
    "    if model_type == 'MLP':\n",
    "        x, y = build_mlp(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'MLP_tuned':\n",
    "        x, y = build_mlp_tuned(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'ResNet':\n",
    "            x, y = build_resnet(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'ResNet_tuned':\n",
    "        num_features0 = 64\n",
    "        num_features1 = 128\n",
    "        filter_size = 4\n",
    "        pooling_size = 8\n",
    "        dropout = 0.5\n",
    "        x, y = build_resnet_tuned(x_train.shape[1:], num_features0, num_features1, filter_size, pooling_size, dropout)\n",
    "    elif model_type == 'FCN':\n",
    "        x, y = build_fcn(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'FCN_tuned':\n",
    "        x, y = build_fcn_tuned(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'CNN':\n",
    "        x, y = build_cnn_harus(x_train.shape[1:], nb_classes)\n",
    "    model = Model(x, y)\n",
    "    print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "  \n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    \n",
    "    # Print results\n",
    "    duration_seconds = round(end-start)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training complete on', fname, 'Duration:', duration_seconds, 'secs; about', duration_minutes, 'minutes.')\n",
    "    \n",
    "    # Print and save results. Print the testing results which has the lowest training loss.\n",
    "    print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "    idx = log['loss'].idxmin()\n",
    "    loss = log.loc[idx]['loss']\n",
    "    val_acc = log.loc[idx]['val_acc']\n",
    "    epoch = idx + 1\n",
    "    print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(epoch), ')')\n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(epoch)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(epoch)+','+ duration_minutes \n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir) \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    model_params = {'x_train_mean':x_train_mean, 'x_train_std':x_train_std}\n",
    "    return summary, summary_csv, model, model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train a model, using repeated k-fold cross validation, if selected '''\n",
    "\n",
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    if do_end_test:\n",
    "        x_other, y_other = readucr(fdir+'/'+fname+'/'+fname+'_END_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all, y_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, summary_csv, model, model_params = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            if do_end_test:\n",
    "                x_in = (x_other - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "                x_in = reshape(x_in, model_type)\n",
    "                _, end_test_acc = model.evaluate(x_in, y_other, batch_size=batch_size)\n",
    "                summary = summary + str(end_test_acc) +' |'\n",
    "                summary_csv = summary_csv + ',' + str(end_test_acc)\n",
    "            with open(logs_dir+'/'+fname+'/devnet_summary.csv', 'a+') as f:\n",
    "                f.write(summary_csv)\n",
    "                f.write('\\n')\n",
    "                print('Added summary row to ', logs_dir+'/'+fname+'/devnet_summary.csv')\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, summary_csv, model, model_params = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        if do_end_test:\n",
    "            x_in = (x_other - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "            x_in = reshape(x_in, model_type)\n",
    "            _, end_test_acc = model.evaluate(x_in, y_other, batch_size=batch_size)\n",
    "            summary = summary + str(end_test_acc) +' |'\n",
    "            summary_csv = summary_csv + ',' + str(end_test_acc)\n",
    "        with open(logs_dir+'/'+fname+'/devnet_summary.csv', 'a+') as f:\n",
    "            f.write(summary_csv)\n",
    "            f.write('\\n')\n",
    "            print('Added summary row to ', logs_dir+'/'+fname+'/devnet_summary.csv')\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print when done\n",
    "print('Done at:' , '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file =  logs_dir+'/'+fname+'/devnet_summary.csv'\n",
    "\n",
    "print(do_end_test)\n",
    "if do_end_test:\n",
    "    data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time', 'end_test_acc'])\n",
    "else:\n",
    "    data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "    \n",
    "accuracy = data['val_acc']\n",
    "print(file)\n",
    "print('Accuracy mean, sample std dev and 95% confidence level is', accuracy.mean(), accuracy.std(), accuracy.std()*2.262)\n",
    "print('95% quantile interval is', accuracy.quantile(0.0025), 'to', accuracy.quantile(0.975))\n",
    "data.boxplot(column=['val_acc'], whis=[2.5,97.5])\n",
    "\n",
    "if do_end_test:\n",
    "    result_set = {'acc_mean':accuracy.mean(), 'std':accuracy.std(), \n",
    "                  'lower':accuracy.quantile(0.0025), 'upper':accuracy.quantile(0.975),\n",
    "                 'end_test_acc_av':data['end_test_acc'].mean(),\n",
    "                 'end_test_acc_std':data['end_test_acc'].std()}\n",
    "else:\n",
    "    result_set = {'acc_mean':accuracy.mean(), 'std':accuracy.std(), \n",
    "              'lower':accuracy.quantile(0.0025), 'upper':accuracy.quantile(0.975)}\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Use trained model (after all epochs) to make predictions '''\n",
    "\n",
    "def predictions(model, model_params, model_type, \n",
    "                x_input, y_input, name, threshold=0.5):\n",
    "    ''' Use the model to make predictions on x_input data. Return the predictions and the calculated accuracy. '''    \n",
    "    do_print = True\n",
    "    y_input = y_input - y_input.min()\n",
    "    x_input = (x_input - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "    x_input = reshape(x_input, model_type)\n",
    "    nb_classes = len(np.unique(y_input))\n",
    "    y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "    # Class balance\n",
    "    n0 = (y_input == 0).sum()\n",
    "    n1 = (y_input == 1).sum()\n",
    "    \n",
    "    # Calculate model prediction\n",
    "    y_probs = model.predict_on_batch(x_input)\n",
    "    if threshold == 0.5:\n",
    "        y_pred = np.round(y_probs).flatten()\n",
    "    else:\n",
    "        y_pred = y_probs.flatten()\n",
    "        y_pred[y_pred > threshold] = 1\n",
    "        y_pred[y_pred <= threshold] = 0\n",
    "        \n",
    "    cm = confusion_matrix(y_input, y_pred, labels=[1,0])\n",
    "    acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    if do_print:\n",
    "        print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "        print('Pred', y_pred[:20])\n",
    "        print('True', y_input[:20].astype(int))\n",
    "        print(cm)\n",
    "        print('Calculated accuracy:',acc_calc)\n",
    "        print('Class balance in test set:', n0, 'to', n1, 'i.e.', n0/(n0+n1))\n",
    "        print('Normalised confusion matrix:\\n', cm_norm)\n",
    "    title = 'Normalised confusion matrix'\n",
    "    plot_confusion_matrix(cm_norm, title=title, name=name)\n",
    "\n",
    "    # ROC and AUC\n",
    "    auc = plot_roc(y_input, y_probs, name=name)\n",
    "    print('AUC:', auc)\n",
    "\n",
    "    report = classification_report(y_input, y_pred)\n",
    "    print('\\n', report)\n",
    "    print('\\nmicro av - averaging the total true positives, false negatives and false positives')\n",
    "    print('macro av - averaging the unweighted mean per label')\n",
    "    print('weighted av - averaging the support-weighted mean per label')\n",
    "    return y_pred, acc_calc\n",
    "\n",
    "y_pred, acc = predictions(model, model_params, model_type, x_test, y_test, fname)\n",
    "result_set['this_model_acc'] = acc\n",
    "\n",
    "# Check results using model.evaluate\n",
    "x_in = (x_test - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "x_in = reshape(x_in, model_type)\n",
    "print('model.evaluate : val_loss, val_acc', model.evaluate(x_in, y_test, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change operating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = fname+'oppoint'\n",
    "threshold = 0.3\n",
    "y_pred, acc = predictions(model, model_params, model_type, x_test, y_test, name, threshold)\n",
    "\n",
    "# Check results using model.evaluate\n",
    "x_in = (x_test - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "x_in = reshape(x_in, model_type)\n",
    "print('model.evaluate : val_loss, val_acc', model.evaluate(x_in, y_test, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(x, y_true, y_pred, title, meta=None):\n",
    "    ''' Plot the data samples, grouped as TP, FN, TN, FP as determined by the \n",
    "    samples true class (y_true) and the predicted class (y_pred) '''\n",
    "    nb_classes = len(np.unique(y_true))\n",
    "    y_true = (y_true - y_true.min())/(y_true.max()-y_true.min())*(nb_classes-1)\n",
    "    if meta is not None:\n",
    "        print(title)\n",
    "    n_plots = 10\n",
    "    fig, ax = plt.subplots(n_plots, 4, sharex='col', sharey='row', figsize=(10, 10))\n",
    "    rows = [0, 0, 0, 0]\n",
    "    green_red = sns.color_palette(\"Paired\")\n",
    "    colors = [green_red[3], green_red[5], green_red[2], green_red[4]]\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_true[i]==1:\n",
    "            if y_pred[i]==1:\n",
    "                col = 0\n",
    "            else:\n",
    "                col = 1\n",
    "                if meta is not None:\n",
    "                    print('FN at ', meta.iloc[i]['filename'], 'sensor', meta.iloc[i]['sensor_number'])\n",
    "        if y_true[i]==0:\n",
    "            if y_pred[i]==0:\n",
    "                col = 2\n",
    "            else:\n",
    "                col = 3\n",
    "                if meta is not None:\n",
    "                    print('FP at ', meta.iloc[i]['filename'], 'sensor', meta.iloc[i]['sensor_number'])\n",
    "        row = rows[col]\n",
    "        rows[col] = rows[col] + 1\n",
    "        if row < n_plots:\n",
    "            ax[row, col].plot(x[i], color=colors[col])\n",
    "            ax[0, col].set_title('True '+str(int(y_true[i]))+': Pred '+str(y_pred[i]))\n",
    "            ax[row, col].set_ylim(bottom=0, top=2.2)\n",
    "    ax[n_plots-1, 0].set_ylabel('x(t)')\n",
    "    ax[n_plots-1, 0].set_xlabel('time, t')\n",
    "    ax[n_plots-1, 1].set_xlabel('time, t')\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig('data_samples_'+title+'.png', bbox_inches='tight')\n",
    "     \n",
    "plot_samples(x_test, y_test, y_pred, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file1 = '../../logs/2019-03-17T12:59/private_dog0_correct/devnet_summary.csv'\n",
    "data1 = pd.read_csv(file1, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "name1 = 'dog0_correct'\n",
    "\n",
    "file = logs_dir+'/'+fname+'/devnet_summary.csv'\n",
    "print('Showing results from:\\n', file1, 'and\\n', file)\n",
    "data2 = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time', 'end_test_acc'])\n",
    "name2 = 'this_run'\n",
    "print(data2)\n",
    "\n",
    "\n",
    "all_data = [data1['val_acc'], data2['val_acc']]\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(data=all_data)\n",
    "ax = sns.swarmplot(data=all_data, color='black')\n",
    "ax.set_xlabel('DevNet')\n",
    "ax.set_ylabel('validation accuracy')\n",
    "ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "plt.xticks([0, 1], [name1, name2])\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplot_devnet.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_end_test:\n",
    "    other = fname+'_END_TEST' #_dog_incorrect' # 'private_dog0_correct_plus_END_TEST'\n",
    "else:\n",
    "    other = fname+'_TEST'\n",
    "datadir = fdir+'/'+fname\n",
    "print('Testing on:', datadir+'/'+other+'.txt')\n",
    "x_other, y_other = readucr(datadir+'/'+other+'.txt')\n",
    "y_other_pred, other_acc = predictions(\n",
    "    model, model_params, model_type, \n",
    "    x_other, y_other, other)\n",
    "# Get dog result\n",
    "meta = pd.read_csv(datadir+'/'+other+'_meta.txt', sep=',', parse_dates=['date'])\n",
    "cm = confusion_matrix(y_other, meta['dog_pred'], labels=[1,0])\n",
    "print('Dog cm \\n', cm)\n",
    "dog_acc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plot_confusion_matrix(cm_norm, title='Dog indications', name='dog_pred')\n",
    "print('True', y_other[:20])\n",
    "print('Dog ', meta['dog_pred'].values[:20])\n",
    "print('Dog accuracy', dog_acc)\n",
    "\n",
    "# Plot data\n",
    "plot_samples(x_other, y_other, y_other_pred, 'DNN predictions', meta)\n",
    "plot_samples(x_other, y_other, meta['dog_pred'], 'Dog indications', meta)\n",
    "    \n",
    "result_set['this_end_test_acc'] = other_acc  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the operating point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "name = fname+'_END_TEST_threshold_'+str(threshold)\n",
    "y_roc_pred, other_acc = predictions(\n",
    "    model, model_params, model_type, \n",
    "    x_other, y_other, name, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the difference in dog and DNN predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_differences(x, y_pred, meta):\n",
    "    ''' Plot the data samples, x, where the DNN prediction (y_pred) differs from\n",
    "    the dogs indication (in meta) '''\n",
    "    # Concatenate all data\n",
    "    y_diff = abs(meta['dog_pred'].values-y_pred.T)\n",
    "    y_diff_df = pd.DataFrame(y_diff, columns=['y_diff'])\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "    x_df = pd.DataFrame(x)\n",
    "    data_meta = pd.concat([y_pred_df, y_diff_df, meta], axis=1)\n",
    "    meta_header = list(data_meta)\n",
    "    data_meta = pd.concat([data_meta, x_df], axis=1)\n",
    "    \n",
    "    # Sort the data\n",
    "    data_meta = data_meta.sort_values(['class', 'y_diff', 'dog_result'])\n",
    "    class0 = data_meta[data_meta['class']==0]\n",
    "    class0 = class0.sort_values(['y_diff', 'dog_result'], ascending=[False, False])\n",
    "    class1 = data_meta[data_meta['class']==1]\n",
    "    class1 = class1.sort_values(['y_diff', 'dog_result'], ascending=[False, True])\n",
    "\n",
    "    # Plot the data where dog and DNN did not agree\n",
    "    for this_class in [class1, class0]:\n",
    "        # Get x data\n",
    "        this_x = this_class[this_class.columns.difference(meta_header)]\n",
    "        assert this_x.shape[1] == 1000\n",
    "        # Count plots required\n",
    "        n = 0\n",
    "        for i in range(len(this_class)):\n",
    "            if this_class.iloc[i]['y_pred'] != this_class.iloc[i]['dog_pred']:\n",
    "                n = n + 1\n",
    "        # Create the plots\n",
    "        fig, ax = plt.subplots(n, 3, sharex='col', sharey='row', figsize=(10, 4.8), squeeze=False)\n",
    "        class_label = this_class.iloc[0]['class']\n",
    "        row = 0\n",
    "        for i in range(len(this_class)):\n",
    "            if row < n:\n",
    "                dog_correct = this_class.iloc[i]['dog_pred'] == this_class.iloc[i]['class']\n",
    "                dnn_correct = this_class.iloc[i]['y_pred'] == this_class.iloc[i]['class']\n",
    "                dog_color = 'green' if dog_correct else 'red'\n",
    "                dnn_color = 'green' if dnn_correct else 'red'  \n",
    "                if this_class.iloc[i]['y_pred'] != this_class.iloc[i]['dog_pred']:\n",
    "                    ax[row, 0].plot(this_x.iloc[i], color=dog_color)\n",
    "                    ax[row, 1].plot(this_x.iloc[i], color=dnn_color)\n",
    "                    ax[row, 0].set_ylim(bottom=0, top=2.2)\n",
    "                    ax[row, 1].set_ylim(bottom=0, top=2.2)\n",
    "                    file = this_class.iloc[i]['filename']\n",
    "                    sensor = str(this_class.iloc[i]['sensor_number'])\n",
    "                    ax[row, 2].text(0, 0.65, str(row+1)+') '+file+' sensor '+sensor)\n",
    "                    row = row + 1\n",
    "        ax[0, 0].set_title('Dog')\n",
    "        ax[0, 1].set_title('DNN')\n",
    "        ax[n-1, 0].set_ylabel('x(t)')\n",
    "        ax[n-1, 0].set_xlabel('time, t')\n",
    "        ax[n-1, 1].set_xlabel('time, t')\n",
    "        ax[n-1, 2].set_xticklabels([])\n",
    "        fig.suptitle('True class: '+str(class_label)+'    green=correct, red=incorrect')\n",
    "        plt.savefig('DogDNN_diffs_class' + str(class_label) + '_'+fname+'.png', bbox_inches='tight')\n",
    "\n",
    "\n",
    "# Operating point not set\n",
    "plot_differences(x_other, y_other_pred, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With operating point threshold set\n",
    "plot_differences(x_other, y_roc_pred, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_dnn_valueadd(x, y_pred, meta):\n",
    "    ''' Plot the data samples, x, where the DNN prediction (y_pred) is \n",
    "    correct and the dogs indication was incorrect '''\n",
    "    # Concatenate all data\n",
    "    y_diff = abs(meta['dog_pred'].values-y_pred.T)\n",
    "    y_diff_df = pd.DataFrame(y_diff, columns=['y_diff'])\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "    x_df = pd.DataFrame(x)\n",
    "    data_meta = pd.concat([y_pred_df, y_diff_df, meta], axis=1)\n",
    "    meta_header = list(data_meta)\n",
    "    data_meta = pd.concat([data_meta, x_df], axis=1)\n",
    "    \n",
    "    # Sort the data\n",
    "    data_meta = data_meta.sort_values(['class', 'y_diff', 'dog_result'])\n",
    "    class0 = data_meta[data_meta['class']==0]\n",
    "    class0 = class0.sort_values(['y_diff', 'dog_result'], ascending=[False, False])\n",
    "    class1 = data_meta[data_meta['class']==1]\n",
    "    class1 = class1.sort_values(['y_diff', 'dog_result'], ascending=[False, True])\n",
    "\n",
    "    # Plot the data where dog and DNN did not agree\n",
    "    for this_class in [class1, class0]:\n",
    "        # Get x data\n",
    "        this_x = this_class[this_class.columns.difference(meta_header)]\n",
    "        assert this_x.shape[1] == 1000\n",
    "        # Count plots required\n",
    "        n = this_class[(this_class['y_pred'] == this_class['class']) & (this_class['y_pred'] != this_class['dog_pred'])].shape[0]\n",
    "        # Create the plots\n",
    "        fig, ax = plt.subplots(n, 1, sharex='col', sharey='row')\n",
    "        class_label = this_class.iloc[0]['class']\n",
    "        row = 0\n",
    "        for i in range(len(this_class)):\n",
    "            if row < n:\n",
    "                this_row = ax[row] if n>1 else ax\n",
    "                true_class = this_class.iloc[i]['class']\n",
    "                dog_class = int(this_class.iloc[i]['dog_pred'])\n",
    "                dnn_class = int(this_class.iloc[i]['y_pred'])\n",
    "                dog_correct = dog_class == true_class\n",
    "                dnn_correct = dnn_class == true_class\n",
    "                dog_color = 'green' if dog_correct else 'red'\n",
    "                dnn_color = 'green' if dnn_correct else 'red'  \n",
    "                if dnn_correct and not dog_correct:\n",
    "                    this_row.plot(this_x.iloc[i], color=dnn_color)\n",
    "                    this_row.set_ylim(bottom=0, top=2.2)\n",
    "                    row = row + 1\n",
    "                    this_row.set_facecolor('lightcyan' if true_class else 'lightyellow')\n",
    "                    #print(meta_header)\n",
    "                    print('True', true_class, ': DNN', dnn_class, ': dog', dog_class, this_class.iloc[i][['filename', 'sensor_number']])\n",
    "        this_row = ax[0] if n>1 else ax\n",
    "        print('n is', n)\n",
    "        this_row.set_title('True '+str(true_class)+' : DNN '+str(dnn_class)+' : dog '+str(dog_class))\n",
    "        this_row = ax[n-1] if n>1 else ax\n",
    "        this_row.set_ylabel('x(t)')\n",
    "        this_row.set_xlabel('time, t')\n",
    "    \n",
    "        plt.savefig('DNN_valueadd_class' + str(class_label) + '_'+fname+'.png', bbox_inches='tight')\n",
    "        \n",
    "plot_dnn_valueadd(x_other, y_roc_pred, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_similarities(x, y_true, y_pred, y_dog, title, meta):\n",
    "    ''' Plot the data samples, x, where the DNN prediction (y_pred) matches\n",
    "    the dogs indication (y_dog) '''\n",
    "    # Calculate number of plots required\n",
    "    rows = [0, 0, 0, 0]\n",
    "    for i in range(len(y_pred)):\n",
    "        col = -1\n",
    "        if y_true[i]==1:\n",
    "            if y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 0\n",
    "            elif y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 1\n",
    "        if y_true[i]==0:\n",
    "            if y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 2\n",
    "            elif y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 3\n",
    "        if col != -1:\n",
    "            rows[col] = rows[col]+1\n",
    "    n_plots = max(rows)\n",
    "    \n",
    "    # Set up the subplots\n",
    "    fig, ax = plt.subplots(n_plots, 4, sharex='col', sharey='row', figsize=(10, 10))\n",
    "    rows = [0, 0, 0, 0]\n",
    "    green_red = sns.color_palette(\"Paired\")\n",
    "    colors = [green_red[3], green_red[5], green_red[2], green_red[4]]\n",
    "    meta_lists = [[], [], [], []]\n",
    "    # Create each plot\n",
    "    for i in range(len(y_pred)):\n",
    "        col = -1\n",
    "        if y_true[i]==1:\n",
    "            if y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 0\n",
    "            elif y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 1\n",
    "        if y_true[i]==0:\n",
    "            if y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 2\n",
    "            elif y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 3\n",
    "        if col != -1:\n",
    "            row = rows[col]\n",
    "            rows[col] = rows[col] + 1\n",
    "            if row < n_plots:\n",
    "                ax[row, col].plot(x[i], color=colors[col])\n",
    "                meta_lists[col].append(meta.iloc[i])\n",
    "                ax[0, col].set_title('True '+str(int(y_true[i]))+': Pred '+str(y_pred[i]))\n",
    "                ax[row, col].set_ylim(bottom=0, top=2.2)\n",
    "                ax[row, 0].set_yticklabels([])\n",
    "    \n",
    "    # Add labels and title\n",
    "    for c in range(4):\n",
    "        ax[n_plots-1, c].set_xlabel('time, t')\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig('DogDNN_match_'+fname+'.png', bbox_inches='tight')\n",
    "    # Print meta data\n",
    "    for c in range(4):\n",
    "        print('Meta data for plots in column', c)\n",
    "        for r in meta_lists[c]:\n",
    "            msg = r['filename'] + '\\tsensor ' + str(r['sensor_number'])\n",
    "            if c < 2:\n",
    "                msg = msg + '\\tconcentration ' +str(r['Concentration'])\n",
    "            print(msg)\n",
    "    \n",
    "    \n",
    "plot_similarities(x_other, y_other, y_roc_pred, meta['dog_pred'], \n",
    "                  'Samples where the DNN\\'s prediction matched the dog\\'s indication',\n",
    "                  meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = logs_dir+'/'+fname+'/model'\n",
    "model_json = model.to_json()\n",
    "with open(modelfile+'.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Save the model's weights\n",
    "model.save_weights(modelfile+'.h5')\n",
    "print('Model saved to', modelfile)\n",
    "# Save the other model parameters (mean and std dev of training data)\n",
    "model_params          \n",
    "with open(logs_dir+'/'+fname+'/model_params.csv', 'a+') as f:\n",
    "    for key in model_params.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,model_params[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Results in', file)\n",
    "print(result_set['lower'])\n",
    "print(result_set['upper'])\n",
    "print(result_set['acc_mean'])\n",
    "print(result_set['std'])\n",
    "print(result_set['end_test_acc_av'])\n",
    "print(result_set['end_test_acc_std'])\n",
    "print('(this_model_acc', result_set['this_model_acc'], ')')\n",
    "print(result_set['this_end_test_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License to use Zhiguang Wang's software\n",
    "\n",
    "UCR Time Series Classification Deep Learning Baseline \n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) [2019] [Zhiguang Wang]\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Reference\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online. Available at https://arxiv.org/abs/1611.06455."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
