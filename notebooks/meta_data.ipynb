{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add to meta data\n",
    "The database spreadsheet contains additional information about each sensor sample, namely the positive scent \n",
    "sample's concentration and the dog's response. This notebook add this additonal meta data into the dataset\n",
    "where corresponding rows can be found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs\n",
    "do_save = True\n",
    "file = '../data/private_data/dog_behaviour_database_private_flat.csv'\n",
    "database = pd.read_csv(file, parse_dates=['Date'])\n",
    "# Load sensor data and its meta data\n",
    "name_root = 'private_events_dev_TRAIN_'\n",
    "file_root = '../data/private_data/private_events_dev/'+name_root\n",
    "meta = pd.read_csv(file_root+'_meta.txt', sep=',', parse_dates=['date'])\n",
    "dataset = pd.DataFrame(np.loadtxt(Path(file_root+'.txt')))\n",
    "print(database.head())\n",
    "print(meta.head())\n",
    "# Join meta and sensor data\n",
    "meta_and_data = pd.concat([meta, dataset], axis=1)\n",
    "print('meta:', meta.shape, 'dataset:', dataset.shape, 'meta_and_data:', meta_and_data.shape)\n",
    "print(meta_and_data.head()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates from database. \n",
    "# E.g. 2018-10-03 had two sessions so the run numbers are re-used in the database. \n",
    "# The sensor data is in two folders 'Session1' and 'Session2' but this information \n",
    "# is not available in the meta data.\n",
    "database_u = database.drop_duplicates(subset=['Date', 'DogName', 'Run', 'Pass', 'SensorNumber', 'y_true'], keep=False)\n",
    "print('Database original shape:', database.shape)\n",
    "print('Duplicates removed:', database_u.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some 30 rows in meta data differ only by timestamp. So we won't be able to match them up to a row in\n",
    "# the database. Show them here; they will be removed by doing a merge using an inner join.\n",
    "md_dups = meta_and_data[meta_and_data.duplicated(subset=['date', 'dog', 'run', 'pass', 'sensor_number', 'class'], keep=False)]\n",
    "print('Duplicates shape:', md_dups.shape)\n",
    "print(md_dups.sort_values(by=['date', 'dog', 'run', 'pass', 'sensor_number', 'class']))\n",
    "md_unique = meta_and_data.drop_duplicates(subset=['date', 'dog', 'run', 'pass', 'sensor_number', 'class'], keep=False)\n",
    "print('Meta original shape:', meta_and_data.shape)\n",
    "print('Effective duplicates removed:', md_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge using an inner join and a one-to-one relationship.\n",
    "# Some rows in meta do not have an entry in database now that the duplicates have been removed.\n",
    "md_all = database_u.merge(meta_and_data, how='inner', \n",
    "                           left_on=['Date', 'DogName', 'Run', 'Pass', 'SensorNumber', 'y_true'], \n",
    "                           right_on=['date', 'dog', 'run', 'pass', 'sensor_number', 'class'],\n",
    "                           validate='one_to_many', indicator=False)\n",
    "print(md_all.tail())\n",
    "print('meta original shape:', meta_and_data.shape)\n",
    "print('meta_u original shape:', md_unique.shape)\n",
    "print('md_all shape:', md_all.shape)\n",
    "print(list(md_all)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data back out into sensor data and meta data\n",
    "meta_new = md_all[md_all.columns[:21]]\n",
    "dataset_new = md_all[md_all.columns[21:]]\n",
    "print(list(meta_new))\n",
    "meta_re = meta_new[['filename', 'date', 'time', 'dog', 'run', 'pass', 'positive_position', \n",
    "                    'sensor_number', 'class', 'breakpoint_0', 'breakpoint_1',\n",
    "                   'Concentration', 'IsLastPass', 'y_pred']]\n",
    "print(list(meta_re))\n",
    "meta_re = meta_re.rename(index=str, columns={'y_pred': 'dog_pred'})\n",
    "print(list(meta_re))\n",
    "print('dataset:', dataset.shape)\n",
    "print('md_all:', md_all.shape)\n",
    "print('meta_re:', meta_re.shape)\n",
    "print('dataset_new:', dataset_new.shape)\n",
    "print(dataset_new.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "if do_save:\n",
    "    file = name_root + 'update_meta.txt'\n",
    "    meta_re.to_csv(file, index=False)\n",
    "    file = name_root + 'update.txt'\n",
    "    np.savetxt(file, dataset_new.to_numpy(), fmt='%f', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
