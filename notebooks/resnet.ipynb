{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(813306)\n",
    "\n",
    "# User inputs ##\n",
    "\n",
    "flist = ['GunPoint'] # List dataset directory names.\n",
    "batch_size = 64 # Wang: int(min(x_train.shape[0]/10, 16)) \n",
    "nb_epochs = 1500 # Wang: 1500\n",
    "k = 10 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 10 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "k_fold_seed = 87\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "early_stopping = False # Not yet ready for use # Use early stopping.\n",
    "# Directories\n",
    "fdir = '../data'  \n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now())\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out\n",
    " \n",
    "       \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "      \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "    x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,1,))\n",
    "     \n",
    "     \n",
    "    x , y = build_resnet(x_train.shape[1:], 64, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    pathlib.Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        pathlib.Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "         patience=10) # when tf updates keras, use restore_best_weights instead of ModelCheckPoint\n",
    "        model_save = keras.callbacks.ModelCheckpoint(logs_dir+'/temp.h5', monitor='val_acc', \n",
    "            save_best_only=True, save_weights_only=True)\n",
    "        callbacks.append(early_stop)\n",
    "        callbacks.append(model_save)\n",
    "\n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history)  \n",
    "    \n",
    "    if not early_stopping:\n",
    "        # Print and save results. Print the testing results which has the lowest training loss.\n",
    "        print('Training complete on', fname)\n",
    "        duration_minutes = str(round((end-start)/60))\n",
    "        print('Training time ', end-start, 'seconds, which is about', duration_minutes, 'minutes.')    \n",
    "        print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "        idx = log['loss'].idxmin()\n",
    "        loss = log.loc[idx]['loss']\n",
    "        val_acc = log.loc[idx]['val_acc']\n",
    "        print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(idx+1), ')')\n",
    "        summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(idx)+' |'+ duration_minutes + 'mins  |'\n",
    "        summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(idx)+','+ duration_minutes \n",
    "    \n",
    "    else:\n",
    "        model.load_weights(logs_dir+'/temp.h5')\n",
    "        eval = model.evaluate(metrics=['loss','accuracy'])\n",
    "        print('From model.evaluate()')\n",
    "        print(eval['loss'], eval['accuracy'])\n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    with open(logs_dir+'/'+fname+'/resnet_summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/resnet_summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    return summary, model\n",
    "\n",
    "\n",
    "# main\n",
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "for each in results:\n",
    "    print(each)\n",
    "#print('Example prediction probabilities')\n",
    "#y_predict = model.predict_proba(x_test)            \n",
    "#for yp in y_predict[1:20]:\n",
    "#    print(yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GunPoint 2018-12-22T14:40\n",
    "\n",
    "Batch size = 64\n",
    "\n",
    "|Run |Loss |Accuracy | Epoch index     | Duration\n",
    "|:---|:--- |:---     |:----------      |:-------------\n",
    "|0  |1.2543987577373627e-05  |0.9799999952316284 |1494 |3mins  |\n",
    "|1  |0.00013335476629436016  |0.9699999952316284 |1496 |3mins  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = '../logs/2018-12-22T15:54/GunPoint/resnet_summary.csv'\n",
    "data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "accuracy = data.iloc[:,2]\n",
    "print(data.describe())\n",
    "print('Accuracy mean and 95% confidence level is', accuracy.mean(), accuracy.std()*1.96)\n",
    "print('95% confidence interval is', accuracy.quantile(0.0025), 'to', accuracy.quantile(0.975))\n",
    "plt.figure(0)\n",
    "data.boxplot(column='loss')\n",
    "plt.figure(1)\n",
    "data.boxplot(column='val_acc')\n",
    "data.hist(column='val_acc')\n",
    "print('Rows with val_acc<1 are')\n",
    "data[data.val_acc<0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary results\n",
    "\n",
    "95% confidence :-\n",
    "\n",
    "|Timestamp |Model |Mean val_acc | +/- | lower |  upper  | Comment\n",
    "|:---|:--- |:---  |:---    |:----------      |:------------- |:-------------\n",
    "|2018-12-01T08:43  |MLP  |0.9715 | 0.0727 |0.8500 |1.0  | 10-fold, 10 resamples |\n",
    "|2018-12-22T15:54  |ResNet  |0.9960 | 0.0301 | 0.9124 |1.0  | 10-fold, 10 resamples |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
