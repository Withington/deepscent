{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(813306)\n",
    "\n",
    "# User inputs ##\n",
    "\n",
    "flist = ['private_balanced'] # List dataset directory names. WormsTwoClass Lightning2 Earthquakes GunPoint \n",
    "batch_size = 64 # Set to -1 to use Wang et al settings\n",
    "nb_epochs = 1500 # Wang: 1500\n",
    "k = 10 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 1 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "k_fold_seed = 87\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "\n",
    "# Directories\n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now())\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "# Input directory\n",
    "if 'private' in flist[0]:\n",
    "    fdir = '../data/private_data/private_events_dev2' \n",
    "else:\n",
    "    fdir = '../data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GunPoint 2018-12-22T14:40\n",
    "\n",
    "Batch size = 64\n",
    "\n",
    "|Run |Loss |Accuracy | Epoch index     | Duration\n",
    "|:---|:--- |:---     |:----------      |:-------------\n",
    "|0  |1.2543987577373627e-05  |0.9799999952316284 |1494 |3mins  |\n",
    "|1  |0.00013335476629436016  |0.9699999952316284 |1496 |3mins  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix', save=False):\n",
    "    ''' Plot the normalised confusion matrix\n",
    "    Parameters\n",
    "    cm : array - normalised confusion matrix\n",
    "    Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    '''\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.clim(0, 1)\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('cm_resnet.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_probs, save=False): \n",
    "    ''' Plot ROC and return AUC\n",
    "    Parameters\n",
    "    y_true : vector of true class labels.\n",
    "    y_probs : array of predicted probabilities, one column for each class.\n",
    "    Returns\n",
    "    auc : float\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:,1])\n",
    "    auc = roc_auc_score(y_true, y_probs[:,1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig('roc_resnet.png', bbox_inches='tight')\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out\n",
    " \n",
    "       \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "   \n",
    "    \n",
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    \n",
    "    print('Running dataset', fname)\n",
    "    if batch_size == -1:\n",
    "        batch = int(min(x_train.shape[0]/10, 16)) # Wang et al. setting.\n",
    "    else:\n",
    "        batch=batch_size\n",
    "        \n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "      \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "    x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,1,))\n",
    "     \n",
    "     \n",
    "    x , y = build_resnet(x_train.shape[1:], 64, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    pathlib.Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        pathlib.Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "  \n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history)  \n",
    "\n",
    "    # Print and save results. Print the testing results which has the lowest training loss.\n",
    "    print('Training complete on', fname)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training time ', end-start, 'seconds, which is about', duration_minutes, 'minutes.')    \n",
    "    print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "    idx = log['loss'].idxmin()\n",
    "    loss = log.loc[idx]['loss']\n",
    "    val_acc = log.loc[idx]['val_acc']\n",
    "    print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(idx+1), ')')\n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(idx)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(idx)+','+ duration_minutes \n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    with open(logs_dir+'/'+fname+'/resnet_summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/resnet_summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    return summary, model\n",
    "\n",
    "\n",
    "# main\n",
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done at\n",
    "'{:%Y-%m-%dT%H:%M}'.format(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model (after all epochs) to make predictions\n",
    "do_print = True\n",
    "x_input = x_test\n",
    "y_input = y_test\n",
    "y_input = y_input - y_input.min()\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_input = (x_input - x_train_mean)/(x_train_std)\n",
    "x_input = x_input.reshape(x_input.shape + (1,1,))\n",
    "nb_classes = len(np.unique(y_input))\n",
    "y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "# Class balance\n",
    "n0 = (y_input == 0).sum()\n",
    "n1 = (y_input == 1).sum()\n",
    "# Calculate model prediction\n",
    "y_probs = model.predict_on_batch(x_input)\n",
    "y_class = y_probs.argmax(axis=1)\n",
    "cm = confusion_matrix(y_input, y_probs.argmax(axis=1), labels=[1,0])\n",
    "acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "if do_print:\n",
    "    print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "    print('Pred', y_class[:20])\n",
    "    print('True', y_input[:20].astype(int))\n",
    "    print(cm)\n",
    "    print('Calculated accuracy:',acc_calc)\n",
    "    print('Class balance in test set:', n0, 'to', n1, 'i.e.', n0/(n0+n1))\n",
    "    print('Normalised confusion matrix:\\n', cm_norm)\n",
    "title = 'Normalised confusion matrix'\n",
    "plot_confusion_matrix(cm_norm, title=title, save=True)\n",
    "\n",
    "# ROC and AUC\n",
    "auc = plot_roc(y_input, y_probs)\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = logs_dir+'/'+fname+'/resnet_summary.csv'\n",
    "data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "accuracy = data.iloc[:,2]\n",
    "print('Results from file', file)\n",
    "print(data.describe())\n",
    "print('Accuracy mean and 95% confidence level is', accuracy.mean(), accuracy.std()*1.96)\n",
    "print('95% confidence interval is', accuracy.quantile(0.0025), 'to', accuracy.quantile(0.975))\n",
    "plt.figure(0)\n",
    "data.boxplot(column='loss')\n",
    "plt.figure(1)\n",
    "data.boxplot(column='val_acc')\n",
    "data.hist(column='val_acc')\n",
    "print('Rows with val_acc<1 are')\n",
    "data[data.val_acc<0.99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary results\n",
    "\n",
    "95% confidence :-\n",
    "\n",
    "|Timestamp |Model |Mean val_acc | +/- | lower |  upper  | Comment\n",
    "|:---|:--- |:---  |:---    |:----------      |:------------- |:-------------\n",
    "|2018-12-01T08:43  |MLP  |0.9715 | 0.0727 |0.8500 |1.0  | 10-fold, 10 resamples |\n",
    "|2018-12-22T15:54  |ResNet  |0.9960 | 0.0301 | 0.9124 |1.0  | 10-fold, 10 resamples |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work in progress - results for ACI dataset\n",
    "\n",
    "Data samples truncated to 12,000 datapoints.\n",
    "Using the 'small' dataset - Train on 164 samples, validate on 1484 samples. Batch size 64 was too big for GPU. Batch size 32 ran, with 100% GPU utilisation.\n",
    "\n",
    "\n",
    "|Timestamp         |Model  |epochs  |loss   | val_acc | at epoch  | run time | Comment\n",
    "|:------           |:---   |:---    |:----  |:-----   |:-----     |:-----    |:-------------\n",
    "|2019-02-03T15:30  |ResNet | 50     |0.5681 | 0.7305  |18         | 19mins   |164 train, 1484 test |\n",
    "|2019-02-03T16:36  |ResNet | 50     |0.5391 | 0.6923  |46         |  5mins   |154 train, 52 test   |\n",
    "|2019-02-03T16:46  |ResNet | 50     |0.5890 | 0.7208  |26         |  3mins   |52 train,  154 test   |\n",
    "|2019-02-03T17:01  |ResNet | 50     |0.5883 | 0.7208  |26         |  3mins   | \"  batch size 16   |\n",
    "|2019-02-03T17:08  |ResNet | 50     |0.0308 | 0.5649 |1490        | 92mins   | \"  acc=1.0   |\n",
    "\n",
    "## Event windowing applied to reduce data to 1000 datapoints\n",
    "Batch size 32. Train on 50 samples, validate on 150 samples.\n",
    "\n",
    "|Timestamp         |Dataset |Model  |epochs  |loss   | val_acc | at epoch  | run time | Comment\n",
    "|:------           |:---    |:---   |:---    |:----  |:-----   |:-----     |:-----    |:-------------\n",
    "|2019-02-17T08:52  |mini_dog2 |ResNet | 1500   |0.007090 |0.5000 |1468 |9mins  | acc=1.0\n",
    "|2019-02-17T09:08  |mini      |ResNet |1500   |0.03282  |0.5066 |1493 |9mins  | acc = 0.98\n",
    "|2019-02-17T17:49 |mini_dog2 but 150:50 train:test |ResNet | 1500 |0.00125 | 0.5000 | 1467 |15mins  |\n",
    "\n",
    "\n",
    "## Removed samples where no event occured\n",
    "\n",
    "|Timestamp         |Dataset |Model  |epochs  |loss   | val_acc | at epoch  | run time | Comment\n",
    "|:------           |:---    |:---   |:---    |:----  |:-----   |:-----     |:-----    |:-------------\n",
    "|2019-02-22T18:02  |private_dog0 (135:45)|MLP | 1500   |0.000256 |0.489 |1473 |1mins  | acc=1.0\n",
    "|2019-02-22T18:05  |private_dog1 (111:37)|MLP | 1500   |2.6e-6 |0.459 |1190 |1mins  | acc=1.0\n",
    "|2019-02-23T11:14  |private_dog2 (120:40)|MLP | 1500   |9e-6 |0.525 |1414 |1mins  | acc=1.0\n",
    "|2019-02-23T08:12  |private_dog0 (135:45)|ResNet | 1500   |0.0802 |0.667 |1499 |13mins  | acc=0.99\n",
    "|2019-02-23T08:59  |private_dog1 (111:37)|ResNet | 1500   |0.000876 |0.568 |1480 |10mins  | acc=1.0\n",
    "|2019-02-21T16:48  |private_dog2 (120:40)|ResNet | 1500   |0.000393 |0.625 |1423 |12mins  | acc=1.0\n",
    "|2019-02-21T17:08 |private_events_dev (831:278)|ResNet | 1500   |8.128e-05|0.637 |1468 |75mins  | acc=1.0, batch size 64\n",
    "|2019-02-22T07:25  |private_mini (50:150) |MLP |1500 | 1.8502e-06 | 0.627 | 1232 | 1min | acc=1.0, batch size 32\n",
    "|2019-02-22T08:41  |private_events_dev (831:278) |MLP  |1500 | 0.0182 | 0.6331 | 1416 | 3mins | acc=0.98 batch size 64\n",
    "\n",
    "\n",
    "\n",
    "NB private_events_dev_TEST is unbalanced; with 0.676 in class 0\n",
    "\n",
    "\n",
    "## Cross fold validation\n",
    "\n",
    "2019-02-22T08:52/private_events_dev  \n",
    "\n",
    "MLP\n",
    "\n",
    "'../logs/2019-02-22T08:52/private_events_dev/summary.csv'\n",
    "\n",
    "6.5h\n",
    "\n",
    "val_acc \n",
    "min 0.558559;\n",
    "max 0.781818 \n",
    "\n",
    "mean and 95% confidence level is 0.6686 0.0821\n",
    "\n",
    "95% confidence interval is 0.5675 to 0.7435\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
