{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dataset GunPoint\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "        -- model was built.\n",
      "Tensorboard logs in ../logs/tensorboard/2018-12-22T15:53/GunPoint_0\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 2s 21ms/step - loss: 0.7216 - acc: 0.6300 - val_loss: 0.6943 - val_acc: 0.4800\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.8411 - acc: 0.6800 - val_loss: 0.6931 - val_acc: 0.5100\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4993 - acc: 0.7600 - val_loss: 0.6927 - val_acc: 0.5900\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3870 - acc: 0.8300 - val_loss: 0.6918 - val_acc: 0.5200\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4210 - acc: 0.6900 - val_loss: 0.6918 - val_acc: 0.5200\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3373 - acc: 0.8500 - val_loss: 0.6912 - val_acc: 0.5200\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2933 - acc: 0.8900 - val_loss: 0.6906 - val_acc: 0.5200\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2351 - acc: 0.9600 - val_loss: 0.6908 - val_acc: 0.5200\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2278 - acc: 0.9400 - val_loss: 0.6918 - val_acc: 0.5200\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1552 - acc: 1.0000 - val_loss: 0.6930 - val_acc: 0.5200\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1449 - acc: 0.9700 - val_loss: 0.6953 - val_acc: 0.5200\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0959 - acc: 0.9900 - val_loss: 0.6986 - val_acc: 0.5200\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1089 - acc: 0.9800 - val_loss: 0.7028 - val_acc: 0.5200\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0834 - acc: 0.9900 - val_loss: 0.7077 - val_acc: 0.5200\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0583 - acc: 0.9900 - val_loss: 0.7127 - val_acc: 0.5200\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1176 - acc: 0.9800 - val_loss: 0.7124 - val_acc: 0.5200\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0655 - acc: 0.9900 - val_loss: 0.7117 - val_acc: 0.5200\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0576 - acc: 1.0000 - val_loss: 0.7112 - val_acc: 0.5200\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0949 - acc: 0.9700 - val_loss: 0.7108 - val_acc: 0.5200\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0634 - acc: 0.9800 - val_loss: 0.7124 - val_acc: 0.5200\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0605 - acc: 0.9800 - val_loss: 0.7142 - val_acc: 0.5200\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0554 - acc: 0.9900 - val_loss: 0.7146 - val_acc: 0.5200\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0554 - acc: 0.9900 - val_loss: 0.7166 - val_acc: 0.5200\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0379 - acc: 1.0000 - val_loss: 0.7193 - val_acc: 0.5200\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0366 - acc: 0.9900 - val_loss: 0.7210 - val_acc: 0.5200\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.7230 - val_acc: 0.5200\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.7265 - val_acc: 0.5200\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.7294 - val_acc: 0.5200\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.7323 - val_acc: 0.5300\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.7358 - val_acc: 0.5500\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.7392 - val_acc: 0.4500\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.7429 - val_acc: 0.5200\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.7486 - val_acc: 0.5200\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.7581 - val_acc: 0.5200\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.7704 - val_acc: 0.5200\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.7908 - val_acc: 0.5200\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.8194 - val_acc: 0.5200\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.8237 - val_acc: 0.5200\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.8170 - val_acc: 0.5200\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.8327 - val_acc: 0.5200\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.8605 - val_acc: 0.5200\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.8821 - val_acc: 0.5200\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.8925 - val_acc: 0.5200\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.9064 - val_acc: 0.5200\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.9308 - val_acc: 0.5200\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.9576 - val_acc: 0.5200\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.9861 - val_acc: 0.5200\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0056 - acc: 1.0000 - val_loss: 1.0375 - val_acc: 0.5200\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 1.0888 - val_acc: 0.5200\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 1.0973 - val_acc: 0.5200\n",
      "Training complete on GunPoint\n",
      "Training time  16.688679218292236 seconds, which is about 0 minutes.\n",
      "Selected the test result with the lowest training loss. Loss and validation accuracy are -\n",
      "0.0021344389021396635 0.5199999976158142 at index 48  (epoch  49 )\n",
      "Tensorboard logs in ../logs/tensorboard/2018-12-22T15:53/GunPoint_0\n",
      "Added summary row to  ../logs/2018-12-22T15:53/GunPoint/resnet_summary.csv\n",
      "Saving logs to ../logs/2018-12-22T15:53/GunPoint/history_0.csv\n",
      "Running dataset GunPoint\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "        -- model was built.\n",
      "Tensorboard logs in ../logs/tensorboard/2018-12-22T15:53/GunPoint_1\n",
      "Train on 100 samples, validate on 100 samples\n",
      "Epoch 1/50\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.7046 - acc: 0.6400 - val_loss: 0.6923 - val_acc: 0.5200\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.8362 - acc: 0.5600 - val_loss: 0.6923 - val_acc: 0.5200\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4855 - acc: 0.7800 - val_loss: 0.6924 - val_acc: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3566 - acc: 0.9200 - val_loss: 0.6933 - val_acc: 0.4900\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3105 - acc: 0.9100 - val_loss: 0.6940 - val_acc: 0.4800\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2517 - acc: 0.9500 - val_loss: 0.6947 - val_acc: 0.4800\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1957 - acc: 0.9700 - val_loss: 0.6954 - val_acc: 0.4800\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1745 - acc: 0.9700 - val_loss: 0.6967 - val_acc: 0.4800\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1360 - acc: 0.9700 - val_loss: 0.6989 - val_acc: 0.4800\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1213 - acc: 0.9800 - val_loss: 0.7015 - val_acc: 0.4800\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1070 - acc: 0.9800 - val_loss: 0.7042 - val_acc: 0.4800\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0767 - acc: 0.9900 - val_loss: 0.7070 - val_acc: 0.4800\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0659 - acc: 1.0000 - val_loss: 0.7101 - val_acc: 0.4800\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0590 - acc: 0.9900 - val_loss: 0.7135 - val_acc: 0.4800\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0585 - acc: 1.0000 - val_loss: 0.7177 - val_acc: 0.4800\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0474 - acc: 0.9900 - val_loss: 0.7226 - val_acc: 0.4800\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0503 - acc: 0.9900 - val_loss: 0.7269 - val_acc: 0.4800\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0425 - acc: 0.9900 - val_loss: 0.7311 - val_acc: 0.4800\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0317 - acc: 0.9900 - val_loss: 0.7341 - val_acc: 0.4800\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.7369 - val_acc: 0.4800\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.7404 - val_acc: 0.4800\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.7453 - val_acc: 0.4800\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.7506 - val_acc: 0.4800\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.7548 - val_acc: 0.4800\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.7580 - val_acc: 0.4800\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.7596 - val_acc: 0.4800\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.7622 - val_acc: 0.4800\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.7655 - val_acc: 0.4800\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.7659 - val_acc: 0.4800\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.7662 - val_acc: 0.4800\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.7703 - val_acc: 0.4800\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.7767 - val_acc: 0.4800\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7802 - val_acc: 0.4800\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.7786 - val_acc: 0.4800\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7766 - val_acc: 0.4800\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.7838 - val_acc: 0.4800\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.7985 - val_acc: 0.4800\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.8056 - val_acc: 0.4800\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.8039 - val_acc: 0.4800\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7988 - val_acc: 0.4800\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.7987 - val_acc: 0.4800\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.8081 - val_acc: 0.4800\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.8204 - val_acc: 0.4800\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.8250 - val_acc: 0.4800\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.8269 - val_acc: 0.4800\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.8255 - val_acc: 0.4800\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.8263 - val_acc: 0.4800\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.8287 - val_acc: 0.4800\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.8273 - val_acc: 0.4800\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.8243 - val_acc: 0.4800\n",
      "Training complete on GunPoint\n",
      "Training time  18.175066709518433 seconds, which is about 0 minutes.\n",
      "Selected the test result with the lowest training loss. Loss and validation accuracy are -\n",
      "0.0014736468996852637 0.4799999976158142 at index 47  (epoch  48 )\n",
      "Tensorboard logs in ../logs/tensorboard/2018-12-22T15:53/GunPoint_1\n",
      "Added summary row to  ../logs/2018-12-22T15:53/GunPoint/resnet_summary.csv\n",
      "Saving logs to ../logs/2018-12-22T15:53/GunPoint/history_1.csv\n",
      "DONE\n",
      "GunPoint 2018-12-22T15:53\n",
      "|0  |0.0021344389021396635  |0.5199999976158142 |48 |0mins  |\n",
      "|1  |0.0014736468996852637  |0.4799999976158142 |47 |0mins  |\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(813306)\n",
    "\n",
    "# User inputs ##\n",
    "\n",
    "flist = ['GunPoint'] # List dataset directory names.\n",
    "batch_size = 64 # Wang: int(min(x_train.shape[0]/10, 16)) \n",
    "nb_epochs = 1500 # Wang: 1500\n",
    "k = 10 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 10 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "k_fold_seed = 87\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "early_stopping = False # Not yet ready for use # Use early stopping.\n",
    "# Directories\n",
    "fdir = '../data'  \n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now())\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out\n",
    " \n",
    "       \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "      \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "    x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,1,))\n",
    "     \n",
    "     \n",
    "    x , y = build_resnet(x_train.shape[1:], 64, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    pathlib.Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        pathlib.Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "         patience=10) # when tf updates keras, use restore_best_weights instead of ModelCheckPoint\n",
    "        model_save = keras.callbacks.ModelCheckpoint(logs_dir+'/temp.h5', monitor='val_acc', \n",
    "            save_best_only=True, save_weights_only=True)\n",
    "        callbacks.append(early_stop)\n",
    "        callbacks.append(model_save)\n",
    "\n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history)  \n",
    "    \n",
    "    if not early_stopping:\n",
    "        # Print and save results. Print the testing results which has the lowest training loss.\n",
    "        print('Training complete on', fname)\n",
    "        duration_minutes = str(round((end-start)/60))\n",
    "        print('Training time ', end-start, 'seconds, which is about', duration_minutes, 'minutes.')    \n",
    "        print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "        idx = log['loss'].idxmin()\n",
    "        loss = log.loc[idx]['loss']\n",
    "        val_acc = log.loc[idx]['val_acc']\n",
    "        print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(idx+1), ')')\n",
    "        summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(idx)+' |'+ duration_minutes + 'mins  |'\n",
    "        summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(idx)+','+ duration_minutes \n",
    "    \n",
    "    else:\n",
    "        model.load_weights(logs_dir+'/temp.h5')\n",
    "        eval = model.evaluate(metrics=['loss','accuracy'])\n",
    "        print('From model.evaluate()')\n",
    "        print(eval['loss'], eval['accuracy'])\n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    with open(logs_dir+'/'+fname+'/resnet_summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/resnet_summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    return summary, model\n",
    "\n",
    "\n",
    "# main\n",
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "for each in results:\n",
    "    print(each)\n",
    "#print('Example prediction probabilities')\n",
    "#y_predict = model.predict_proba(x_test)            \n",
    "#for yp in y_predict[1:20]:\n",
    "#    print(yp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GunPoint 2018-12-22T14:40\n",
    "\n",
    "Batch size = 64\n",
    "\n",
    "|Run |Loss |Accuracy | Epoch index     | Duration\n",
    "|:---|:--- |:---     |:----------      |:-------------\n",
    "|0  |1.2543987577373627e-05  |0.9799999952316284 |1494 |3mins  |\n",
    "|1  |0.00013335476629436016  |0.9699999952316284 |1496 |3mins  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
