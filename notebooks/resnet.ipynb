{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "build conv_x\n",
      "build conv_y\n",
      "build conv_z\n",
      "Merging skip connection\n",
      "        -- model was built.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 150, 1, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 150, 1, 1)    4           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 150, 1, 64)   4160        batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 150, 1, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 150, 1, 64)   0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 150, 1, 64)   102464      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 150, 1, 64)   256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 150, 1, 64)   0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 150, 1, 64)   128         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 150, 1, 64)   36928       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 150, 1, 64)   256         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 150, 1, 64)   256         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 150, 1, 64)   0           batch_normalization_49[0][0]     \n",
      "                                                                 batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 150, 1, 64)   0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 150, 1, 128)  524416      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 150, 1, 128)  512         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 150, 1, 128)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 150, 1, 128)  409728      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 150, 1, 128)  512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 150, 1, 128)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 150, 1, 128)  8320        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 150, 1, 128)  147584      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 150, 1, 128)  512         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 150, 1, 128)  512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 150, 1, 128)  0           batch_normalization_53[0][0]     \n",
      "                                                                 batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 150, 1, 128)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 150, 1, 128)  1048704     activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 150, 1, 128)  512         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 150, 1, 128)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 150, 1, 128)  409728      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 150, 1, 128)  512         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 150, 1, 128)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 150, 1, 128)  16512       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 150, 1, 128)  147584      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 150, 1, 128)  512         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 150, 1, 128)  512         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 150, 1, 128)  0           batch_normalization_57[0][0]     \n",
      "                                                                 batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 150, 1, 128)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 128)          0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            258         global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 2,861,638\n",
      "Trainable params: 2,859,076\n",
      "Non-trainable params: 2,562\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 50 samples, validate on 150 samples\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 7s 134ms/step - loss: 0.9186 - acc: 0.4800 - val_loss: 0.6933 - val_acc: 0.4933\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5557 - acc: 0.7000 - val_loss: 0.6937 - val_acc: 0.4933\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4854 - acc: 0.8200 - val_loss: 0.6944 - val_acc: 0.4933\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6491 - acc: 0.7200 - val_loss: 0.6945 - val_acc: 0.4933\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4908 - acc: 0.7000 - val_loss: 0.6949 - val_acc: 0.4933\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3507 - acc: 0.8400 - val_loss: 0.6951 - val_acc: 0.4933\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3401 - acc: 0.8000 - val_loss: 0.6952 - val_acc: 0.4933\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2857 - acc: 0.9000 - val_loss: 0.6950 - val_acc: 0.4933\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2468 - acc: 0.9200 - val_loss: 0.6949 - val_acc: 0.4933\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1973 - acc: 0.9600 - val_loss: 0.6948 - val_acc: 0.4933\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1590 - acc: 0.9600 - val_loss: 0.6948 - val_acc: 0.4933\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1226 - acc: 0.9800 - val_loss: 0.6950 - val_acc: 0.4933\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0986 - acc: 0.9800 - val_loss: 0.6954 - val_acc: 0.4933\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0799 - acc: 1.0000 - val_loss: 0.6958 - val_acc: 0.4933\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0638 - acc: 1.0000 - val_loss: 0.6961 - val_acc: 0.4933\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0522 - acc: 1.0000 - val_loss: 0.6964 - val_acc: 0.4933\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.6968 - val_acc: 0.4933\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0339 - acc: 1.0000 - val_loss: 0.6971 - val_acc: 0.4933\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.6973 - val_acc: 0.4933\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0223 - acc: 1.0000 - val_loss: 0.6976 - val_acc: 0.4933\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.6981 - val_acc: 0.4933\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.6987 - val_acc: 0.4933\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.6994 - val_acc: 0.4933\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.7003 - val_acc: 0.4933\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.7012 - val_acc: 0.4933\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.7022 - val_acc: 0.4933\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.7033 - val_acc: 0.4933\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.7045 - val_acc: 0.4933\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.7058 - val_acc: 0.4933\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.7073 - val_acc: 0.4933\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.7088 - val_acc: 0.4933\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7104 - val_acc: 0.4933\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.7121 - val_acc: 0.4933\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.7138 - val_acc: 0.4933\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.7155 - val_acc: 0.4933\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.7173 - val_acc: 0.4933\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.7191 - val_acc: 0.4933\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.7209 - val_acc: 0.4933\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.7228 - val_acc: 0.4933\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.7247 - val_acc: 0.4933\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.7266 - val_acc: 0.4933\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.7285 - val_acc: 0.4933\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.7304 - val_acc: 0.4933\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.7323 - val_acc: 0.4933\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.7343 - val_acc: 0.4933\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7364 - val_acc: 0.4933\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.7385 - val_acc: 0.4933\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7407 - val_acc: 0.4933\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.7429 - val_acc: 0.4933\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.7451 - val_acc: 0.4933\n",
      "0.0010362016037106514 0.49333333333333335\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "np.random.seed(813306)\n",
    "\n",
    "# User inputs ##\n",
    "\n",
    "flist = ['GunPoint'] # List dataset directory names.\n",
    "batch_size = 64 # Wang: int(min(x_train.shape[0]/10, 16)) \n",
    "nb_epochs = 1500 # Wang: 1500\n",
    "\n",
    "# Directories\n",
    "fdir = '../data'  \n",
    " \n",
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out\n",
    " \n",
    "       \n",
    "def readucr(filename):\n",
    "    data = np.loadtxt(filename)\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "   \n",
    "\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std)\n",
    "      \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "    x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,1,))\n",
    "     \n",
    "     \n",
    "    x , y = build_resnet(x_train.shape[1:], 64, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    print(model.summary())\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "      \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks = [reduce_lr])\n",
    "    log = pd.DataFrame(hist.history)\n",
    "    print(log.loc[log['loss'].idxmin]['loss'], log.loc[log['loss'].idxmin]['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
