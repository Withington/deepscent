{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis\n",
    "Load model and analyse performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = '../../data/private_data/private_events_dev2'\n",
    "fname = 'private_correct_plus'\n",
    "model_type = 'MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = '../../logs/2019-05-11T19:09/private_correct_plus/model'\n",
    "json_file = open(modelfile+'.json', 'r')\n",
    "loaded_json_model = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_json_model)\n",
    "# load weights into new model\n",
    "model.load_weights(modelfile+'.h5')\n",
    "print('Model loaded from file', modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def reshape(x, model_type):\n",
    "    ''' Reshape data into input format for the selected DNN '''\n",
    "    if model_type == 'ResNet':\n",
    "        return reshape_2d(x)\n",
    "    elif model_type == 'FCN' or model_type == 'FCN_HARUS' or model_type == 'ResNet_tuned':\n",
    "        return reshape_1d(x)\n",
    "    elif model_type == 'MLP':\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unrecognised model type')\n",
    "    return x\n",
    "\n",
    "\n",
    "# Estimate x_train mean and std\n",
    "x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "model_params = {'x_train_mean':x_train_mean, 'x_train_std':x_train_std}\n",
    "\n",
    "other = fname+'_END_TEST' #_dog_incorrect' # 'private_dog0_correct_plus_END_TEST'\n",
    "datadir = fdir+'/'+fname\n",
    "print('Testing on:', datadir+'/'+other+'.txt')\n",
    "x_other, y_other = readucr(datadir+'/'+other+'.txt')\n",
    "\n",
    "\n",
    "def predictions(model, model_params, model_type, \n",
    "                x_input, y_input, name, threshold=0.5):\n",
    "    ''' Use the model to make predictions on x_input data. Return the predictions and the calculated accuracy. '''    \n",
    "    do_print = True\n",
    "    y_input = y_input - y_input.min()\n",
    "    x_input = (x_input - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "    x_input = reshape(x_input, model_type)\n",
    "    nb_classes = len(np.unique(y_input))\n",
    "    y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "    # Class balance\n",
    "    n0 = (y_input == 0).sum()\n",
    "    n1 = (y_input == 1).sum()\n",
    "    \n",
    "    # Calculate model prediction\n",
    "    y_probs = model.predict_on_batch(x_input)\n",
    "    if threshold == 0.5:\n",
    "        y_pred = np.round(y_probs).flatten()\n",
    "    else:\n",
    "        y_pred = y_probs.flatten()\n",
    "        y_pred[y_pred > threshold] = 1\n",
    "        y_pred[y_pred <= threshold] = 0\n",
    "        \n",
    "    cm = confusion_matrix(y_input, y_pred, labels=[1,0])\n",
    "    acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    if do_print:\n",
    "        print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "        print('Pred', y_pred[:20])\n",
    "        print('True', y_input[:20].astype(int))\n",
    "        print(cm)\n",
    "        print('Calculated accuracy:',acc_calc)\n",
    "        print('Class balance in test set:', n0, 'to', n1, 'i.e.', n0/(n0+n1))\n",
    "\n",
    "    return y_probs, y_pred, acc_calc\n",
    "\n",
    "#y_probs, y_pred, acc = predictions(model, model_params, model_type, x_test, y_test, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = fname+'_END_TEST' \n",
    "datadir = fdir+'/'+fname\n",
    "print('Testing on:', datadir+'/'+other+'.txt')\n",
    "x_other, y_other = readucr(datadir+'/'+other+'.txt')\n",
    "y_other_probs, y_other_pred, other_acc = predictions(\n",
    "    model, model_params, model_type, \n",
    "    x_other, y_other, other)\n",
    "# Get dog result\n",
    "meta = pd.read_csv(datadir+'/'+other+'_meta.txt', sep=',', parse_dates=['date'])\n",
    "cm = confusion_matrix(y_other, meta['dog_pred'], labels=[1,0])\n",
    "print('Dog cm \\n', cm)\n",
    "dog_acc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('True', y_other[:20])\n",
    "print('Dog ', meta['dog_pred'].values[:20])\n",
    "print('Dog accuracy', dog_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification probability\n",
    "On tuned MLP, trained on all dogs correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = ['darkorange', 'steelblue']\n",
    "class_cmap = matplotlib.colors.ListedColormap(class_colors)\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(np.arange(y_other_probs.shape[0]), y_other_probs, linestyle='None', marker='x', \n",
    "            c=y_other, cmap=class_cmap)\n",
    "plt.title('Orange: true class 0\\nBlue: true class 1')\n",
    "ax.set_xlabel('Test sample number')\n",
    "ax.set_ylabel('Model: probability of belonging to class 1')\n",
    "ax.set_ylim(bottom=0, top=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df = pd.DataFrame({'true': y_other.astype(int), 'dnn_pred': y_other_pred.astype(int), 'dnn_prob': y_other_probs[:,0]})\n",
    "df = pd.concat([meta, probs_df], axis=1)\n",
    "print(df.head())\n",
    "\n",
    "# Most confident incorrect answer for true class 1\n",
    "min_prob = probs_df[probs_df['dnn_pred'] != probs_df['true']]['dnn_prob'].min()\n",
    "print(probs_df[probs_df['dnn_prob'] == min_prob])\n",
    "print(probs_df[probs_df['dnn_pred'] != probs_df['true']]['dnn_prob'].nsmallest(10))\n",
    "\n",
    "# Most confident incorrect answer for true class 0\n",
    "max_prob = probs_df[probs_df['dnn_pred'] != probs_df['true']]['dnn_prob'].max()\n",
    "print(probs_df[probs_df['dnn_prob'] == max_prob])\n",
    "print(probs_df[probs_df['dnn_pred'] != probs_df['true']]['dnn_prob'].nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_key(true, pred):\n",
    "    ''' Return TP, TN, FP, FN as appropriate '''\n",
    "    print(true, pred)\n",
    "    if true == 1:\n",
    "        print('true 1')\n",
    "        if pred == 1:\n",
    "            print('pred 1')\n",
    "            result = 'TP'\n",
    "        else:\n",
    "            print('pred 0')\n",
    "            result = 'FN'\n",
    "    else:\n",
    "        print('true 0')\n",
    "        if pred == 1:\n",
    "            result = 'FP'\n",
    "        else:\n",
    "            result = 'TN'\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_to_plot(plot_i, data_i, color):\n",
    "    ''' Add a subplot using the given data sample '''\n",
    "    print(df.iloc[data_i][['filename', 'sensor_number', 'dog_result', 'Concentration']])\n",
    "    ax[plot_i].set_ylim(bottom=0, top=2.2)\n",
    "    ax[plot_i].plot(x_other[data_i], color='red')\n",
    "    ax[plot_i].set_facecolor(color)\n",
    "    true_class = df.iloc[data_i]['class']\n",
    "    dnn_class = df.iloc[data_i]['dnn_pred']\n",
    "    dnn_prob = '{0:.2f}'.format(df.iloc[data_i]['dnn_prob'])\n",
    "    dog_pred = int(df.iloc[data_i]['dog_pred'])\n",
    "    dnn_result = get_result_key(true_class, dnn_class)\n",
    "    dog_result = get_result_key(true_class, dog_pred)\n",
    "    title = 'True '+str(true_class)+' : DNN '+str(dnn_class)+' (p = '+str(dnn_prob)+') : dog '+str(dog_pred)\n",
    "    title = title + '   (DNN '+dnn_result+' : dog '+dog_result+')'\n",
    "    ax[plot_i].set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 4 most confident incorrect answer for true class 1\n",
    "fig, ax = plt.subplots(4, 1, sharex='col', sharey='row', figsize=(10, 8))\n",
    "add_to_plot(0, 138, 'lightcyan')\n",
    "add_to_plot(1, 126, 'lightcyan')\n",
    "add_to_plot(2, 44, 'lightcyan')\n",
    "add_to_plot(3, 108, 'lightcyan')\n",
    "plt.savefig('model_analysis_class_1_'+fname+'.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next 4 most confident incorrect answer for true class 1\n",
    "fig, ax = plt.subplots(4, 1, sharex='col', sharey='row', figsize=(10, 8))\n",
    "add_to_plot(0, 105, 'lightcyan')\n",
    "add_to_plot(1, 32, 'lightcyan')\n",
    "add_to_plot(2, 74, 'lightcyan')\n",
    "add_to_plot(3, 27, 'lightcyan')\n",
    "\n",
    "plt.savefig('model_analysis_class_1_plot2_'+fname+'.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 8 plots above, concentration is from \n",
    "+ 2e-8 which is 1 in 50m\n",
    "+ to\n",
    "+ 2e-7 which is 1 in 5m\n",
    "\n",
    "In the lab tests the lowest concentration in the tests was 1 im 200m. Only 22 passes were at 1 in 50m or weaker but around 30% of postive passes were at 1 in 25m or weaker. So the passes where the DNN gave FN were not unusually weak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 4 most confident incorrect answer for true class 0\n",
    "fig, ax = plt.subplots(4, 1, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "add_to_plot(0, 135, 'lightyellow')\n",
    "add_to_plot(1, 141, 'lightyellow')\n",
    "add_to_plot(2, 144, 'lightyellow')\n",
    "add_to_plot(3, 9, 'lightyellow')\n",
    "\n",
    "plt.savefig('model_analysis_class' + str(0) + '_'+fname+'.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the plots above - plots 0 and 3 show a long delay between first touch and second search. Both are dog0. Maybe dog0 data needs to have a longer event window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilities by concentration, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_df = df[df['true']==1]\n",
    "print(this_df.head())\n",
    "print('Concentration range', this_df['Concentration'].min(), this_df['Concentration'].max())\n",
    "df_a = this_df[this_df['dog_pred']==1]\n",
    "df_b = this_df[this_df['dog_pred']==0]\n",
    "fig, ax = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(10, 4))\n",
    "ax[0].scatter(df_a['Concentration'], df_a['dnn_prob'], linestyle='None', marker='x', \n",
    "            color='green', label='dog 1 (TP)')\n",
    "ax[0].scatter(df_b['Concentration'], df_b['dnn_prob'], linestyle='None', marker='x', \n",
    "            color='red', label='dog 0 (FN)')\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_ylabel('DNN: probability of belonging to class 1')\n",
    "ax[0].set_xlim(left = 1e-8, right = 1e-3)\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_ylim(bottom=0, top=1)\n",
    "ax[0].set_xlabel('Concentration')\n",
    "ax[0].set_title('True class : 1')\n",
    "ax[0].set_facecolor('lightcyan')\n",
    "\n",
    "ax[1].scatter(np.arange(df['dnn_prob'].shape[0]), df['dnn_prob'], linestyle='None', marker='x', \n",
    "            c=df['true'], cmap=class_cmap)\n",
    "ax[1].set_title('Orange: true class 0\\nBlue: true class 1')\n",
    "ax[1].set_xlabel('Test sample number')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above -\n",
    "+ The dog gives FN at the weaker concentrations\n",
    "+ Most dog FNs also then generate DNN FNs. I.e. all but 2 dog FNs result in DNN FNs.\n",
    "+ No clear trend from DNN or dog that \"correctness\" is a function of concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouper = 'time'\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(10, 4))\n",
    "\n",
    "this_df = df[df['true']==1]\n",
    "df_a = this_df[(df['dog_result']=='TN') | (this_df['dog_result']=='TP')]\n",
    "df_b = this_df[(df['dog_result']=='FN') | (this_df['dog_result']=='FP')]\n",
    "\n",
    "ax[0].scatter(df_a[grouper], df_a['dnn_prob'], linestyle='None', marker='x', \n",
    "            color='green', label='dog 1 (TP)')\n",
    "ax[0].scatter(df_b[grouper], df_b['dnn_prob'], linestyle='None', marker='x', \n",
    "            color='red', label='dog 0 (FN)')\n",
    "#ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlabel(grouper)\n",
    "ax[0].set_ylabel('DNN: probability of belonging to class 1')\n",
    "ax[0].set_ylim(bottom=0, top=1)\n",
    "ax[0].set_facecolor('lightcyan')\n",
    "\n",
    "\n",
    "this_df = df[df['true']==0]\n",
    "df_a = this_df[(df['dog_result']=='TN') | (this_df['dog_result']=='TP')]\n",
    "df_b = this_df[(df['dog_result']=='FN') | (this_df['dog_result']=='FP')]\n",
    "ax[1].scatter(df_a[grouper], df_a['dnn_prob'], linestyle='None', marker='x', \n",
    "            color='green', label='dog 1 (TP)')\n",
    "ax[1].scatter(df_b[grouper], df_b['dnn_prob'], linestyle='None', marker='x', \n",
    "            color='red', label='dog 0 (FN)')\n",
    "#ax[1].legend(loc=\"upper right\")\n",
    "ax[1].set_xlabel(grouper)\n",
    "ax[1].set_ylabel('DNN: probability of belonging to class 1')\n",
    "ax[1].set_ylim(bottom=0, top=1)\n",
    "ax[1].set_facecolor('lightyellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dogs are getting more incorrect later in the day. Is that true for the entire dataset? It could be directly related to concentration - more weak concentration tests were run late in the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
