{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model analysis\n",
    "Load model and analyse performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = '../../data/private_data/private_events_dev2'\n",
    "fname = 'private_correct_plus'\n",
    "model_type = 'MLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = '../../logs/2019-05-11T19:09/private_correct_plus/model'\n",
    "json_file = open(modelfile+'.json', 'r')\n",
    "loaded_json_model = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_json_model)\n",
    "# load weights into new model\n",
    "model.load_weights(modelfile+'.h5')\n",
    "print('Model loaded from file', modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def reshape(x, model_type):\n",
    "    ''' Reshape data into input format for the selected DNN '''\n",
    "    if model_type == 'ResNet':\n",
    "        return reshape_2d(x)\n",
    "    elif model_type == 'FCN' or model_type == 'FCN_HARUS' or model_type == 'ResNet_tuned':\n",
    "        return reshape_1d(x)\n",
    "    elif model_type == 'MLP':\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unrecognised model type')\n",
    "    return x\n",
    "\n",
    "\n",
    "# Estimate x_train mean and std\n",
    "x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "model_params = {'x_train_mean':x_train_mean, 'x_train_std':x_train_std}\n",
    "\n",
    "other = fname+'_END_TEST' #_dog_incorrect' # 'private_dog0_correct_plus_END_TEST'\n",
    "datadir = fdir+'/'+fname\n",
    "print('Testing on:', datadir+'/'+other+'.txt')\n",
    "x_other, y_other = readucr(datadir+'/'+other+'.txt')\n",
    "\n",
    "\n",
    "def predictions(model, model_params, model_type, \n",
    "                x_input, y_input, name, threshold=0.5):\n",
    "    ''' Use the model to make predictions on x_input data. Return the predictions and the calculated accuracy. '''    \n",
    "    do_print = True\n",
    "    y_input = y_input - y_input.min()\n",
    "    x_input = (x_input - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "    x_input = reshape(x_input, model_type)\n",
    "    nb_classes = len(np.unique(y_input))\n",
    "    y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "    # Class balance\n",
    "    n0 = (y_input == 0).sum()\n",
    "    n1 = (y_input == 1).sum()\n",
    "    \n",
    "    # Calculate model prediction\n",
    "    y_probs = model.predict_on_batch(x_input)\n",
    "    if threshold == 0.5:\n",
    "        y_pred = np.round(y_probs).flatten()\n",
    "    else:\n",
    "        y_pred = y_probs.flatten()\n",
    "        y_pred[y_pred > threshold] = 1\n",
    "        y_pred[y_pred <= threshold] = 0\n",
    "        \n",
    "    cm = confusion_matrix(y_input, y_pred, labels=[1,0])\n",
    "    acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    if do_print:\n",
    "        print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "        print('Pred', y_pred[:20])\n",
    "        print('True', y_input[:20].astype(int))\n",
    "        print(cm)\n",
    "        print('Calculated accuracy:',acc_calc)\n",
    "        print('Class balance in test set:', n0, 'to', n1, 'i.e.', n0/(n0+n1))\n",
    "\n",
    "    return y_probs, y_pred, acc_calc\n",
    "\n",
    "#y_probs, y_pred, acc = predictions(model, model_params, model_type, x_test, y_test, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = fname+'_END_TEST' \n",
    "datadir = fdir+'/'+fname\n",
    "print('Testing on:', datadir+'/'+other+'.txt')\n",
    "x_other, y_other = readucr(datadir+'/'+other+'.txt')\n",
    "y_other_probs, y_other_pred, other_acc = predictions(\n",
    "    model, model_params, model_type, \n",
    "    x_other, y_other, other)\n",
    "# Get dog result\n",
    "meta = pd.read_csv(datadir+'/'+other+'_meta.txt', sep=',', parse_dates=['date'])\n",
    "cm = confusion_matrix(y_other, meta['dog_pred'], labels=[1,0])\n",
    "print('Dog cm \\n', cm)\n",
    "dog_acc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('True', y_other[:20])\n",
    "print('Dog ', meta['dog_pred'].values[:20])\n",
    "print('Dog accuracy', dog_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification probability\n",
    "On tuned MLP, trained on all dogs correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_colors = ['darkorange', 'steelblue']\n",
    "print(y_other_probs.shape[0])\n",
    "x = np.arange(y_other_probs.shape[0])\n",
    "class_cmap = matplotlib.colors.ListedColormap(class_colors)\n",
    "fig, ax = plt.subplots()\n",
    "plt.scatter(np.arange(y_other_probs.shape[0]), y_other_probs, linestyle='None', marker='x', \n",
    "            c=y_other, cmap=class_cmap)\n",
    "plt.title('Orange: true class 0\\nBlue: true class 1')\n",
    "ax.set_xlabel('Test sample number')\n",
    "ax.set_ylabel('Model: probability of belonging to class 1')\n",
    "ax.set_ylim(bottom=0, top=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
