{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet plus developments\n",
    "\n",
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ['private_dog2'] # List of dataset directory names. WormsTwoClass Lightning2 Earthquakes GunPoint \n",
    "batch_size = 64 \n",
    "nb_epochs = 500\n",
    "model_type = 'MLP'\n",
    "k = 1 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 10 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "early_stopping = False \n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "k_fold_seed = 765432\n",
    "\n",
    "# Output directories\n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now())\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "# Input directory\n",
    "if 'private' in flist[0]:\n",
    "    fdir = '../data/private_data/private_events_dev' \n",
    "else:\n",
    "    fdir = '../data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix', save=False):\n",
    "    ''' Plot the normalised confusion matrix\n",
    "    Parameters\n",
    "    cm : array - normalised confusion matrix\n",
    "    Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    '''\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.clim(0, 1)\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('cm_devnet.png', bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "def plot_roc(y_true, y_probs, save=False): \n",
    "    ''' Plot ROC and return AUC\n",
    "    Parameters\n",
    "    y_true : vector of true class labels.\n",
    "    y_probs : array of predicted probabilities, one column for each class.\n",
    "    Returns\n",
    "    auc : float\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:,1])\n",
    "    auc = roc_auc_score(y_true, y_probs[:,1])\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        plt.savefig('roc_devnet.png', bbox_inches='tight')\n",
    "    return auc\n",
    "\n",
    "\n",
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DNN\n",
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "         \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, nb_classes):\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dense(128, activation='relu', name='Dense010')(x)\n",
    "    y = Dense(128, activation='relu', name='Dense020')(y)\n",
    "    out = Dense(nb_classes, activation='sigmoid', name='Dense040')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "     \n",
    "    if model_type == 'MLP':\n",
    "        x, y = build_mlp(x_train.shape[1:], nb_classes)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "        x_test = x_test.reshape(x_test.shape + (1,1,))\n",
    "        x, y = build_resnet(x_train.shape[1:], 64, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop()#.Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', baseline=0.6, min_delta=0.001, \n",
    "         patience=100, verbose=1) # when tf updates keras, use restore_best_weights instead of ModelCheckPoint\n",
    "        # save_best_only - we are interested in only the very best model observed during training, \n",
    "        # rather than the best compared to the previous epoch, which might not be the best overall \n",
    "        # if training is noisy\n",
    "        model_save = keras.callbacks.ModelCheckpoint(logs_dir+'/temp.h5', monitor='val_acc', \n",
    "            save_best_only=True, save_weights_only=True, verbose=1) #\n",
    "        callbacks.append(early_stop)\n",
    "        callbacks.append(model_save)\n",
    "  \n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    \n",
    "    # Print results\n",
    "    duration_seconds = round(end-start)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training complete on', fname, 'Duration:', duration_seconds, 'secs; about', duration_minutes, 'minutes.')\n",
    "    \n",
    "    if not early_stopping:\n",
    "        # Print and save results. Print the testing results which has the lowest training loss.\n",
    "        print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "        idx = log['loss'].idxmin()\n",
    "        loss = log.loc[idx]['loss']\n",
    "        val_acc = log.loc[idx]['val_acc']\n",
    "        epoch = idx + 1\n",
    "        print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(epoch), ')')\n",
    "    else:\n",
    "        print('Early stopping.')\n",
    "        model.load_weights(logs_dir+'/temp.h5')\n",
    "        eval = model.evaluate(x_train, Y_train)\n",
    "        print('From model.evaluate() on training set', model.metrics_names)\n",
    "        print(eval)\n",
    "        eval = model.evaluate(x_test, Y_test)\n",
    "        print('From model.evaluate()', model.metrics_names)\n",
    "        print(eval)\n",
    "        loss = eval[0] # val_loss\n",
    "        val_acc = eval[1]\n",
    "        epoch = early_stop.stopped_epoch\n",
    "        print('stop_epoch:', epoch)\n",
    "        idx = epoch - 1\n",
    "        logged_loss = log.loc[idx]['loss']\n",
    "        logged_val_acc = log.loc[idx]['val_acc']\n",
    "        print('logged loss and val_acc', logged_loss, logged_val_acc)\n",
    "        \n",
    "        \n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(epoch)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(epoch)+','+ duration_minutes \n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    with open(logs_dir+'/'+fname+'/devnet_summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/devnet_summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    return summary, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dataset private_dog2\n",
      "Tensorboard logs in ../logs/tensorboard/2019-03-09T09:33/private_dog2_0\n",
      "Train on 120 samples, validate on 40 samples\n",
      "Epoch 1/500\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.8888 - acc: 0.4458 - val_loss: 0.8923 - val_acc: 0.6000\n",
      "Epoch 2/500\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.7384 - acc: 0.6083 - val_loss: 0.7033 - val_acc: 0.6375\n",
      "Epoch 3/500\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.5632 - acc: 0.7083 - val_loss: 0.7632 - val_acc: 0.6125\n",
      "Epoch 4/500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5483 - acc: 0.7083 - val_loss: 0.7482 - val_acc: 0.6250\n",
      "Epoch 5/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5170 - acc: 0.7208 - val_loss: 0.8352 - val_acc: 0.5250\n",
      "Epoch 6/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5198 - acc: 0.7375 - val_loss: 0.7996 - val_acc: 0.6000\n",
      "Epoch 7/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5114 - acc: 0.7250 - val_loss: 0.8919 - val_acc: 0.4875\n",
      "Epoch 8/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5495 - acc: 0.7000 - val_loss: 0.8370 - val_acc: 0.5375\n",
      "Epoch 9/500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.5065 - acc: 0.7500 - val_loss: 0.8107 - val_acc: 0.5375\n",
      "Epoch 10/500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4963 - acc: 0.7750 - val_loss: 0.7751 - val_acc: 0.6000\n",
      "Epoch 11/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4684 - acc: 0.7500 - val_loss: 0.8524 - val_acc: 0.6000\n",
      "Epoch 12/500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4481 - acc: 0.7708 - val_loss: 0.8574 - val_acc: 0.6000\n",
      "Epoch 13/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4524 - acc: 0.7542 - val_loss: 0.9355 - val_acc: 0.6250\n",
      "Epoch 14/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4999 - acc: 0.7417 - val_loss: 0.9301 - val_acc: 0.6000\n",
      "Epoch 15/500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4469 - acc: 0.7667 - val_loss: 0.8901 - val_acc: 0.5750\n",
      "Epoch 16/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4797 - acc: 0.6792 - val_loss: 0.9634 - val_acc: 0.6500\n",
      "Epoch 17/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4459 - acc: 0.7625 - val_loss: 0.8790 - val_acc: 0.5750\n",
      "Epoch 18/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4314 - acc: 0.7708 - val_loss: 0.9629 - val_acc: 0.5750\n",
      "Epoch 19/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4276 - acc: 0.7875 - val_loss: 0.9306 - val_acc: 0.5625\n",
      "Epoch 20/500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4606 - acc: 0.7042 - val_loss: 1.0640 - val_acc: 0.6250\n",
      "Epoch 21/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4618 - acc: 0.7833 - val_loss: 1.0235 - val_acc: 0.5625\n",
      "Epoch 22/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4025 - acc: 0.7875 - val_loss: 1.1320 - val_acc: 0.6000\n",
      "Epoch 23/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4367 - acc: 0.7500 - val_loss: 1.0648 - val_acc: 0.5625\n",
      "Epoch 24/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3977 - acc: 0.8083 - val_loss: 1.1426 - val_acc: 0.5500\n",
      "Epoch 25/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3848 - acc: 0.8583 - val_loss: 1.1822 - val_acc: 0.5250\n",
      "Epoch 26/500\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4176 - acc: 0.7833 - val_loss: 1.2419 - val_acc: 0.5000\n",
      "Epoch 27/500\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4896 - acc: 0.6958 - val_loss: 1.2004 - val_acc: 0.5500\n",
      "Epoch 28/500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4134 - acc: 0.8208 - val_loss: 1.2629 - val_acc: 0.5625\n",
      "Epoch 29/500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3698 - acc: 0.8417 - val_loss: 1.0787 - val_acc: 0.5750\n",
      "Epoch 30/500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3651 - acc: 0.8000 - val_loss: 1.2310 - val_acc: 0.5250\n",
      "Epoch 31/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4029 - acc: 0.7875 - val_loss: 1.2004 - val_acc: 0.5625\n",
      "Epoch 32/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3693 - acc: 0.8292 - val_loss: 1.1987 - val_acc: 0.5625\n",
      "Epoch 33/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3614 - acc: 0.8042 - val_loss: 1.3045 - val_acc: 0.5500\n",
      "Epoch 34/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3854 - acc: 0.8125 - val_loss: 1.3411 - val_acc: 0.6125\n",
      "Epoch 35/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3434 - acc: 0.8375 - val_loss: 1.2576 - val_acc: 0.5625\n",
      "Epoch 36/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3419 - acc: 0.8333 - val_loss: 1.2883 - val_acc: 0.6000\n",
      "Epoch 37/500\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3803 - acc: 0.8125 - val_loss: 1.2566 - val_acc: 0.5625\n",
      "Epoch 38/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3476 - acc: 0.8708 - val_loss: 1.2980 - val_acc: 0.5875\n",
      "Epoch 39/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3347 - acc: 0.8417 - val_loss: 1.2899 - val_acc: 0.5750\n",
      "Epoch 40/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3888 - acc: 0.7875 - val_loss: 1.2835 - val_acc: 0.5625\n",
      "Epoch 41/500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3876 - acc: 0.7875 - val_loss: 1.3254 - val_acc: 0.5625\n",
      "Epoch 42/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3481 - acc: 0.8042 - val_loss: 1.3068 - val_acc: 0.5375\n",
      "Epoch 43/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3327 - acc: 0.8500 - val_loss: 1.3008 - val_acc: 0.5625\n",
      "Epoch 44/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3044 - acc: 0.8708 - val_loss: 1.3223 - val_acc: 0.5875\n",
      "Epoch 45/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3040 - acc: 0.8625 - val_loss: 1.3817 - val_acc: 0.6250\n",
      "Epoch 46/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3112 - acc: 0.8333 - val_loss: 1.3450 - val_acc: 0.5750\n",
      "Epoch 47/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2985 - acc: 0.8625 - val_loss: 1.4116 - val_acc: 0.5125\n",
      "Epoch 48/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3118 - acc: 0.8417 - val_loss: 1.4228 - val_acc: 0.5500\n",
      "Epoch 49/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3615 - acc: 0.7958 - val_loss: 1.3670 - val_acc: 0.5375\n",
      "Epoch 50/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3349 - acc: 0.8125 - val_loss: 1.3718 - val_acc: 0.5625\n",
      "Epoch 51/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3112 - acc: 0.8500 - val_loss: 1.3810 - val_acc: 0.5750\n",
      "Epoch 52/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3072 - acc: 0.8250 - val_loss: 1.4663 - val_acc: 0.5875\n",
      "Epoch 53/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3008 - acc: 0.8333 - val_loss: 1.4680 - val_acc: 0.5875\n",
      "Epoch 54/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.2752 - acc: 0.8708 - val_loss: 1.4510 - val_acc: 0.6125\n",
      "Epoch 55/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2831 - acc: 0.8792 - val_loss: 1.5412 - val_acc: 0.5000\n",
      "Epoch 56/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.2937 - acc: 0.8708 - val_loss: 1.5333 - val_acc: 0.5500\n",
      "Epoch 57/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3096 - acc: 0.8500 - val_loss: 1.4944 - val_acc: 0.5250\n",
      "Epoch 58/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2771 - acc: 0.8792 - val_loss: 1.4920 - val_acc: 0.5750\n",
      "Epoch 59/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2600 - acc: 0.8792 - val_loss: 1.5330 - val_acc: 0.6000\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 166us/step - loss: 0.2522 - acc: 0.9000 - val_loss: 1.5483 - val_acc: 0.6000\n",
      "Epoch 61/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4249 - acc: 0.7792 - val_loss: 1.6409 - val_acc: 0.5875\n",
      "Epoch 62/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3537 - acc: 0.8167 - val_loss: 1.6542 - val_acc: 0.5750\n",
      "Epoch 63/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2899 - acc: 0.8917 - val_loss: 1.5613 - val_acc: 0.5500\n",
      "Epoch 64/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2638 - acc: 0.8958 - val_loss: 1.5033 - val_acc: 0.5500\n",
      "Epoch 65/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.2604 - acc: 0.8750 - val_loss: 1.5074 - val_acc: 0.5500\n",
      "Epoch 66/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2773 - acc: 0.8667 - val_loss: 1.5780 - val_acc: 0.5750\n",
      "Epoch 67/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2896 - acc: 0.8458 - val_loss: 1.4976 - val_acc: 0.5500\n",
      "Epoch 68/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.2649 - acc: 0.8583 - val_loss: 1.5618 - val_acc: 0.5875\n",
      "Epoch 69/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2597 - acc: 0.9042 - val_loss: 1.5872 - val_acc: 0.6000\n",
      "Epoch 70/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.2490 - acc: 0.9000 - val_loss: 1.6226 - val_acc: 0.5625\n",
      "Epoch 71/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2518 - acc: 0.8917 - val_loss: 1.5917 - val_acc: 0.5375\n",
      "Epoch 72/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2449 - acc: 0.8750 - val_loss: 1.6364 - val_acc: 0.5875\n",
      "Epoch 73/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2323 - acc: 0.9000 - val_loss: 1.6241 - val_acc: 0.5250\n",
      "Epoch 74/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2412 - acc: 0.9042 - val_loss: 1.6251 - val_acc: 0.5750\n",
      "Epoch 75/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2265 - acc: 0.8917 - val_loss: 1.6561 - val_acc: 0.5625\n",
      "Epoch 76/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3120 - acc: 0.8250 - val_loss: 1.6611 - val_acc: 0.5500\n",
      "Epoch 77/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.2787 - acc: 0.8542 - val_loss: 1.5770 - val_acc: 0.5750\n",
      "Epoch 78/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2362 - acc: 0.8917 - val_loss: 1.5882 - val_acc: 0.5125\n",
      "Epoch 79/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2601 - acc: 0.8958 - val_loss: 1.6715 - val_acc: 0.5375\n",
      "Epoch 80/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2516 - acc: 0.8583 - val_loss: 1.6407 - val_acc: 0.5375\n",
      "Epoch 81/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.2262 - acc: 0.9083 - val_loss: 1.7107 - val_acc: 0.6125\n",
      "Epoch 82/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2320 - acc: 0.8833 - val_loss: 1.6697 - val_acc: 0.6000\n",
      "Epoch 83/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.2756 - acc: 0.8875 - val_loss: 1.8705 - val_acc: 0.4625\n",
      "Epoch 84/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.2465 - acc: 0.8917 - val_loss: 1.6911 - val_acc: 0.5250\n",
      "Epoch 85/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2101 - acc: 0.9167 - val_loss: 1.6765 - val_acc: 0.5750\n",
      "Epoch 86/500\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.2263 - acc: 0.9000 - val_loss: 1.6975 - val_acc: 0.5750\n",
      "Epoch 87/500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.2491 - acc: 0.8792 - val_loss: 1.6205 - val_acc: 0.5750\n",
      "Epoch 88/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2276 - acc: 0.9083 - val_loss: 1.6782 - val_acc: 0.5625\n",
      "Epoch 89/500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3002 - acc: 0.8208 - val_loss: 1.7130 - val_acc: 0.5625\n",
      "Epoch 90/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2434 - acc: 0.8875 - val_loss: 1.6881 - val_acc: 0.5625\n",
      "Epoch 91/500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.2307 - acc: 0.8917 - val_loss: 1.7124 - val_acc: 0.5750\n",
      "Epoch 92/500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.2470 - acc: 0.8667 - val_loss: 1.7786 - val_acc: 0.5500\n",
      "Epoch 93/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2419 - acc: 0.8667 - val_loss: 1.7501 - val_acc: 0.5750\n",
      "Epoch 94/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2100 - acc: 0.9042 - val_loss: 1.7530 - val_acc: 0.5875\n",
      "Epoch 95/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2113 - acc: 0.9083 - val_loss: 1.9619 - val_acc: 0.5125\n",
      "Epoch 96/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2824 - acc: 0.8750 - val_loss: 1.8450 - val_acc: 0.5250\n",
      "Epoch 97/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.2161 - acc: 0.9250 - val_loss: 1.8255 - val_acc: 0.5875\n",
      "Epoch 98/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.2457 - acc: 0.8542 - val_loss: 1.9471 - val_acc: 0.5750\n",
      "Epoch 99/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2577 - acc: 0.8750 - val_loss: 1.8558 - val_acc: 0.5750\n",
      "Epoch 100/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.1991 - acc: 0.9042 - val_loss: 1.8035 - val_acc: 0.5750\n",
      "Epoch 101/500\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.2151 - acc: 0.9042 - val_loss: 1.8129 - val_acc: 0.6000\n",
      "Epoch 102/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.2049 - acc: 0.8917 - val_loss: 1.8585 - val_acc: 0.5625\n",
      "Epoch 103/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2164 - acc: 0.8833 - val_loss: 1.8069 - val_acc: 0.6000\n",
      "Epoch 104/500\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.1839 - acc: 0.9292 - val_loss: 1.8652 - val_acc: 0.5750\n",
      "Epoch 105/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2356 - acc: 0.8833 - val_loss: 1.8650 - val_acc: 0.6000\n",
      "Epoch 106/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.1992 - acc: 0.9083 - val_loss: 1.8455 - val_acc: 0.5750\n",
      "Epoch 107/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.2206 - acc: 0.8917 - val_loss: 1.9291 - val_acc: 0.5250\n",
      "Epoch 108/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.2563 - acc: 0.8667 - val_loss: 2.0997 - val_acc: 0.4875\n",
      "Epoch 109/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3106 - acc: 0.8208 - val_loss: 1.8184 - val_acc: 0.6000\n",
      "Epoch 110/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2087 - acc: 0.8792 - val_loss: 1.8489 - val_acc: 0.6000\n",
      "Epoch 111/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.1818 - acc: 0.9208 - val_loss: 1.8432 - val_acc: 0.5750\n",
      "Epoch 112/500\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.1739 - acc: 0.9417 - val_loss: 1.9050 - val_acc: 0.5750\n",
      "Epoch 113/500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.1969 - acc: 0.9042 - val_loss: 1.9670 - val_acc: 0.5375\n",
      "Epoch 114/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.1857 - acc: 0.9000 - val_loss: 1.9192 - val_acc: 0.5750\n",
      "Epoch 115/500\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.1915 - acc: 0.9167 - val_loss: 1.9628 - val_acc: 0.6000\n",
      "Epoch 116/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2092 - acc: 0.9083 - val_loss: 1.9329 - val_acc: 0.5750\n",
      "Epoch 117/500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.2097 - acc: 0.9000 - val_loss: 2.0860 - val_acc: 0.5875\n",
      "Epoch 118/500\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.2424 - acc: 0.8708 - val_loss: 2.0250 - val_acc: 0.5875\n",
      "Epoch 119/500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.1880 - acc: 0.9042 - val_loss: 2.0794 - val_acc: 0.5250\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 142us/step - loss: 0.2202 - acc: 0.8958 - val_loss: 1.9111 - val_acc: 0.5625\n",
      "Epoch 121/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2117 - acc: 0.9125 - val_loss: 2.0488 - val_acc: 0.6000\n",
      "Epoch 122/500\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.1814 - acc: 0.9292 - val_loss: 2.0296 - val_acc: 0.6000\n",
      "Epoch 123/500\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.1759 - acc: 0.9208 - val_loss: 2.0388 - val_acc: 0.5125\n",
      "Epoch 124/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.2192 - acc: 0.8833 - val_loss: 2.1551 - val_acc: 0.5500\n",
      "Epoch 125/500\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.1766 - acc: 0.9208 - val_loss: 1.9812 - val_acc: 0.5750\n",
      "Epoch 126/500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.2040 - acc: 0.9167 - val_loss: 1.8890 - val_acc: 0.5500\n",
      "Epoch 127/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.2620 - acc: 0.8583 - val_loss: 2.0139 - val_acc: 0.5625\n",
      "Epoch 128/500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.2041 - acc: 0.9167 - val_loss: 1.9542 - val_acc: 0.5875\n",
      "Epoch 129/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1711 - acc: 0.9167 - val_loss: 2.0404 - val_acc: 0.5750\n",
      "Epoch 130/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.1637 - acc: 0.9292 - val_loss: 2.1435 - val_acc: 0.6000\n",
      "Epoch 131/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.1750 - acc: 0.9167 - val_loss: 2.0211 - val_acc: 0.5500\n",
      "Epoch 132/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.1629 - acc: 0.9417 - val_loss: 2.0392 - val_acc: 0.6000\n",
      "Epoch 133/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.1522 - acc: 0.9333 - val_loss: 2.1259 - val_acc: 0.5125\n",
      "Epoch 134/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.1932 - acc: 0.8833 - val_loss: 2.1023 - val_acc: 0.5625\n",
      "Epoch 135/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.1997 - acc: 0.9000 - val_loss: 2.0786 - val_acc: 0.5375\n",
      "Epoch 136/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.2140 - acc: 0.8708 - val_loss: 2.0871 - val_acc: 0.5125\n",
      "Epoch 137/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.1657 - acc: 0.9292 - val_loss: 2.0479 - val_acc: 0.5750\n",
      "Epoch 138/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.1604 - acc: 0.9333 - val_loss: 2.1084 - val_acc: 0.5625\n",
      "Epoch 139/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.1743 - acc: 0.9208 - val_loss: 2.1784 - val_acc: 0.5625\n",
      "Epoch 140/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.1720 - acc: 0.9250 - val_loss: 2.1102 - val_acc: 0.5500\n",
      "Epoch 141/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1508 - acc: 0.9208 - val_loss: 2.1023 - val_acc: 0.5750\n",
      "Epoch 142/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.1893 - acc: 0.9125 - val_loss: 2.2201 - val_acc: 0.5250\n",
      "Epoch 143/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.2136 - acc: 0.9125 - val_loss: 2.0677 - val_acc: 0.5750\n",
      "Epoch 144/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.1801 - acc: 0.8958 - val_loss: 2.1700 - val_acc: 0.5000\n",
      "Epoch 145/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2803 - acc: 0.8375 - val_loss: 2.0802 - val_acc: 0.5625\n",
      "Epoch 146/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.1781 - acc: 0.9208 - val_loss: 2.0701 - val_acc: 0.5500\n",
      "Epoch 147/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.1537 - acc: 0.9250 - val_loss: 2.1266 - val_acc: 0.5500\n",
      "Epoch 148/500\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.1789 - acc: 0.9167 - val_loss: 2.1051 - val_acc: 0.5500\n",
      "Epoch 149/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.1526 - acc: 0.9292 - val_loss: 2.1858 - val_acc: 0.6000\n",
      "Epoch 150/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.1649 - acc: 0.9250 - val_loss: 2.1697 - val_acc: 0.5500\n",
      "Epoch 151/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2003 - acc: 0.9000 - val_loss: 2.2479 - val_acc: 0.5375\n",
      "Epoch 152/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.1895 - acc: 0.9208 - val_loss: 2.1527 - val_acc: 0.5625\n",
      "Epoch 153/500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.1491 - acc: 0.9333 - val_loss: 2.2069 - val_acc: 0.5500\n",
      "Epoch 154/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.1762 - acc: 0.9375 - val_loss: 2.2749 - val_acc: 0.5375\n",
      "Epoch 155/500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.1576 - acc: 0.9167 - val_loss: 2.1943 - val_acc: 0.5500\n",
      "Epoch 156/500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.1526 - acc: 0.9417 - val_loss: 2.3082 - val_acc: 0.5625\n",
      "Epoch 157/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1944 - acc: 0.8917 - val_loss: 2.2444 - val_acc: 0.6000\n",
      "Epoch 158/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1517 - acc: 0.9583 - val_loss: 2.1986 - val_acc: 0.5875\n",
      "Epoch 159/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.1375 - acc: 0.9583 - val_loss: 2.2239 - val_acc: 0.5500\n",
      "Epoch 160/500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.1649 - acc: 0.9208 - val_loss: 2.2536 - val_acc: 0.5500\n",
      "Epoch 161/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.1812 - acc: 0.8958 - val_loss: 2.2738 - val_acc: 0.5750\n",
      "Epoch 162/500\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.1619 - acc: 0.9292 - val_loss: 2.2378 - val_acc: 0.5750\n",
      "Epoch 163/500\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.1407 - acc: 0.9417 - val_loss: 2.3460 - val_acc: 0.5750\n",
      "Epoch 164/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.1760 - acc: 0.9250 - val_loss: 2.4896 - val_acc: 0.6000\n",
      "Epoch 165/500\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.2211 - acc: 0.8958 - val_loss: 2.2520 - val_acc: 0.5875\n",
      "Epoch 166/500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.1445 - acc: 0.9417 - val_loss: 2.1832 - val_acc: 0.5500\n",
      "Epoch 167/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.1315 - acc: 0.9458 - val_loss: 2.3038 - val_acc: 0.5750\n",
      "Epoch 168/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.1503 - acc: 0.9292 - val_loss: 2.2444 - val_acc: 0.5750\n",
      "Epoch 169/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.1400 - acc: 0.9500 - val_loss: 2.3133 - val_acc: 0.5750\n",
      "Epoch 170/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.1537 - acc: 0.9292 - val_loss: 2.4815 - val_acc: 0.5250\n",
      "Epoch 171/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.1467 - acc: 0.9333 - val_loss: 2.3967 - val_acc: 0.5750\n",
      "Epoch 172/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.1271 - acc: 0.9333 - val_loss: 2.2779 - val_acc: 0.5750\n",
      "Epoch 173/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1455 - acc: 0.9167 - val_loss: 2.3688 - val_acc: 0.5250\n",
      "Epoch 174/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.1314 - acc: 0.9500 - val_loss: 2.5280 - val_acc: 0.5375\n",
      "Epoch 175/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2199 - acc: 0.8833 - val_loss: 2.2428 - val_acc: 0.5750\n",
      "Epoch 176/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.1630 - acc: 0.9083 - val_loss: 2.4796 - val_acc: 0.5625\n",
      "Epoch 177/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.1662 - acc: 0.9333 - val_loss: 2.9437 - val_acc: 0.4875\n",
      "Epoch 178/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3629 - acc: 0.9000 - val_loss: 2.3844 - val_acc: 0.5500\n",
      "Epoch 179/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.1299 - acc: 0.9458 - val_loss: 2.3080 - val_acc: 0.5500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.1373 - acc: 0.9500 - val_loss: 2.3007 - val_acc: 0.5500\n",
      "Epoch 181/500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.1275 - acc: 0.9375 - val_loss: 2.3995 - val_acc: 0.5500\n",
      "Epoch 182/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.1790 - acc: 0.9125 - val_loss: 2.3623 - val_acc: 0.5875\n",
      "Epoch 183/500\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.1475 - acc: 0.9458 - val_loss: 2.3218 - val_acc: 0.5500\n",
      "Epoch 184/500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.1575 - acc: 0.9292 - val_loss: 2.2671 - val_acc: 0.5750\n",
      "Epoch 185/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.1205 - acc: 0.9667 - val_loss: 2.3344 - val_acc: 0.5500\n",
      "Epoch 186/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.1222 - acc: 0.9500 - val_loss: 2.3846 - val_acc: 0.5500\n",
      "Epoch 187/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.1555 - acc: 0.9250 - val_loss: 2.4104 - val_acc: 0.5500\n",
      "Epoch 188/500\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.1181 - acc: 0.9542 - val_loss: 2.4327 - val_acc: 0.5625\n",
      "Epoch 189/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.1330 - acc: 0.9375 - val_loss: 2.3763 - val_acc: 0.5500\n",
      "Epoch 190/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.1792 - acc: 0.9167 - val_loss: 2.4098 - val_acc: 0.5000\n",
      "Epoch 191/500\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.1777 - acc: 0.9125 - val_loss: 2.3323 - val_acc: 0.5375\n",
      "Epoch 192/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.1543 - acc: 0.9167 - val_loss: 2.3387 - val_acc: 0.5625\n",
      "Epoch 193/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1080 - acc: 0.9750 - val_loss: 2.3662 - val_acc: 0.5625\n",
      "Epoch 194/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.1120 - acc: 0.9500 - val_loss: 2.3709 - val_acc: 0.5500\n",
      "Epoch 195/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1190 - acc: 0.9458 - val_loss: 2.3348 - val_acc: 0.5500\n",
      "Epoch 196/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.1836 - acc: 0.8792 - val_loss: 2.4614 - val_acc: 0.5250\n",
      "Epoch 197/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1986 - acc: 0.9083 - val_loss: 2.2799 - val_acc: 0.5500\n",
      "Epoch 198/500\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.1270 - acc: 0.9542 - val_loss: 2.3079 - val_acc: 0.5500\n",
      "Epoch 199/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1146 - acc: 0.9458 - val_loss: 2.3620 - val_acc: 0.5625\n",
      "Epoch 200/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1458 - acc: 0.9208 - val_loss: 2.4004 - val_acc: 0.5750\n",
      "Epoch 201/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.1047 - acc: 0.9667 - val_loss: 2.3886 - val_acc: 0.5500\n",
      "Epoch 202/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.1312 - acc: 0.9333 - val_loss: 2.3214 - val_acc: 0.5500\n",
      "Epoch 203/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.1374 - acc: 0.9375 - val_loss: 2.4803 - val_acc: 0.5875\n",
      "Epoch 204/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.1297 - acc: 0.9625 - val_loss: 2.3934 - val_acc: 0.5625\n",
      "Epoch 205/500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.1227 - acc: 0.9583 - val_loss: 2.3382 - val_acc: 0.5625\n",
      "Epoch 206/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.1374 - acc: 0.9583 - val_loss: 2.4170 - val_acc: 0.5250\n",
      "Epoch 207/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.1073 - acc: 0.9625 - val_loss: 2.4031 - val_acc: 0.5875\n",
      "Epoch 208/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.1126 - acc: 0.9417 - val_loss: 2.4092 - val_acc: 0.5375\n",
      "Epoch 209/500\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.1432 - acc: 0.9208 - val_loss: 2.3823 - val_acc: 0.5500\n",
      "Epoch 210/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0979 - acc: 0.9667 - val_loss: 2.4675 - val_acc: 0.5625\n",
      "Epoch 211/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0965 - acc: 0.9583 - val_loss: 2.5369 - val_acc: 0.5875\n",
      "Epoch 212/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1260 - acc: 0.9333 - val_loss: 2.5373 - val_acc: 0.5625\n",
      "Epoch 213/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1159 - acc: 0.9500 - val_loss: 2.5174 - val_acc: 0.5000\n",
      "Epoch 214/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.1188 - acc: 0.9417 - val_loss: 2.4810 - val_acc: 0.6000\n",
      "Epoch 215/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.1095 - acc: 0.9583 - val_loss: 2.5361 - val_acc: 0.5625\n",
      "Epoch 216/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.1238 - acc: 0.9500 - val_loss: 2.5719 - val_acc: 0.5750\n",
      "Epoch 217/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.1528 - acc: 0.9208 - val_loss: 2.4574 - val_acc: 0.5625\n",
      "Epoch 218/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1465 - acc: 0.9458 - val_loss: 2.8536 - val_acc: 0.4875\n",
      "Epoch 219/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.2638 - acc: 0.8792 - val_loss: 2.8953 - val_acc: 0.5250\n",
      "Epoch 220/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3280 - acc: 0.9292 - val_loss: 2.5288 - val_acc: 0.5750\n",
      "Epoch 221/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.1162 - acc: 0.9583 - val_loss: 2.6928 - val_acc: 0.5750\n",
      "Epoch 222/500\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.1089 - acc: 0.9458 - val_loss: 2.6120 - val_acc: 0.5625\n",
      "Epoch 223/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0862 - acc: 0.9750 - val_loss: 2.6131 - val_acc: 0.5750\n",
      "Epoch 224/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.1042 - acc: 0.9583 - val_loss: 2.5965 - val_acc: 0.5875\n",
      "Epoch 225/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.1090 - acc: 0.9667 - val_loss: 2.6781 - val_acc: 0.5750\n",
      "Epoch 226/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.1046 - acc: 0.9375 - val_loss: 2.5372 - val_acc: 0.6000\n",
      "Epoch 227/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0987 - acc: 0.9583 - val_loss: 2.6836 - val_acc: 0.5750\n",
      "Epoch 228/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0990 - acc: 0.9500 - val_loss: 2.6644 - val_acc: 0.5625\n",
      "Epoch 229/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1223 - acc: 0.9583 - val_loss: 2.5048 - val_acc: 0.5500\n",
      "Epoch 230/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.1372 - acc: 0.9292 - val_loss: 2.5100 - val_acc: 0.6000\n",
      "Epoch 231/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2220 - acc: 0.8750 - val_loss: 2.5361 - val_acc: 0.5500\n",
      "Epoch 232/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.1283 - acc: 0.9500 - val_loss: 2.4032 - val_acc: 0.5750\n",
      "Epoch 233/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1264 - acc: 0.9333 - val_loss: 2.5143 - val_acc: 0.5125\n",
      "Epoch 234/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.1464 - acc: 0.9292 - val_loss: 2.4186 - val_acc: 0.6000\n",
      "Epoch 235/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0827 - acc: 0.9750 - val_loss: 2.4881 - val_acc: 0.5750\n",
      "Epoch 236/500\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.0915 - acc: 0.9750 - val_loss: 2.5463 - val_acc: 0.5375\n",
      "Epoch 237/500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.1369 - acc: 0.9583 - val_loss: 2.5052 - val_acc: 0.5750\n",
      "Epoch 238/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0945 - acc: 0.9625 - val_loss: 2.3880 - val_acc: 0.5875\n",
      "Epoch 239/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0982 - acc: 0.9708 - val_loss: 2.4998 - val_acc: 0.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1037 - acc: 0.9583 - val_loss: 2.6874 - val_acc: 0.5375\n",
      "Epoch 241/500\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.1445 - acc: 0.9292 - val_loss: 2.4785 - val_acc: 0.5625\n",
      "Epoch 242/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0846 - acc: 0.9583 - val_loss: 2.4936 - val_acc: 0.5750\n",
      "Epoch 243/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.1181 - acc: 0.9417 - val_loss: 2.5095 - val_acc: 0.5750\n",
      "Epoch 244/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0995 - acc: 0.9708 - val_loss: 2.6449 - val_acc: 0.5750\n",
      "Epoch 245/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.1599 - acc: 0.9000 - val_loss: 2.5806 - val_acc: 0.5750\n",
      "Epoch 246/500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.1681 - acc: 0.9167 - val_loss: 2.4394 - val_acc: 0.6000\n",
      "Epoch 247/500\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.0820 - acc: 0.9833 - val_loss: 2.4654 - val_acc: 0.5750\n",
      "Epoch 248/500\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.0704 - acc: 0.9917 - val_loss: 2.5556 - val_acc: 0.5750\n",
      "Epoch 249/500\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.0663 - acc: 0.9833 - val_loss: 2.5342 - val_acc: 0.5750\n",
      "Epoch 250/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0963 - acc: 0.9542 - val_loss: 2.5856 - val_acc: 0.5875\n",
      "Epoch 251/500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.1186 - acc: 0.9500 - val_loss: 2.5894 - val_acc: 0.5750\n",
      "Epoch 252/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.1164 - acc: 0.9375 - val_loss: 2.5692 - val_acc: 0.5375\n",
      "Epoch 253/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.1088 - acc: 0.9583 - val_loss: 2.5319 - val_acc: 0.6000\n",
      "Epoch 254/500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.1076 - acc: 0.9417 - val_loss: 2.4464 - val_acc: 0.5750\n",
      "Epoch 255/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0778 - acc: 0.9833 - val_loss: 2.4734 - val_acc: 0.5750\n",
      "Epoch 256/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0742 - acc: 0.9750 - val_loss: 2.5562 - val_acc: 0.5750\n",
      "Epoch 257/500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.0940 - acc: 0.9333 - val_loss: 2.5657 - val_acc: 0.5875\n",
      "Epoch 258/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.1320 - acc: 0.9417 - val_loss: 2.5097 - val_acc: 0.5750\n",
      "Epoch 259/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0863 - acc: 0.9667 - val_loss: 2.5881 - val_acc: 0.5875\n",
      "Epoch 260/500\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.0926 - acc: 0.9708 - val_loss: 2.5378 - val_acc: 0.5500\n",
      "Epoch 261/500\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.0987 - acc: 0.9667 - val_loss: 2.6725 - val_acc: 0.5875\n",
      "Epoch 262/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.1577 - acc: 0.9167 - val_loss: 2.7082 - val_acc: 0.5250\n",
      "Epoch 263/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.1128 - acc: 0.9583 - val_loss: 2.4852 - val_acc: 0.5750\n",
      "Epoch 264/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0859 - acc: 0.9750 - val_loss: 2.6047 - val_acc: 0.5625\n",
      "Epoch 265/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0749 - acc: 0.9625 - val_loss: 2.6034 - val_acc: 0.5750\n",
      "Epoch 266/500\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.0820 - acc: 0.9625 - val_loss: 2.6789 - val_acc: 0.5625\n",
      "Epoch 267/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0841 - acc: 0.9833 - val_loss: 2.6815 - val_acc: 0.5500\n",
      "Epoch 268/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0791 - acc: 0.9708 - val_loss: 2.5957 - val_acc: 0.5750\n",
      "Epoch 269/500\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.0885 - acc: 0.9500 - val_loss: 2.5374 - val_acc: 0.6125\n",
      "Epoch 270/500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.0872 - acc: 0.9750 - val_loss: 2.8038 - val_acc: 0.5750\n",
      "Epoch 271/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0825 - acc: 0.9667 - val_loss: 2.6563 - val_acc: 0.5750\n",
      "Epoch 272/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0582 - acc: 0.9917 - val_loss: 2.6811 - val_acc: 0.5625\n",
      "Epoch 273/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0762 - acc: 0.9583 - val_loss: 3.0415 - val_acc: 0.5125\n",
      "Epoch 274/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4026 - acc: 0.8375 - val_loss: 2.8066 - val_acc: 0.5500\n",
      "Epoch 275/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.1223 - acc: 0.9625 - val_loss: 2.7726 - val_acc: 0.5500\n",
      "Epoch 276/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0743 - acc: 0.9833 - val_loss: 2.7334 - val_acc: 0.5500\n",
      "Epoch 277/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0625 - acc: 0.9750 - val_loss: 2.8009 - val_acc: 0.5500\n",
      "Epoch 278/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0525 - acc: 0.9875 - val_loss: 2.7994 - val_acc: 0.5500\n",
      "Epoch 279/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0738 - acc: 0.9708 - val_loss: 2.8467 - val_acc: 0.5500\n",
      "Epoch 280/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0971 - acc: 0.9667 - val_loss: 2.7974 - val_acc: 0.5750\n",
      "Epoch 281/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0765 - acc: 0.9667 - val_loss: 2.9783 - val_acc: 0.5625\n",
      "Epoch 282/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0631 - acc: 0.9833 - val_loss: 2.9511 - val_acc: 0.5500\n",
      "Epoch 283/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0558 - acc: 0.9833 - val_loss: 2.9492 - val_acc: 0.5500\n",
      "Epoch 284/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0528 - acc: 0.9917 - val_loss: 3.0048 - val_acc: 0.5750\n",
      "Epoch 285/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0616 - acc: 0.9792 - val_loss: 2.9234 - val_acc: 0.5125\n",
      "Epoch 286/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0657 - acc: 0.9708 - val_loss: 2.8648 - val_acc: 0.5500\n",
      "Epoch 287/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0768 - acc: 0.9750 - val_loss: 3.0386 - val_acc: 0.5625\n",
      "Epoch 288/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0738 - acc: 0.9708 - val_loss: 2.9013 - val_acc: 0.5750\n",
      "Epoch 289/500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0857 - acc: 0.9667 - val_loss: 2.8981 - val_acc: 0.6000\n",
      "Epoch 290/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.1130 - acc: 0.9417 - val_loss: 3.0477 - val_acc: 0.5500\n",
      "Epoch 291/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.1283 - acc: 0.9292 - val_loss: 2.8291 - val_acc: 0.5500\n",
      "Epoch 292/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0924 - acc: 0.9667 - val_loss: 2.7448 - val_acc: 0.5875\n",
      "Epoch 293/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0626 - acc: 0.9917 - val_loss: 2.8113 - val_acc: 0.5750\n",
      "Epoch 294/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0497 - acc: 0.9917 - val_loss: 2.7930 - val_acc: 0.5875\n",
      "Epoch 295/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0552 - acc: 0.9750 - val_loss: 2.8790 - val_acc: 0.5875\n",
      "Epoch 296/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0470 - acc: 0.9917 - val_loss: 3.0121 - val_acc: 0.5375\n",
      "Epoch 297/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0448 - acc: 0.9958 - val_loss: 2.8259 - val_acc: 0.5750\n",
      "Epoch 298/500\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.0713 - acc: 0.9750 - val_loss: 2.9712 - val_acc: 0.5625\n",
      "Epoch 299/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0479 - acc: 0.9875 - val_loss: 2.9850 - val_acc: 0.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0482 - acc: 0.9875 - val_loss: 3.0794 - val_acc: 0.5625\n",
      "Epoch 301/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0694 - acc: 0.9750 - val_loss: 3.1126 - val_acc: 0.5500\n",
      "Epoch 302/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0726 - acc: 0.9833 - val_loss: 2.9141 - val_acc: 0.5750\n",
      "Epoch 303/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0858 - acc: 0.9667 - val_loss: 3.2044 - val_acc: 0.5750\n",
      "Epoch 304/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2028 - acc: 0.9083 - val_loss: 3.0196 - val_acc: 0.5250\n",
      "Epoch 305/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0965 - acc: 0.9625 - val_loss: 2.9362 - val_acc: 0.5750\n",
      "Epoch 306/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0790 - acc: 0.9667 - val_loss: 2.7628 - val_acc: 0.5750\n",
      "Epoch 307/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0415 - acc: 0.9917 - val_loss: 2.8720 - val_acc: 0.5750\n",
      "Epoch 308/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0363 - acc: 1.0000 - val_loss: 2.9254 - val_acc: 0.5500\n",
      "Epoch 309/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0362 - acc: 1.0000 - val_loss: 2.9008 - val_acc: 0.5750\n",
      "Epoch 310/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0334 - acc: 0.9958 - val_loss: 2.9351 - val_acc: 0.5500\n",
      "Epoch 311/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0376 - acc: 0.9875 - val_loss: 3.0611 - val_acc: 0.5500\n",
      "Epoch 312/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.0760 - acc: 0.9750 - val_loss: 3.0895 - val_acc: 0.5750\n",
      "Epoch 313/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.1145 - acc: 0.9250 - val_loss: 3.1689 - val_acc: 0.5625\n",
      "Epoch 314/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.1328 - acc: 0.9500 - val_loss: 3.0037 - val_acc: 0.5500\n",
      "Epoch 315/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0452 - acc: 0.9917 - val_loss: 2.9583 - val_acc: 0.5500\n",
      "Epoch 316/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0429 - acc: 0.9917 - val_loss: 3.0493 - val_acc: 0.5625\n",
      "Epoch 317/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0490 - acc: 0.9833 - val_loss: 2.9986 - val_acc: 0.5875\n",
      "Epoch 318/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0422 - acc: 0.9833 - val_loss: 3.0417 - val_acc: 0.5500\n",
      "Epoch 319/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 3.1363 - val_acc: 0.6000\n",
      "Epoch 320/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0515 - acc: 0.9917 - val_loss: 3.0675 - val_acc: 0.5750\n",
      "Epoch 321/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.0415 - acc: 0.9958 - val_loss: 3.2479 - val_acc: 0.5750\n",
      "Epoch 322/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0554 - acc: 0.9750 - val_loss: 3.1573 - val_acc: 0.5875\n",
      "Epoch 323/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0467 - acc: 0.9917 - val_loss: 3.1135 - val_acc: 0.5750\n",
      "Epoch 324/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0715 - acc: 0.9792 - val_loss: 3.2363 - val_acc: 0.5750\n",
      "Epoch 325/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2620 - acc: 0.8542 - val_loss: 3.2453 - val_acc: 0.5250\n",
      "Epoch 326/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0802 - acc: 0.9792 - val_loss: 2.9400 - val_acc: 0.5250\n",
      "Epoch 327/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0486 - acc: 0.9917 - val_loss: 2.9313 - val_acc: 0.5750\n",
      "Epoch 328/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 2.9648 - val_acc: 0.5625\n",
      "Epoch 329/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0352 - acc: 0.9917 - val_loss: 3.0356 - val_acc: 0.5625\n",
      "Epoch 330/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0267 - acc: 1.0000 - val_loss: 3.0696 - val_acc: 0.5750\n",
      "Epoch 331/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0334 - acc: 0.9833 - val_loss: 3.1215 - val_acc: 0.5625\n",
      "Epoch 332/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2247 - acc: 0.9000 - val_loss: 3.3309 - val_acc: 0.5875\n",
      "Epoch 333/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.0755 - acc: 0.9667 - val_loss: 3.0589 - val_acc: 0.5750\n",
      "Epoch 334/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0352 - acc: 0.9833 - val_loss: 3.0427 - val_acc: 0.5500\n",
      "Epoch 335/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0339 - acc: 1.0000 - val_loss: 3.1315 - val_acc: 0.5875\n",
      "Epoch 336/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0302 - acc: 1.0000 - val_loss: 3.1380 - val_acc: 0.5625\n",
      "Epoch 337/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0322 - acc: 1.0000 - val_loss: 3.2201 - val_acc: 0.5875\n",
      "Epoch 338/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0296 - acc: 0.9917 - val_loss: 3.1252 - val_acc: 0.5375\n",
      "Epoch 339/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0721 - acc: 0.9750 - val_loss: 3.2240 - val_acc: 0.5125\n",
      "Epoch 340/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.1482 - acc: 0.9250 - val_loss: 3.2038 - val_acc: 0.5500\n",
      "Epoch 341/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2012 - acc: 0.9375 - val_loss: 3.1884 - val_acc: 0.5125\n",
      "Epoch 342/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3541 - acc: 0.9375 - val_loss: 3.1323 - val_acc: 0.5500\n",
      "Epoch 343/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2997 - acc: 0.9833 - val_loss: 3.1611 - val_acc: 0.5750\n",
      "Epoch 344/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2984 - acc: 0.9750 - val_loss: 3.1521 - val_acc: 0.5625\n",
      "Epoch 345/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3009 - acc: 0.9750 - val_loss: 3.1408 - val_acc: 0.5500\n",
      "Epoch 346/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2929 - acc: 0.9833 - val_loss: 3.1772 - val_acc: 0.5625\n",
      "Epoch 347/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2957 - acc: 0.9750 - val_loss: 3.1714 - val_acc: 0.5500\n",
      "Epoch 348/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2900 - acc: 0.9833 - val_loss: 3.2760 - val_acc: 0.5625\n",
      "Epoch 349/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2963 - acc: 0.9750 - val_loss: 3.1679 - val_acc: 0.5750\n",
      "Epoch 350/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2929 - acc: 0.9792 - val_loss: 3.2216 - val_acc: 0.5750\n",
      "Epoch 351/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3035 - acc: 0.9750 - val_loss: 3.2426 - val_acc: 0.5500\n",
      "Epoch 352/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3044 - acc: 0.9750 - val_loss: 3.2883 - val_acc: 0.5500\n",
      "Epoch 353/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3487 - acc: 0.9500 - val_loss: 3.2550 - val_acc: 0.5500\n",
      "Epoch 354/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3183 - acc: 0.9667 - val_loss: 3.2844 - val_acc: 0.5250\n",
      "Epoch 355/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2957 - acc: 0.9833 - val_loss: 3.2763 - val_acc: 0.5500\n",
      "Epoch 356/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3066 - acc: 0.9667 - val_loss: 3.2733 - val_acc: 0.5500\n",
      "Epoch 357/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3019 - acc: 0.9708 - val_loss: 3.1893 - val_acc: 0.5625\n",
      "Epoch 358/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2959 - acc: 0.9750 - val_loss: 3.2710 - val_acc: 0.6000\n",
      "Epoch 359/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3261 - acc: 0.9458 - val_loss: 3.4207 - val_acc: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4501 - acc: 0.8875 - val_loss: 3.7172 - val_acc: 0.5625\n",
      "Epoch 361/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4487 - acc: 0.9250 - val_loss: 3.2390 - val_acc: 0.5750\n",
      "Epoch 362/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3111 - acc: 0.9708 - val_loss: 3.2039 - val_acc: 0.5625\n",
      "Epoch 363/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2906 - acc: 0.9833 - val_loss: 3.2295 - val_acc: 0.5625\n",
      "Epoch 364/500\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.2882 - acc: 0.9833 - val_loss: 3.2565 - val_acc: 0.5750\n",
      "Epoch 365/500\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.2873 - acc: 0.9833 - val_loss: 3.2728 - val_acc: 0.5625\n",
      "Epoch 366/500\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.2881 - acc: 0.9833 - val_loss: 3.1592 - val_acc: 0.5750\n",
      "Epoch 367/500\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.2866 - acc: 0.9833 - val_loss: 3.2102 - val_acc: 0.5625\n",
      "Epoch 368/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2848 - acc: 0.9833 - val_loss: 3.2463 - val_acc: 0.5750\n",
      "Epoch 369/500\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.2847 - acc: 0.9833 - val_loss: 3.2623 - val_acc: 0.5500\n",
      "Epoch 370/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2842 - acc: 0.9833 - val_loss: 3.3441 - val_acc: 0.5750\n",
      "Epoch 371/500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.2946 - acc: 0.9750 - val_loss: 3.3861 - val_acc: 0.5625\n",
      "Epoch 372/500\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4168 - acc: 0.9250 - val_loss: 3.4353 - val_acc: 0.5875\n",
      "Epoch 373/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4456 - acc: 0.9167 - val_loss: 3.2600 - val_acc: 0.5500\n",
      "Epoch 374/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3438 - acc: 0.9583 - val_loss: 3.3480 - val_acc: 0.5625\n",
      "Epoch 375/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.2944 - acc: 0.9833 - val_loss: 3.2865 - val_acc: 0.5375\n",
      "Epoch 376/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2864 - acc: 0.9833 - val_loss: 3.3449 - val_acc: 0.5625\n",
      "Epoch 377/500\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2906 - acc: 0.9750 - val_loss: 3.3674 - val_acc: 0.5750\n",
      "Epoch 378/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2842 - acc: 0.9833 - val_loss: 3.3642 - val_acc: 0.5750\n",
      "Epoch 379/500\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.2840 - acc: 0.9833 - val_loss: 3.3500 - val_acc: 0.5500\n",
      "Epoch 380/500\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2943 - acc: 0.9833 - val_loss: 3.4285 - val_acc: 0.5375\n",
      "Epoch 381/500\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.2864 - acc: 0.9833 - val_loss: 3.3442 - val_acc: 0.5500\n",
      "Epoch 382/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2808 - acc: 0.9833 - val_loss: 3.3659 - val_acc: 0.5500\n",
      "Epoch 383/500\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.2803 - acc: 0.9833 - val_loss: 3.3837 - val_acc: 0.5500\n",
      "Epoch 384/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2829 - acc: 0.9833 - val_loss: 3.4048 - val_acc: 0.5625\n",
      "Epoch 385/500\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.2823 - acc: 0.9833 - val_loss: 3.3921 - val_acc: 0.5500\n",
      "Epoch 386/500\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.2801 - acc: 0.9833 - val_loss: 3.4269 - val_acc: 0.5750\n",
      "Epoch 387/500\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2806 - acc: 0.9833 - val_loss: 3.4201 - val_acc: 0.5500\n",
      "Epoch 388/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2852 - acc: 0.9833 - val_loss: 3.4501 - val_acc: 0.5375\n",
      "Epoch 389/500\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2922 - acc: 0.9667 - val_loss: 3.4650 - val_acc: 0.5500\n",
      "Epoch 390/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.2994 - acc: 0.9750 - val_loss: 3.4215 - val_acc: 0.5625\n",
      "Epoch 391/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2867 - acc: 0.9833 - val_loss: 3.4208 - val_acc: 0.5500\n",
      "Epoch 392/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2828 - acc: 0.9833 - val_loss: 3.4125 - val_acc: 0.5500\n",
      "Epoch 393/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.2803 - acc: 0.9833 - val_loss: 3.4692 - val_acc: 0.5500\n",
      "Epoch 394/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2813 - acc: 0.9833 - val_loss: 3.4670 - val_acc: 0.5500\n",
      "Epoch 395/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2790 - acc: 0.9833 - val_loss: 3.4747 - val_acc: 0.5500\n",
      "Epoch 396/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.2779 - acc: 0.9833 - val_loss: 3.4976 - val_acc: 0.5500\n",
      "Epoch 397/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2783 - acc: 0.9833 - val_loss: 3.5010 - val_acc: 0.5500\n",
      "Epoch 398/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2804 - acc: 0.9833 - val_loss: 3.4737 - val_acc: 0.5500\n",
      "Epoch 399/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2800 - acc: 0.9833 - val_loss: 3.5332 - val_acc: 0.5500\n",
      "Epoch 400/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2789 - acc: 0.9833 - val_loss: 3.4812 - val_acc: 0.5500\n",
      "Epoch 401/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2771 - acc: 0.9833 - val_loss: 3.5765 - val_acc: 0.5500\n",
      "Epoch 402/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.2766 - acc: 0.9833 - val_loss: 3.5801 - val_acc: 0.5500\n",
      "Epoch 403/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2784 - acc: 0.9833 - val_loss: 3.5928 - val_acc: 0.5750\n",
      "Epoch 404/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2823 - acc: 0.9833 - val_loss: 3.5108 - val_acc: 0.5500\n",
      "Epoch 405/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.2857 - acc: 0.9833 - val_loss: 3.6751 - val_acc: 0.5500\n",
      "Epoch 406/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2950 - acc: 0.9750 - val_loss: 3.5658 - val_acc: 0.5500\n",
      "Epoch 407/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2837 - acc: 0.9833 - val_loss: 3.5831 - val_acc: 0.5750\n",
      "Epoch 408/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2968 - acc: 0.9750 - val_loss: 3.6077 - val_acc: 0.5375\n",
      "Epoch 409/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2916 - acc: 0.9667 - val_loss: 3.5558 - val_acc: 0.5375\n",
      "Epoch 410/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2762 - acc: 0.9833 - val_loss: 3.5749 - val_acc: 0.5500\n",
      "Epoch 411/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2750 - acc: 0.9833 - val_loss: 3.5876 - val_acc: 0.5500\n",
      "Epoch 412/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.2755 - acc: 0.9833 - val_loss: 3.6643 - val_acc: 0.5625\n",
      "Epoch 413/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.2757 - acc: 0.9833 - val_loss: 3.5807 - val_acc: 0.5500\n",
      "Epoch 414/500\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.2786 - acc: 0.9833 - val_loss: 3.5942 - val_acc: 0.5500\n",
      "Epoch 415/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.2764 - acc: 0.9833 - val_loss: 3.6592 - val_acc: 0.5875\n",
      "Epoch 416/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2795 - acc: 0.9833 - val_loss: 3.7609 - val_acc: 0.5750\n",
      "Epoch 417/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2829 - acc: 0.9833 - val_loss: 3.5600 - val_acc: 0.6125\n",
      "Epoch 418/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2864 - acc: 0.9833 - val_loss: 3.5356 - val_acc: 0.5750\n",
      "Epoch 419/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2878 - acc: 0.9833 - val_loss: 3.6084 - val_acc: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 420/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2789 - acc: 0.9833 - val_loss: 3.6022 - val_acc: 0.5500\n",
      "Epoch 421/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.2771 - acc: 0.9833 - val_loss: 3.6371 - val_acc: 0.5500\n",
      "Epoch 422/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.2751 - acc: 0.9833 - val_loss: 3.6354 - val_acc: 0.5500\n",
      "Epoch 423/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2739 - acc: 0.9833 - val_loss: 3.6382 - val_acc: 0.5500\n",
      "Epoch 424/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2744 - acc: 0.9833 - val_loss: 3.6620 - val_acc: 0.5625\n",
      "Epoch 425/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.2747 - acc: 0.9833 - val_loss: 3.6651 - val_acc: 0.5750\n",
      "Epoch 426/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.2750 - acc: 0.9833 - val_loss: 3.7833 - val_acc: 0.5500\n",
      "Epoch 427/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.2954 - acc: 0.9750 - val_loss: 3.7267 - val_acc: 0.5625\n",
      "Epoch 428/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3064 - acc: 0.9583 - val_loss: 3.7538 - val_acc: 0.5625\n",
      "Epoch 429/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3301 - acc: 0.9583 - val_loss: 3.3323 - val_acc: 0.6125\n",
      "Epoch 430/500\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 3.3544 - val_acc: 0.6000\n",
      "Epoch 431/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 3.3720 - val_acc: 0.6000\n",
      "Epoch 432/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 3.3460 - val_acc: 0.6000\n",
      "Epoch 433/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.3596 - val_acc: 0.5750\n",
      "Epoch 434/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.3745 - val_acc: 0.5750\n",
      "Epoch 435/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.3465 - val_acc: 0.5875\n",
      "Epoch 436/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 3.3788 - val_acc: 0.5875\n",
      "Epoch 437/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3924 - val_acc: 0.5750\n",
      "Epoch 438/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3789 - val_acc: 0.5875\n",
      "Epoch 439/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.3982 - val_acc: 0.5750\n",
      "Epoch 440/500\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.4055 - val_acc: 0.5750\n",
      "Epoch 441/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4272 - val_acc: 0.5750\n",
      "Epoch 442/500\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.4387 - val_acc: 0.5750\n",
      "Epoch 443/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 3.4212 - val_acc: 0.5750\n",
      "Epoch 444/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 3.3949 - val_acc: 0.6000\n",
      "Epoch 445/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 3.4577 - val_acc: 0.5750\n",
      "Epoch 446/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0293 - acc: 0.9917 - val_loss: 3.4550 - val_acc: 0.6000\n",
      "Epoch 447/500\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 3.4322 - val_acc: 0.5750\n",
      "Epoch 448/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.4288 - val_acc: 0.5750\n",
      "Epoch 449/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4443 - val_acc: 0.5750\n",
      "Epoch 450/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4726 - val_acc: 0.5750\n",
      "Epoch 451/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 3.4787 - val_acc: 0.5750\n",
      "Epoch 452/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 3.4658 - val_acc: 0.5750\n",
      "Epoch 453/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.5182 - val_acc: 0.5750\n",
      "Epoch 454/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4621 - val_acc: 0.5750\n",
      "Epoch 455/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 3.4751 - val_acc: 0.5875\n",
      "Epoch 456/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.5332 - val_acc: 0.5750\n",
      "Epoch 457/500\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.4248 - val_acc: 0.5750\n",
      "Epoch 458/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 3.6903 - val_acc: 0.5875\n",
      "Epoch 459/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0148 - acc: 0.9917 - val_loss: 3.6874 - val_acc: 0.5750\n",
      "Epoch 460/500\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.0652 - acc: 0.9750 - val_loss: 3.5494 - val_acc: 0.5875\n",
      "Epoch 461/500\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.0231 - acc: 0.9917 - val_loss: 3.4842 - val_acc: 0.5875\n",
      "Epoch 462/500\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 3.4126 - val_acc: 0.5875\n",
      "Epoch 463/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4198 - val_acc: 0.5750\n",
      "Epoch 464/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.4367 - val_acc: 0.5750\n",
      "Epoch 465/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.4658 - val_acc: 0.5750\n",
      "Epoch 466/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4758 - val_acc: 0.5750\n",
      "Epoch 467/500\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.4739 - val_acc: 0.5750\n",
      "Epoch 468/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.4858 - val_acc: 0.5750\n",
      "Epoch 469/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 3.5004 - val_acc: 0.5875\n",
      "Epoch 470/500\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 3.5122 - val_acc: 0.5750\n",
      "Epoch 471/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 3.5464 - val_acc: 0.5875\n",
      "Epoch 472/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5509 - val_acc: 0.5750\n",
      "Epoch 473/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.5577 - val_acc: 0.5750\n",
      "Epoch 474/500\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 3.5733 - val_acc: 0.5875\n",
      "Epoch 475/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 3.6831 - val_acc: 0.6000\n",
      "Epoch 476/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0595 - acc: 0.9750 - val_loss: 3.7397 - val_acc: 0.5625\n",
      "Epoch 477/500\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 3.4715 - val_acc: 0.5750\n",
      "Epoch 478/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.4644 - val_acc: 0.5750\n",
      "Epoch 479/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.4891 - val_acc: 0.5750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 480/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.5128 - val_acc: 0.5750\n",
      "Epoch 481/500\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5086 - val_acc: 0.5750\n",
      "Epoch 482/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 3.5293 - val_acc: 0.5750\n",
      "Epoch 483/500\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 3.5455 - val_acc: 0.5750\n",
      "Epoch 484/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 3.6131 - val_acc: 0.5625\n",
      "Epoch 485/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 3.6053 - val_acc: 0.5625\n",
      "Epoch 486/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.6024 - val_acc: 0.5625\n",
      "Epoch 487/500\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.0183 - acc: 0.9917 - val_loss: 3.6053 - val_acc: 0.5625\n",
      "Epoch 488/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0313 - acc: 0.9792 - val_loss: 3.6530 - val_acc: 0.5750\n",
      "Epoch 489/500\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 3.4819 - val_acc: 0.5750\n",
      "Epoch 490/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 3.5070 - val_acc: 0.5750\n",
      "Epoch 491/500\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.5197 - val_acc: 0.5750\n",
      "Epoch 492/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.5453 - val_acc: 0.5750\n",
      "Epoch 493/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.5602 - val_acc: 0.5750\n",
      "Epoch 494/500\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 3.6018 - val_acc: 0.5750\n",
      "Epoch 495/500\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 3.5991 - val_acc: 0.5750\n",
      "Epoch 496/500\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 3.5922 - val_acc: 0.5750\n",
      "Epoch 497/500\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 3.6489 - val_acc: 0.5750\n",
      "Epoch 498/500\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 3.5753 - val_acc: 0.5750\n",
      "Epoch 499/500\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 3.6393 - val_acc: 0.5750\n",
      "Epoch 500/500\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 3.6536 - val_acc: 0.5750\n",
      "Training complete on private_dog2 Duration: 12 secs; about 0 minutes.\n",
      "Selected the test result with the lowest training loss. Loss and validation accuracy are -\n",
      "0.0027229006169363855 0.574999988079071 at index 494  (epoch  495 )\n",
      "Tensorboard logs in ../logs/tensorboard/2019-03-09T09:33/private_dog2_0\n",
      "Added summary row to  ../logs/2019-03-09T09:33/private_dog2/devnet_summary.csv\n",
      "Saving logs to ../logs/2019-03-09T09:33/private_dog2/history_0.csv\n",
      "DONE\n",
      "private_dog2 2019-03-09T09:33\n",
      "train:test 120 40\n",
      "|0  |0.0027229006169363855  |0.574999988079071 |495 |0mins  |\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at: 2019-03-09T09:34\n"
     ]
    }
   ],
   "source": [
    "# Print when done\n",
    "print('Done at:' , '{:%Y-%m-%dT%H:%M}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class probabilities:\n",
      " [[9.9999070e-01 2.7888302e-06]\n",
      " [8.3129351e-08 9.9999988e-01]\n",
      " [9.5110756e-01 4.9663812e-02]\n",
      " [9.9389738e-01 1.2116169e-02]\n",
      " [3.1827353e-02 9.2261881e-01]]\n",
      "Pred [0 1 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1]\n",
      "True [1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1]\n",
      "[[12  8]\n",
      " [ 9 11]]\n",
      "Calculated accuracy: 0.575\n",
      "Normalised confusion matrix:\n",
      " [[0.6  0.4 ]\n",
      " [0.45 0.55]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.6299999999999999\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.55      0.56        20\n",
      "         1.0       0.57      0.60      0.59        20\n",
      "\n",
      "   micro avg       0.57      0.57      0.57        40\n",
      "   macro avg       0.58      0.57      0.57        40\n",
      "weighted avg       0.58      0.57      0.57        40\n",
      "\n",
      "\n",
      "micro av - averaging the total true positives, false negatives and false positives\n",
      "macro av - averaging the unweighted mean per label\n",
      "weighted av - averaging the support-weighted mean per label\n"
     ]
    }
   ],
   "source": [
    "# Use trained model (after all epochs) to make predictions\n",
    "do_print = True\n",
    "x_input = x_test\n",
    "y_input = y_test\n",
    "y_input = y_input - y_input.min()\n",
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_input = (x_input - x_train_mean)/(x_train_std)\n",
    "if model_type != 'MLP':\n",
    "    x_input = x_input.reshape(x_input.shape + (1,1,))\n",
    "nb_classes = len(np.unique(y_input))\n",
    "y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "# Calculate model prediction\n",
    "y_probs = model.predict_on_batch(x_input)\n",
    "y_class = y_probs.argmax(axis=1)\n",
    "cm = confusion_matrix(y_input, y_probs.argmax(axis=1), labels=[1,0])\n",
    "acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "if do_print:\n",
    "    print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "    print('Pred', y_class[:20])\n",
    "    print('True', y_input[:20].astype(int))\n",
    "    print(cm)\n",
    "    print('Calculated accuracy:',acc_calc) \n",
    "    print('Normalised confusion matrix:\\n', cm_norm)\n",
    "title = 'Normalised confusion matrix'\n",
    "plot_confusion_matrix(cm_norm, title=title, save=True)\n",
    "\n",
    "# ROC and AUC\n",
    "auc = plot_roc(y_input, y_probs)\n",
    "print('AUC:', auc)\n",
    "\n",
    "report = classification_report(y_input, y_class)\n",
    "print('\\n', report)\n",
    "print('\\nmicro av - averaging the total true positives, false negatives and false positives')\n",
    "print('macro av - averaging the unweighted mean per label')\n",
    "print('weighted av - averaging the support-weighted mean per label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
