{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet plus developments\n",
    "\n",
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ['private_dog0_correct_plus'] #, 'private_dog0_correct_plus', 'private_dog2_correct'] # List of dataset directory names. WormsTwoClass Lightning2 Earthquakes GunPoint \n",
    "batch_size = 32 \n",
    "nb_epochs = 500\n",
    "truncate_data = False # Truncate pressure samples to first n data points\n",
    "model_type = 'MLP' #'MLP'\n",
    "filter_data = False # Filter out noise below a threshold\n",
    "\n",
    "\n",
    "k = 10 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 1 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "k_fold_seed = 765432\n",
    "\n",
    "# ResNet / FCN parameters\n",
    "feature_maps = 128\n",
    "\n",
    "# Output directories\n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\")))\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "# Input directory\n",
    "if 'private' in flist[0]:\n",
    "    fdir = '../data/private_data/private_events_dev2' \n",
    "else:\n",
    "    fdir = '../data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix', name=''):\n",
    "    ''' Plot the normalised confusion matrix\n",
    "    Parameters\n",
    "    cm : array - normalised confusion matrix\n",
    "    Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    '''\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    sns.set_style('dark')\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(format=FuncFormatter('{0:.0%}'.format))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.clim(0, 1)\n",
    "    fmt = '.0%'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    file_name = 'cm_devnet_'+name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "def plot_roc(y_true, y_probs, name): \n",
    "    ''' Plot ROC and return AUC\n",
    "    Parameters\n",
    "    y_true : vector of true class labels.\n",
    "    y_probs : array of predicted probabilities, one column for each class.\n",
    "    Returns\n",
    "    auc : float\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:,1])\n",
    "    auc = roc_auc_score(y_true, y_probs[:,1])\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    file_name = 'roc_devnet_'+name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    return auc\n",
    "\n",
    "def filter_out(x, threshold):\n",
    "    if x < threshold:\n",
    "        return 0\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def preprocess(X):\n",
    "    ''' Apply preprocessing to the input data X'''\n",
    "    if filter_data:\n",
    "        threshold = 0.1\n",
    "        X = np.piecewise(X, [X < threshold, X >= threshold], [lambda X: 0, lambda X: X])\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    if truncate_data:\n",
    "        X = X[:,:300]\n",
    "    X = preprocess(X)\n",
    "    return X, Y\n",
    "   \n",
    "def reshape(x, model_type):\n",
    "    if model_type == 'ResNet':\n",
    "        return reshape_resnet(x)\n",
    "    elif model_type == 'FCN' or model_type == 'FCN_HARUS':\n",
    "        return reshape_fcn(x)\n",
    "    elif model_type == 'MLP':\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unrecognised model type')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DNN\n",
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_resnet(x):\n",
    "    x = x.reshape(x.shape + (1,1,))\n",
    "    return x\n",
    "\n",
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    # Parameters\n",
    "    # kernels = [8, 5, 3, 1]\n",
    "    k = 1 # kernel multiplier\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8*k, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1*k, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8*k, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "         \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1*k, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8*k, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1*k, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='sigmoid')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_fcn(x):\n",
    "    x = x.reshape(x.shape + (1,))\n",
    "    return x\n",
    "    \n",
    "def build_fcn(input_shape, n_feature_maps, nb_classes):\n",
    "    # Parameters\n",
    "    k = 1 # kernel multiplier\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    #conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps, 8*k, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    conv_z = Activation('relu')(conv_z)\n",
    "    \n",
    "    #print ('build conv_za')\n",
    "    #conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_z)\n",
    "    #conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    #conv_z = Activation('relu')(conv_z)\n",
    "    \n",
    "    #print ('build conv_zb')\n",
    "    #conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_z)\n",
    "    #conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    #conv_z = Activation('relu')(conv_z)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv_z)\n",
    "    #full = Dense(128, activation='relu')(full)\n",
    "    out = Dense(nb_classes, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN HARUS style\n",
    "\n",
    "Using the DNN architecture of\n",
    "Ackermann, Nils, 2018, Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences\n",
    "https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn_harus(input_shape, n_feature_maps, nb_classes):\n",
    "    # Parameters\n",
    "    n_features_a = 100 # Ackermann 100\n",
    "    n_features_b = 160 # Ackermann 160\n",
    "    filter_size = 10   # Ackermann 10\n",
    "    pooling_size = 3   # Ackermann 3\n",
    "    dropout = 0.5      # Ackermann 0.5\n",
    "    \n",
    "    print ('build FCN HARUS')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    conv_x = keras.layers.Conv1D(n_features_a, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_a, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.MaxPooling1D(pooling_size)(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_b, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_b, filter_size, activation='relu')(conv_x)\n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv_x)\n",
    "    y = Dropout(dropout,name='Dropout')(full)\n",
    "    out = Dense(nb_classes, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, nb_classes):\n",
    "    drop = 0.2\n",
    "    num = 64\n",
    "    l2 = 0.1\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dropout(drop,name='Drop010')(x)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense010')(y)\n",
    "    y = Dropout(drop,name='Drop020')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(drop,name='Drop021')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense021')(y)\n",
    "    y = Dropout(drop,name='Drop031')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense030')(y)\n",
    "    #y = Dropout(drop,name='Drop041')(y)\n",
    "    #y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense040')(y) \n",
    "    y = Dropout(drop,name='Drop081')(y)\n",
    "    out = Dense(nb_classes, activation='sigmoid', name='Dense080')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function - train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "     \n",
    "    x_train = reshape(x_train, model_type)\n",
    "    x_test = reshape(x_test, model_type)\n",
    "    if model_type == 'MLP':\n",
    "        x, y = build_mlp(x_train.shape[1:], nb_classes)\n",
    "    else:\n",
    "        if model_type == 'ResNet':\n",
    "            x, y = build_resnet(x_train.shape[1:], feature_maps, nb_classes)\n",
    "        elif model_type == 'FCN':\n",
    "            x, y = build_fcn(x_train.shape[1:], feature_maps, nb_classes)\n",
    "        elif model_type == 'FCN_HARUS':\n",
    "            x, y = build_fcn_harus(x_train.shape[1:], feature_maps, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "  \n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    \n",
    "    # Print results\n",
    "    duration_seconds = round(end-start)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training complete on', fname, 'Duration:', duration_seconds, 'secs; about', duration_minutes, 'minutes.')\n",
    "    \n",
    "    # Print and save results. Print the testing results which has the lowest training loss.\n",
    "    print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "    idx = log['loss'].idxmin()\n",
    "    loss = log.loc[idx]['loss']\n",
    "    val_acc = log.loc[idx]['val_acc']\n",
    "    epoch = idx + 1\n",
    "    print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(epoch), ')')\n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(epoch)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(epoch)+','+ duration_minutes \n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    with open(logs_dir+'/'+fname+'/devnet_summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/devnet_summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    return summary, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all, y_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print when done\n",
    "print('Done at:' , '{:%Y-%m-%dT%H:%M}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file =  logs_dir+'/'+fname+'/devnet_summary.csv'\n",
    "data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "accuracy = data['val_acc']\n",
    "print(file)\n",
    "print('Accuracy mean and 95% confidence level is', accuracy.mean(), accuracy.std()*1.96)\n",
    "print('95% confidence interval is', accuracy.quantile(0.0025), 'to', accuracy.quantile(0.975))\n",
    "data.boxplot(column=['val_acc'], whis=[2.5,97.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use trained model (after all epochs) to make predictions\n",
    "\n",
    "def predictions(model, model_type, x_input, y_input, name):\n",
    "    do_print = True\n",
    "    y_input = y_input - y_input.min()\n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_input = (x_input - x_train_mean)/(x_train_std)\n",
    "    x_input = reshape(x_input, model_type)\n",
    "    nb_classes = len(np.unique(y_input))\n",
    "    y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "    # Class balance\n",
    "    n0 = (y_input == 0).sum()\n",
    "    n1 = (y_input == 1).sum()\n",
    "    # Calculate model prediction\n",
    "    y_probs = model.predict_on_batch(x_input)\n",
    "    y_class = y_probs.argmax(axis=1)\n",
    "    cm = confusion_matrix(y_input, y_probs.argmax(axis=1), labels=[1,0])\n",
    "    acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    if do_print:\n",
    "        print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "        print('Pred', y_class[:20])\n",
    "        print('True', y_input[:20].astype(int))\n",
    "        print(cm)\n",
    "        print('Calculated accuracy:',acc_calc)\n",
    "        print('Class balance in test set:', n0, 'to', n1, 'i.e.', n0/(n0+n1))\n",
    "        print('Normalised confusion matrix:\\n', cm_norm)\n",
    "    title = 'Normalised confusion matrix'\n",
    "    plot_confusion_matrix(cm_norm, title=title, name=name)\n",
    "\n",
    "    # ROC and AUC\n",
    "    auc = plot_roc(y_input, y_probs, name=name)\n",
    "    print('AUC:', auc)\n",
    "\n",
    "    report = classification_report(y_input, y_class)\n",
    "    print('\\n', report)\n",
    "    print('\\nmicro av - averaging the total true positives, false negatives and false positives')\n",
    "    print('macro av - averaging the unweighted mean per label')\n",
    "    print('weighted av - averaging the support-weighted mean per label')\n",
    "    return y_class\n",
    "    \n",
    "y_pred = predictions(model, model_type, x_test, y_test, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(x, y_true, y_pred, title, meta=None):\n",
    "    if meta is not None:\n",
    "        print(title)\n",
    "    n_plots = 10\n",
    "    fig, ax = plt.subplots(n_plots, 4, sharex='col', sharey='row', figsize=(10, 10))\n",
    "    rows = [0, 0, 0, 0]\n",
    "    green_red = sns.color_palette(\"Paired\")\n",
    "    colors = [green_red[3], green_red[5], green_red[2], green_red[4]]\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_true[i]==1:\n",
    "            if y_pred[i]==1:\n",
    "                col = 0\n",
    "            else:\n",
    "                col = 1\n",
    "                if meta is not None:\n",
    "                    print('FN at ', meta.iloc[i]['filename'], 'sensor', meta.iloc[i]['sensor_number'])\n",
    "        if y_true[i]==0:\n",
    "            if y_pred[i]==0:\n",
    "                col = 2\n",
    "            else:\n",
    "                col = 3\n",
    "                if meta is not None:\n",
    "                    print('FP at ', meta.iloc[i]['filename'], 'sensor', meta.iloc[i]['sensor_number'])\n",
    "        row = rows[col]\n",
    "        rows[col] = rows[col] + 1\n",
    "        if row < n_plots:\n",
    "            ax[row, col].plot(x[i], color=colors[col])\n",
    "            ax[0, col].set_title('True '+str(int(y_true[i]))+': Pred '+str(y_pred[i]))\n",
    "            ax[row, col].set_ylim(bottom=0, top=2.2)\n",
    "    ax[n_plots-1, 0].set_ylabel('x(t)')\n",
    "    ax[n_plots-1, 0].set_xlabel('time, t')\n",
    "    ax[n_plots-1, 1].set_xlabel('time, t')\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig('data_samples_'+title+'.png', bbox_inches='tight')\n",
    "    \n",
    "plot_samples(x_test, y_test, y_pred, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = '../logs/2019-03-17T12:59/private_dog0_correct/devnet_summary.csv'\n",
    "data1 = pd.read_csv(file1, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "name1 = 'dog0_correct'\n",
    "\n",
    "file = logs_dir+'/'+fname+'/devnet_summary.csv'\n",
    "print('Showing results from:\\n', file1, 'and\\n', file)\n",
    "data2 = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "name2 = 'this_run'\n",
    "\n",
    "all_data = [data1['val_acc'], data2['val_acc']]\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(data=all_data)\n",
    "ax = sns.swarmplot(data=all_data, color='black')\n",
    "ax.set_xlabel('DevNet')\n",
    "ax.set_ylabel('validation accuracy')\n",
    "ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "plt.xticks([0, 1], [name1, name2])\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplot_devnet.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NB consider if any of this dataset was present in the model's training set\n",
    "if True:\n",
    "    other = fname+'_END_TEST' #_dog_incorrect' # 'private_dog0_correct_plus_END_TEST'\n",
    "    datadir = fdir+'/'+fname\n",
    "    print('Testing on:', datadir+'/'+other+'.txt')\n",
    "    x_other, y_other = readucr(datadir+'/'+other+'.txt')\n",
    "    y_other_pred = predictions(model, model_type, x_other, y_other, other)\n",
    "    # Get dog result\n",
    "    meta = pd.read_csv(datadir+'/'+other+'_meta.txt', sep=',', parse_dates=['date'])\n",
    "    cm = confusion_matrix(y_other, meta['dog_pred'], labels=[1,0])\n",
    "    print('Dog cm \\n', cm)\n",
    "    dog_acc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    plot_confusion_matrix(cm_norm, title='Dog indications', name='dog_pred')\n",
    "    print('True', y_other[:20])\n",
    "    print('Dog ', meta['dog_pred'].values[:20])\n",
    "    print('Dog accuracy', dog_acc)\n",
    "    \n",
    "    # Plot data\n",
    "    plot_samples(x_other, y_other, y_other_pred, 'DNN predictions', meta)\n",
    "    plot_samples(x_other, y_other, meta['dog_pred'], 'Dog indications', meta)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the difference in dog and DNN predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_differences(x, y_pred, meta):\n",
    "    # Concatenate all data\n",
    "    y_diff = abs(meta['dog_pred'].values-y_pred.T)\n",
    "    y_diff_df = pd.DataFrame(y_diff, columns=['y_diff'])\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "    x_df = pd.DataFrame(x)\n",
    "    data_meta = pd.concat([y_pred_df, y_diff_df, meta], axis=1)\n",
    "    meta_header = list(data_meta)\n",
    "    data_meta = pd.concat([data_meta, x_df], axis=1)\n",
    "    \n",
    "    # Sort the data\n",
    "    data_meta = data_meta.sort_values(['class', 'y_diff', 'dog_result'])\n",
    "    class0 = data_meta[data_meta['class']==0]\n",
    "    class0 = class0.sort_values(['y_diff', 'dog_result'], ascending=[False, False])\n",
    "    class1 = data_meta[data_meta['class']==1]\n",
    "    class1 = class1.sort_values(['y_diff', 'dog_result'], ascending=[False, True])\n",
    "\n",
    "    # Plot the data where dog and DNN did not agree\n",
    "    for this_class in [class1, class0]:\n",
    "        # Get x data\n",
    "        this_x = this_class[this_class.columns.difference(meta_header)]\n",
    "        assert this_x.shape[1] == 1000\n",
    "        # Count plots required\n",
    "        n = 0\n",
    "        for i in range(len(this_class)):\n",
    "            if this_class.iloc[i]['y_pred'] != this_class.iloc[i]['dog_pred']:\n",
    "                n = n + 1\n",
    "        # Create the plots\n",
    "        fig, ax = plt.subplots(n, 3, sharex='col', sharey='row', figsize=(10, 4.8))\n",
    "        class_label = this_class.iloc[0]['class']\n",
    "        row = 0\n",
    "        for i in range(len(this_class)):\n",
    "            if row < n:\n",
    "                dog_correct = this_class.iloc[i]['dog_pred'] == this_class.iloc[i]['class']\n",
    "                dnn_correct = this_class.iloc[i]['y_pred'] == this_class.iloc[i]['class']\n",
    "                dog_color = 'green' if dog_correct else 'red'\n",
    "                dnn_color = 'green' if dnn_correct else 'red'  \n",
    "                if this_class.iloc[i]['y_pred'] != this_class.iloc[i]['dog_pred']:\n",
    "                    ax[row, 0].plot(this_x.iloc[i], color=dog_color)\n",
    "                    ax[row, 1].plot(this_x.iloc[i], color=dnn_color)\n",
    "                    ax[row, 0].set_ylim(bottom=0, top=2.2)\n",
    "                    ax[row, 1].set_ylim(bottom=0, top=2.2)\n",
    "                    file = this_class.iloc[i]['filename']\n",
    "                    sensor = str(this_class.iloc[i]['sensor_number'])\n",
    "                    ax[row, 2].text(0, 0.65, str(row+1)+') '+file+' sensor '+sensor)\n",
    "                    row = row + 1\n",
    "        ax[0, 0].set_title('Dog')\n",
    "        ax[0, 1].set_title('DNN')\n",
    "        ax[n-1, 0].set_ylabel('x(t)')\n",
    "        ax[n-1, 0].set_xlabel('time, t')\n",
    "        ax[n-1, 1].set_xlabel('time, t')\n",
    "        ax[n-1, 2].set_xticklabels([])\n",
    "        fig.suptitle('True class: '+str(class_label)+'    green=correct, red=incorrect')\n",
    "        plt.savefig('DogDNN_diffs_class' + str(class_label) + '_'+fname+'.png', bbox_inches='tight')\n",
    "\n",
    "plot_differences(x_other, y_other_pred, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(x, y_true, y_pred, y_dog, title):\n",
    "    # Calculate number of plots required\n",
    "    rows = [0, 0, 0, 0]\n",
    "    for i in range(len(y_pred)):\n",
    "        col = -1\n",
    "        if y_true[i]==1:\n",
    "            if y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 0\n",
    "            elif y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 1\n",
    "        if y_true[i]==0:\n",
    "            if y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 2\n",
    "            elif y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 3\n",
    "        if col != -1:\n",
    "            rows[col] = rows[col]+1\n",
    "    n_plots = max(rows)\n",
    "    \n",
    "    # Set up the subplots\n",
    "    fig, ax = plt.subplots(n_plots, 4, sharex='col', sharey='row', figsize=(10, 10))\n",
    "    rows = [0, 0, 0, 0]\n",
    "    green_red = sns.color_palette(\"Paired\")\n",
    "    colors = [green_red[3], green_red[5], green_red[2], green_red[4]]\n",
    "    \n",
    "    # Create each plot\n",
    "    for i in range(len(y_pred)):\n",
    "        col = -1\n",
    "        if y_true[i]==1:\n",
    "            if y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 0\n",
    "            elif y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 1\n",
    "        if y_true[i]==0:\n",
    "            if y_pred[i]==0 and y_dog[i]==0:\n",
    "                col = 2\n",
    "            elif y_pred[i]==1 and y_dog[i]==1:\n",
    "                col = 3\n",
    "        if col != -1:\n",
    "            row = rows[col]\n",
    "            rows[col] = rows[col] + 1\n",
    "            if row < n_plots:\n",
    "                ax[row, col].plot(x[i], color=colors[col])\n",
    "                ax[0, col].set_title('True '+str(int(y_true[i]))+': Pred '+str(y_pred[i]))\n",
    "                ax[row, col].set_ylim(bottom=0, top=2.2)\n",
    "                ax[row, 0].set_yticklabels([])\n",
    "    \n",
    "    # Add labels and title\n",
    "    for c in range(4):\n",
    "        ax[n_plots-1, c].set_xlabel('time, t')\n",
    "    fig.suptitle(title)\n",
    "    plt.savefig('DogDNN_match_'+fname+'.png', bbox_inches='tight')\n",
    "    \n",
    "plot_similarities(x_other, y_other, y_other_pred, meta['dog_pred'], 'Samples where the DNN\\'s prediction matched the dog\\'s indication')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
