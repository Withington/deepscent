{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet plus developments\n",
    "\n",
    "This software uses cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline\n",
    "\n",
    "See MIT License in https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline README.md\n",
    "\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ['private_dog0_correct'] #, 'private_dog1_correct', 'private_dog2_correct'] # List of dataset directory names. WormsTwoClass Lightning2 Earthquakes GunPoint \n",
    "batch_size = 16 \n",
    "nb_epochs = 500\n",
    "model_type = 'MLP' #'MLP'\n",
    "k = 10 # For k-fold cross validation. If k=1, the original test-train split is used.\n",
    "m = 1 # Number of repetitions of k-fold cross validation (if k>1).\n",
    "early_stopping = False \n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "k_fold_seed = 765432\n",
    "\n",
    "# Output directories\n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now())\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "\n",
    "# Input directory\n",
    "if 'private' in flist[0]:\n",
    "    fdir = '../data/private_data/private_events_dev2' \n",
    "else:\n",
    "    fdir = '../data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix', dataset_name=''):\n",
    "    ''' Plot the normalised confusion matrix\n",
    "    Parameters\n",
    "    cm : array - normalised confusion matrix\n",
    "    Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    '''\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    sns.set_style('dark')\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(format=FuncFormatter('{0:.0%}'.format))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.clim(0, 1)\n",
    "    fmt = '.0%'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    file_name = 'cm_devnet'+dataset_name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "def plot_roc(y_true, y_probs, dataset_name): \n",
    "    ''' Plot ROC and return AUC\n",
    "    Parameters\n",
    "    y_true : vector of true class labels.\n",
    "    y_probs : array of predicted probabilities, one column for each class.\n",
    "    Returns\n",
    "    auc : float\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs[:,1])\n",
    "    auc = roc_auc_score(y_true, y_probs[:,1])\n",
    "    sns.set_style('darkgrid')\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    file_name = 'roc_devnet'+dataset_name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    return auc\n",
    "\n",
    "\n",
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DNN\n",
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "         \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(nb_classes, activation='softmax')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, nb_classes):\n",
    "    drop = 0.2\n",
    "    num = 64\n",
    "    l2 = 0.1\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dropout(drop,name='Drop010')(x)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense010')(y)\n",
    "    y = Dropout(drop,name='Drop020')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(drop,name='Drop021')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense021')(y)\n",
    "    y = Dropout(drop,name='Drop031')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense030')(y)\n",
    "    #y = Dropout(drop,name='Drop041')(y)\n",
    "    #y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense040')(y) \n",
    "    y = Dropout(drop,name='Drop081')(y)\n",
    "    out = Dense(nb_classes, activation='sigmoid', name='Dense080')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "     \n",
    "    y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "    Y_train = utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = utils.to_categorical(y_test, nb_classes)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "     \n",
    "    if model_type == 'MLP':\n",
    "        x, y = build_mlp(x_train.shape[1:], nb_classes)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape + (1,1,))\n",
    "        x_test = x_test.reshape(x_test.shape + (1,1,))\n",
    "        x, y = build_resnet(x_train.shape[1:], 64, nb_classes)\n",
    "    model = Model(x, y)\n",
    "    #print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.RMSprop()#.Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', baseline=0.6, min_delta=0.001, \n",
    "         patience=100, verbose=1) # when tf updates keras, use restore_best_weights instead of ModelCheckPoint\n",
    "        # save_best_only - we are interested in only the very best model observed during training, \n",
    "        # rather than the best compared to the previous epoch, which might not be the best overall \n",
    "        # if training is noisy\n",
    "        model_save = keras.callbacks.ModelCheckpoint(logs_dir+'/temp.h5', monitor='val_acc', \n",
    "            save_best_only=True, save_weights_only=True, verbose=1) #\n",
    "        callbacks.append(early_stop)\n",
    "        callbacks.append(model_save)\n",
    "  \n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    \n",
    "    # Print results\n",
    "    duration_seconds = round(end-start)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training complete on', fname, 'Duration:', duration_seconds, 'secs; about', duration_minutes, 'minutes.')\n",
    "    \n",
    "    if not early_stopping:\n",
    "        # Print and save results. Print the testing results which has the lowest training loss.\n",
    "        print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "        idx = log['loss'].idxmin()\n",
    "        loss = log.loc[idx]['loss']\n",
    "        val_acc = log.loc[idx]['val_acc']\n",
    "        epoch = idx + 1\n",
    "        print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(epoch), ')')\n",
    "    else:\n",
    "        print('Early stopping.')\n",
    "        model.load_weights(logs_dir+'/temp.h5')\n",
    "        eval = model.evaluate(x_train, Y_train)\n",
    "        print('From model.evaluate() on training set', model.metrics_names)\n",
    "        print(eval)\n",
    "        eval = model.evaluate(x_test, Y_test)\n",
    "        print('From model.evaluate()', model.metrics_names)\n",
    "        print(eval)\n",
    "        loss = eval[0] # val_loss\n",
    "        val_acc = eval[1]\n",
    "        epoch = early_stop.stopped_epoch\n",
    "        print('stop_epoch:', epoch)\n",
    "        idx = epoch - 1\n",
    "        logged_loss = log.loc[idx]['loss']\n",
    "        logged_val_acc = log.loc[idx]['val_acc']\n",
    "        print('logged loss and val_acc', logged_loss, logged_val_acc)\n",
    "        \n",
    "        \n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(epoch)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(epoch)+','+ duration_minutes \n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    with open(logs_dir+'/'+fname+'/devnet_summary.csv', 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', logs_dir+'/'+fname+'/devnet_summary.csv')  \n",
    "    print('Saving logs to',logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    log.to_csv(logs_dir+'/'+fname+'/history_'+label+'.csv')\n",
    "    \n",
    "    return summary, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for each in flist:\n",
    "    fname = each\n",
    "    x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "    # k-fold cross validation setup\n",
    "    if k > 1:\n",
    "        x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "        y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "        kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "        count = 0\n",
    "        for train, test in kfold.split(x_all, y_all):\n",
    "            x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "            summary, model = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "            results.append(summary)\n",
    "            count = count + 1\n",
    "    else:\n",
    "        summary, model = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "        results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print when done\n",
    "print('Done at:' , '{:%Y-%m-%dT%H:%M}'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use trained model (after all epochs) to make predictions\n",
    "\n",
    "\n",
    "def predictions(model, model_type, x_input, y_input, dataset_name):\n",
    "    do_print = True\n",
    "    y_input = y_input - y_input.min()\n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_input = (x_input - x_train_mean)/(x_train_std)\n",
    "    if model_type != 'MLP':\n",
    "        x_input = x_input.reshape(x_input.shape + (1,1,))\n",
    "    nb_classes = len(np.unique(y_input))\n",
    "    y_input = (y_input - y_input.min())/(y_input.max()-y_input.min())*(nb_classes-1)\n",
    "    # Class balance\n",
    "    n0 = (y_input == 0).sum()\n",
    "    n1 = (y_input == 1).sum()\n",
    "    # Calculate model prediction\n",
    "    y_probs = model.predict_on_batch(x_input)\n",
    "    y_class = y_probs.argmax(axis=1)\n",
    "    cm = confusion_matrix(y_input, y_probs.argmax(axis=1), labels=[1,0])\n",
    "    acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    if do_print:\n",
    "        print('Predicted class probabilities:\\n', y_probs[:5,:])\n",
    "        print('Pred', y_class[:20])\n",
    "        print('True', y_input[:20].astype(int))\n",
    "        print(cm)\n",
    "        print('Calculated accuracy:',acc_calc)\n",
    "        print('Class balance in test set:', n0, 'to', n1, 'i.e.', n0/(n0+n1))\n",
    "        print('Normalised confusion matrix:\\n', cm_norm)\n",
    "    title = 'Normalised confusion matrix'\n",
    "    plot_confusion_matrix(cm_norm, title=title, dataset_name=dataset_name)\n",
    "\n",
    "    # ROC and AUC\n",
    "    auc = plot_roc(y_input, y_probs, dataset_name=dataset_name)\n",
    "    print('AUC:', auc)\n",
    "\n",
    "    report = classification_report(y_input, y_class)\n",
    "    print('\\n', report)\n",
    "    print('\\nmicro av - averaging the total true positives, false negatives and false positives')\n",
    "    print('macro av - averaging the unweighted mean per label')\n",
    "    print('weighted av - averaging the support-weighted mean per label')\n",
    "    \n",
    "predictions(model, model_type, x_test, y_test, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file =  logs_dir+'/'+fname+'/devnet_summary.csv'\n",
    "data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "accuracy = data['val_acc']\n",
    "print(file)\n",
    "print('Accuracy mean and 95% confidence level is', accuracy.mean(), accuracy.std()*1.96)\n",
    "print('95% confidence interval is', accuracy.quantile(0.0025), 'to', accuracy.quantile(0.975))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.boxplot(column=['val_acc'], whis=[2.5,97.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '../logs/2019-03-17T12:59/private_dog0_correct/devnet_summary.csv'\n",
    "data1 = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "name1 = 'dog0_correct'\n",
    "\n",
    "file = logs_dir+'/'+fname+'/devnet_summary.csv'\n",
    "print('Showing results from', file)\n",
    "data2 = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "name2 = 'this_run'\n",
    "\n",
    "all_data = [data1['val_acc'], data2['val_acc']]\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(data=all_data)\n",
    "ax = sns.swarmplot(data=all_data, color='black')\n",
    "ax.set_xlabel('DevNet')\n",
    "ax.set_ylabel('validation accuracy')\n",
    "ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "plt.xticks([0, 1], [name1, name2])\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplot_devnet.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB consider if any of this dataset was present in the model's training set\n",
    "other = 'private_dog0_gooddays' #'private_dog0'\n",
    "datadir = fdir # '../data/private_data/private_events_dev'\n",
    "x_other_train, y_other_train = readucr(datadir+'/'+other+'/'+other+'_TRAIN.txt')\n",
    "x_other_test, y_other_test = readucr(datadir+'/'+other+'/'+other+'_TEST.txt')\n",
    "x_other = np.concatenate((x_other_train, x_other_test), axis=0)\n",
    "y_other = np.concatenate((y_other_train, y_other_test), axis=0)\n",
    "predictions(model, model_type, x_other, y_other, other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
