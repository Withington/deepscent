{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification\n",
    "\n",
    "Time series classification (TSC) operates on time series data, a series of values that is ordered by time. Data samples are labelled as belonging to a particular class. The TSC system is trained using this data to classify unlabelled samples. There is a wide range of TSC applications. Smartwatch data is used to classify human activities (walking, running, ascending stairs, etc.). Animal behaviour (hunting, sleeping) is monitored using accelerometers on tagged, wild animals for environmental studies. Sensors on industrial machines are used to classify time series samples as either normal or preceding a failure, informing machine maintenance schedules.\n",
    "\n",
    "This exercise uses the SonyAIBORobotSurface1 dataset from the UEA & UCR Time Series Classification Repository (Dau et al, 2018). This dataset was collected by Vail and Veloso (2004), Carnegie Mellon University, from an accelerometer on a Sony AIBO robot. Their aim was to detect the surface that the robot was walking on in order to optimise its gait for that surface. The robots competed in the RoboCup League, a football game played on a carpeted field.\n",
    "\n",
    "![The Sony AIBO Robot is a robot dog. It is pictured with a ball.](https://i1.wp.com/www.techdigest.tv/wp-content/uploads/2015/06/aibo-560.jpg \"Sony AIBO Robot\")\n",
    "\n",
    "## References\n",
    "Dau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A. and Keogh, E. (2018) ‘The UCR Time Series Archive’, [Online]. Available at http://arxiv.org/abs/1810.07758 (Accessed 4 May 2019).\n",
    "\n",
    "Vail, D. and Veloso, M. (2004) ‘Learning from accelerometer data on a legged robot’, *IFAC Proceedings*, vol. 37, no. 8, pp. 822–827 [Online]. Available at https://www.cs.cmu.edu/~mmv/papers/04iav-doug.pdf (Accessed 4 May 2019).\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python packages\n",
    "Import the Python packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# General settings\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_web = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "The robot data provided is the x-axis accelerometer data sampled at 125Hz (125 times per second). A positive value relates acceleration in the forward direction. Each data sample has 70 data points (0.56s) and is labelled as either cement or carpet. The original data had a positive mean, because the robot leans forwards slightly, and was in the range approximately [0, 0.4] gravities. The dataset provided has been normalised.\n",
    "\n",
    "The machine learning approach that Vail and Veloso took was to take a one second window and extract statistical features from all three accelerometer axes. Six features were calculated – variance in acceleration and correlation between the accelerations. A decision tree was used for learning. The paper reports on three classes – walking on cement, carpet in their laboratory and carpet on the football field. The overall classification accuracy was 84.9%.\n",
    "\n",
    "The dataset has been split into two, balanced, datasets. One for model development and one for our final test to evaluate the finished model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_web:\n",
    "    url = 'https://raw.githubusercontent.com/Withington/deepscent/master/data/SonyAIBORobotSurface1_IoC/SonyAIBORobotSurface1_IoC_DEV.txt'\n",
    "    robot_df = pd.read_csv(url, sep='\\t', header=None)\n",
    "    print('Loaded from', url)\n",
    "    robot_data = robot_df.values\n",
    "else:\n",
    "    data_dir = '../../data'\n",
    "    data_name = 'SonyAIBORobotSurface1_IoC'\n",
    "    data_filename = data_dir+'/'+data_name+'/'+data_name+'_DEV.txt'\n",
    "    robot_data = np.loadtxt(Path(data_filename))\n",
    "    print('Loaded from', data_filename)\n",
    "print('The shape of robot_data is', robot_data.shape)\n",
    "print('robot_data:', robot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the labels, y, and the data samples, x. For convenience we will use labels class 0 and 1 instead of classes 1 and 2. \n",
    "\n",
    "class 0 : cement\n",
    "\n",
    "class 1 : carpet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = robot_data[:,0]\n",
    "x_dev = robot_data[:,1:]\n",
    "print('The shape of x_dev is', x_dev.shape)\n",
    "print('The shape of y_dev is', y_dev.shape)\n",
    "\n",
    "# Change from classes 1 and 2 to classes 0 and 1\n",
    "y_dev = (y_dev - y_dev.min())/(y_dev.max()-y_dev.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_dev[sample_number], label='category'+str(y_dev[sample_number]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_dev[sample_a], label='category'+str(y_dev[sample_a]))\n",
    "plt.plot(x_dev[sample_b], label='category'+str(y_dev[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_dev[3], label='category'+str(y_dev[3]))\n",
    "plt.plot(x_dev[7], label='category'+str(y_dev[7]))\n",
    "plt.plot(x_dev[8], label='category'+str(y_dev[8]))\n",
    "plt.plot(x_dev[10], label='category'+str(y_dev[10]))\n",
    "plt.plot(x_dev[11], label='category'+str(y_dev[11]))\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Walking on cement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_dev[0], label='category'+str(y_dev[0]))\n",
    "plt.plot(x_dev[1], label='category'+str(y_dev[1]))\n",
    "plt.plot(x_dev[2], label='category'+str(y_dev[2]))\n",
    "plt.plot(x_dev[4], label='category'+str(y_dev[4]))\n",
    "plt.plot(x_dev[5], label='category'+str(y_dev[5]))\n",
    "plt.legend(loc='upper right', frameon=False)\n",
    "plt.ylim([-3.5, 3.5])\n",
    "plt.title('Walking on carpet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the development dataset into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples of class 0', (y_dev == 0).sum())\n",
    "print('Number of samples of class 1', (y_dev == 1).sum())\n",
    "y_dev_df = pd.DataFrame(y_dev)\n",
    "y_dev_df[0].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_dev, y_dev, test_size=100, random_state=21, stratify=y_dev)\n",
    "print('The shape of train_data is', x_train.shape)\n",
    "print('The shape of test_data is', x_test.shape)\n",
    "print('Training data:')\n",
    "print('Number of samples of class 0', (y_train == 0).sum())\n",
    "print('Number of samples of class 1', (y_train == 1).sum())\n",
    "print('Test data:')\n",
    "print('Number of samples of class 0', (y_test == 0).sum())\n",
    "print('Number of samples of class 1', (y_test == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "\n",
    "print('x_train_mean', x_train_mean)\n",
    "print('x_train_std', x_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 4 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_train[sample_a], label='category'+str(y_train[sample_a]))\n",
    "plt.plot(x_train[sample_b], label='category'+str(y_train[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP 1\n",
    "Create an multilayer perceptron (MLP). This first MLP is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 16\n",
    "x = Input(shape=(input_shape), name='MLP1InputLayer')\n",
    "y = Dense(n0, activation='relu', name='Dense010')(x)\n",
    "# Output layer\n",
    "out = Dense(1, activation='sigmoid', name='OutputLayer')(y)\n",
    "\n",
    "# Build model\n",
    "model_mlp1 = Model(x, out)\n",
    "print(model_mlp1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the number of parameters\n",
    "TODO - exercise around the calculation that arrives at the number of parameters in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(70*16+16)\n",
    "print(16+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "model_mlp1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# TODO - can we access TensorBoard on colab? If so, add tensorboard callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model_mlp1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first five rows in log are')\n",
    "log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['loss', 'val_loss']].plot()\n",
    "# TODO add axes labels, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using MLP 1\n",
    "Classify the data using MLP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_mlp1.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Validation accuracy is', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probability = model_mlp1.predict_on_batch(x_test)\n",
    "y_predicted_class = np.round(y_probability).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 3\n",
    "print('The probability that sample', sample, 'belongs to class 1 is', y_probability[sample][0])\n",
    "print('The model classifies sample', sample, 'as class', y_predicted_class[sample])\n",
    "print('The true class of sample', sample, 'is class', y_test[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_test[sample_a], label='True:'+str(y_test[sample_a])+' Pred:'+str(y_predicted_class[sample_a]))\n",
    "plt.plot(x_test[sample_b], label='True:'+str(y_test[sample_b])+' Pred:'+str(y_predicted_class[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP 2\n",
    "This time we will create a function that builds our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    x = Input(shape=(input_shape), name='MLP2InputLayer')\n",
    "    ### CHANGE PARAMETERS HERE ###\n",
    "    y = Dropout(0.1,name='Drop010')(x)\n",
    "    y = Dense(16, activation='relu', name='Dense010')(y) \n",
    "    y = Dropout(0.2,name='Drop020')(y)\n",
    "    y = Dense(16, activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(0.2,name='Drop030')(y)\n",
    "    y = Dense(16, activation='relu', name='Dense030')(y)\n",
    "    y = Dropout(0.3,name='Drop040')(y)\n",
    "    ### END OF CHANGE PARAMETERS ###\n",
    "    out = Dense(1, activation='sigmoid', name='OutputLayer')(y)\n",
    "\n",
    "    # Build model and compile the model\n",
    "    model = Model(x, out)\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 50\n",
    "start = time.time()\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['acc', 'val_acc']].plot()\n",
    "model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Validation accuracy is', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHANGE PARAMETERS HERE ###\n",
    "k = 3 \n",
    "m = 5 \n",
    "batch_size = 10\n",
    "epochs = 30\n",
    "### END OF CHANGE PARAMETERS ###\n",
    "\n",
    "kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=76)\n",
    "count = 0\n",
    "val_acc = list()\n",
    "start = time.time()\n",
    "for train, test in kfold.split(x_dev, y_dev):\n",
    "    x_train, y_train, x_test, y_test = x_dev[train], y_dev[train], x_dev[test], y_dev[test]\n",
    "    # Normalise the data\n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "    # Build and train a model\n",
    "    model = build_model()\n",
    "    fold_start = time.time()\n",
    "    hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "    fold_end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    print('Training of iteration', count, 'complete in', round(fold_end-fold_start), 'seconds')\n",
    "    val_acc.append(log.iloc[-1]['val_acc'])\n",
    "    count = count + 1\n",
    "\n",
    "end = time.time()\n",
    "val_acc = pd.DataFrame(val_acc, columns=['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_acc)\n",
    "print(m, 'repeats of', k, '-fold cross validation completed in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the k-fold cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=val_acc)\n",
    "ax = sns.swarmplot(data=val_acc, color='black')\n",
    "print('Validation accuracy mean and sample standard deviation', val_acc['val_acc'].mean(), val_acc['val_acc'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU\n",
    "Using a GPU can speed up calculations. However, it can take longer to transfer the data to the GPU.\n",
    "\n",
    "You are more likely to see a speed-up if batch size is large. As you increase batch size, check that valuation accuracy does not deteriorate.\n",
    "\n",
    "To use a GPU in colab select Edit - Notebook settings and then set Hardware accelerator to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if you are using a GPU.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    print('GPU device not found')\n",
    "print('Found a GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
