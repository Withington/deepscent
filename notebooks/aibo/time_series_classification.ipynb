{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series classification\n",
    "Time series classification (TSC)\n",
    "\n",
    "Using data from timeseriesclassification.com\n",
    "TODO add reference to Dau et al. 2018\n",
    "\n",
    "The SonyAIBORobotSurface dataset was donated to the TSC website by Manuela Veloso and Douglas Vail of Carnegie Mellon University. Their paper on robot surface detection is\n",
    "https://www.cs.cmu.edu/~mmv/papers/04iav-doug.pdf\n",
    "TODO - get full reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Python packages\n",
    "Import the Python packages that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_web = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "TODO introduce the dataset and what it is used for.\n",
    "\n",
    "The dataset has been split into two, balanced, datasets. One for model development and one for our final test to evaluate the finished model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_from_web:\n",
    "    url = 'https://raw.githubusercontent.com/Withington/deepscent/master/data/SonyAIBORobotSurface1_IoC/SonyAIBORobotSurface1_IoC_DEV.txt'\n",
    "    robot_df = pd.read_csv(url, sep='\\t', header=None)\n",
    "    print('Loaded from', url)\n",
    "    robot_data = robot_df.values\n",
    "else:\n",
    "    data_dir = '../../data'\n",
    "    data_name = 'SonyAIBORobotSurface1_IoC'\n",
    "    data_filename = data_dir+'/'+data_name+'/'+data_name+'_DEV.txt'\n",
    "    robot_data = np.loadtxt(Path(data_filename))\n",
    "    print('Loaded from', data_filename)\n",
    "print('The shape of robot_data is', robot_data.shape)\n",
    "print('robot_data:', robot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev = robot_data[:,0]\n",
    "x_dev = robot_data[:,1:]\n",
    "print('The shape of x_dev is', x_dev.shape)\n",
    "print('The shape of y_dev is', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change from classes 1 and 2 to classes 0 and 1\n",
    "y_dev = (y_dev - y_dev.min())/(y_dev.max()-y_dev.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples of class 0', (y_dev == 0).sum())\n",
    "print('Number of samples of class 1', (y_dev == 1).sum())\n",
    "y_dev_df = pd.DataFrame(y_dev)\n",
    "y_dev_df[0].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_number = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_dev[sample_number], label='category'+str(y_dev[sample_number]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_dev[sample_a], label='category'+str(y_dev[sample_a]))\n",
    "plt.plot(x_dev[sample_b], label='category'+str(y_dev[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the development dataset into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_dev, y_dev, test_size=100, random_state=21, stratify=y_dev)\n",
    "print('The shape of train_data is', x_train.shape)\n",
    "print('The shape of test_data is', x_test.shape)\n",
    "print('Training data:')\n",
    "print('Number of samples of class 0', (y_train == 0).sum())\n",
    "print('Number of samples of class 1', (y_train == 1).sum())\n",
    "print('Test data:')\n",
    "print('Number of samples of class 0', (y_test == 0).sum())\n",
    "print('Number of samples of class 1', (y_test == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mean = x_train.mean()\n",
    "x_train_std = x_train.std()\n",
    "x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "\n",
    "print('x_train_mean', x_train_mean)\n",
    "print('x_train_std', x_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 4 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_train[sample_a], label='category'+str(y_train[sample_a]))\n",
    "plt.plot(x_train[sample_b], label='category'+str(y_train[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP 1\n",
    "Create an multilayer perceptron (MLP). This first MLP is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 16\n",
    "x = Input(shape=(input_shape), name='MLP1InputLayer')\n",
    "y = Dense(n0, activation='relu', name='Dense010')(x)\n",
    "# Output layer\n",
    "out = Dense(1, activation='sigmoid', name='OutputLayer')(y)\n",
    "\n",
    "# Build model\n",
    "model_mlp1 = Model(x, out)\n",
    "print(model_mlp1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the number of parameters\n",
    "TODO - exercise around the calculation that arrives at the number of parameters in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(70*16+16)\n",
    "print(16+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "model_mlp1.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# TODO - can we access TensorBoard on colab? If so, add tensorboard callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "hist = model_mlp1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first five rows in log are')\n",
    "log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['loss', 'val_loss']].plot()\n",
    "# TODO add axes labels, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['acc', 'val_acc']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using MLP 1\n",
    "Classify the data using MLP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_mlp1.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Validation accuracy is', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probability = model_mlp1.predict_on_batch(x_test)\n",
    "y_predicted_class = np.round(y_probability).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 3\n",
    "print('The probability that sample', sample, 'belongs to class 1 is', y_probability[sample][0])\n",
    "print('The model classifies sample', sample, 'as class', y_predicted_class[sample])\n",
    "print('The true class of sample', sample, 'is class', y_test[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_test[sample_a], label='True:'+str(y_test[sample_a])+' Pred:'+str(y_predicted_class[sample_a]))\n",
    "plt.plot(x_test[sample_b], label='True:'+str(y_test[sample_b])+' Pred:'+str(y_predicted_class[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP 2\n",
    "This time we will create a function that builds our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    num = 16\n",
    "    x = Input(shape=(input_shape), name='MLP2InputLayer')\n",
    "    y = Dropout(0.1,name='Drop010')(x)\n",
    "    y = Dense(num, activation='relu', name='Dense010')(y)\n",
    "    y = Dropout(0.2,name='Drop020')(y)\n",
    "    y = Dense(num, activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(0.2,name='Drop030')(y)\n",
    "    y = Dense(num, activation='relu', name='Dense021')(y)\n",
    "    y = Dropout(0.3,name='Drop040')(y)\n",
    "    out = Dense(1, activation='sigmoid', name='OutputLayer')(y)\n",
    "\n",
    "    # Build model and compile the model\n",
    "    model = Model(x, out)\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "epochs = 50\n",
    "start = time.time()\n",
    "hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "end = time.time()\n",
    "log = pd.DataFrame(hist.history) \n",
    "print('Training complete in', round(end-start), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['loss', 'val_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log[['acc', 'val_acc']].plot()\n",
    "model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print('Validation accuracy is', result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "#model.summary()\n",
    "sample_a = 0 ### CHANGE PARAMETER HERE ###\n",
    "sample_b = 3 ### CHANGE PARAMETER HERE ###\n",
    "plt.plot(x_dev[sample_a], label='True:'+str(y_dev[sample_a]))\n",
    "plt.plot(x_dev[sample_b], label='True:'+str(y_dev[sample_b]))\n",
    "plt.legend(loc='upper right', frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "batch_size = 10\n",
    "epochs = 2\n",
    "kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=1, random_state=76)\n",
    "count = 0\n",
    "val_acc = list()\n",
    "for train, test in kfold.split(x_dev, y_dev):\n",
    "    x_train, y_train, x_test, y_test = x_dev[train], y_dev[train], x_dev[test], y_dev[test]\n",
    "    # Normalise the data\n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "    # Build and train a model\n",
    "    model = build_model()\n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), verbose=1)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    print('Training of fold', count, 'complete in', round(end-start), 'seconds')\n",
    "    val_acc.append(log.iloc[-1]['val_acc'])\n",
    "    count = count + 1\n",
    "\n",
    "val_acc = pd.DataFrame(val_acc, columns=['val_acc'])\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the k-fold cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=val_acc)\n",
    "ax = sns.swarmplot(data=val_acc, color='black')\n",
    "print('Validation accuracy mean and sample standard deviation', val_acc['val_acc'].mean(), val_acc['val_acc'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
