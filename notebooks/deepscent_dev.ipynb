{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepscent development\n",
    "Train and test the models used in developing Deepscent. The 'GunPoint' dataset from the [UEA & UCR Time Series \n",
    "Classification Repository](http://www.timeseriesclassification.com \n",
    "\"timeseriesclassification.com\") is used in place of the detection dogs' data.\n",
    "\n",
    "\n",
    "MIT license to use [software by Zhiguang Wang](https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline/blob/master/README.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, classification_report\n",
    "\n",
    "np.random.seed(999123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model with hyperparameters as per Wang et al. (2017): \n",
    "# MLP, FCN, ResNet\n",
    "# or\n",
    "# select a model with hyperparameters tuned to optimise performance on the ACI detection dogs dataset:\n",
    "# MLP_tuned, FCN_tuned, ResNet_tuned, CNN\n",
    "model_type = 'MLP_tuned' # MLP, MLP_tuned, FCN, FCN_tuned, CNN, ResNet, ResNet_tuned\n",
    "\n",
    "# Provide the dataset directory name. \n",
    "fname = 'GunPoint' # GunPoint\n",
    "\n",
    "batch_size = 32 \n",
    "k = 3 # k-fold cross validation: number of folds. If k=1, the original test-train split is used.\n",
    "m = 4 # k-fold cross validation: number of repetitions (if k>1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_dict = {'MLP':5000, 'FCN':2000, 'ResNet':1500, 'MLP_tuned':500, 'FCN_tuned':1000, 'CNN':1000, 'ResNet_tuned':1000}\n",
    "nb_epochs = epochs_dict[model_type]\n",
    "\n",
    "truncate_data = False # Truncate pressure samples to first n data points\n",
    "filter_data = False # Filter out noise below a threshold\n",
    "\n",
    "data_augmentation = False\n",
    "tensorboard = True # Set to True to write logs for use by TensorBoard\n",
    "k_fold_seed = 765432\n",
    "\n",
    "# Output directories\n",
    "logs_dir = '../logs'\n",
    "tensorboard_dir = '../logs/tensorboard'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\")))\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "tensorboard_dir = tensorboard_dir +'/' + timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Normalised confusion matrix', name=''):\n",
    "    ''' Plot the normalised confusion matrix\n",
    "    Parameters\n",
    "    cm : array - normalised confusion matrix\n",
    "    Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "    'Confusion Matrix' https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    '''\n",
    "    classes = ['Positive', 'Negative']\n",
    "    cmap=plt.cm.Blues\n",
    "    sns.set_style('dark')\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(format=FuncFormatter('{0:.0%}'.format))\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.clim(0, 1)\n",
    "    fmt = '.0%'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.tight_layout()\n",
    "    file_name = 'cm_deepscent_dev_'+name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "def plot_roc(y_true, y_probs, name): \n",
    "    ''' Plot ROC and return AUC\n",
    "    Parameters\n",
    "    y_true : vector of true class labels\n",
    "    y_probs : vector of predicted probabilities\n",
    "    Returns\n",
    "    auc : float\n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "    auc = roc_auc_score(y_true, y_probs)\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    file_name = 'roc_deepscent_dev_'+name+'.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    return auc\n",
    "\n",
    "\n",
    "def filter_out(x, threshold):\n",
    "    ''' Filter out any data points in x that are below the threshold by setting them to zero.\n",
    "    Return the modified data, x '''\n",
    "    if x < threshold:\n",
    "        return 0\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def preprocess(X):\n",
    "    ''' Apply preprocessing to the input data X'''\n",
    "    if filter_data:\n",
    "        threshold = 0.1\n",
    "        X = np.piecewise(X, [X < threshold, X >= threshold], [lambda X: 0, lambda X: X])\n",
    "    return X\n",
    "    \n",
    "    \n",
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : input data, one sample per row\n",
    "    Y : class labels corresponding to each row of X\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    if truncate_data:\n",
    "        X = X[:,:300]\n",
    "    X = preprocess(X)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def load_data_from_web(url, sep=' '):\n",
    "    ''' Load the data file from a url.\n",
    "    File format - UCR TSC Archive\n",
    "    i.e. space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : input data, one sample per row\n",
    "    Y : class labels corresponding to each row of X\n",
    "    '''\n",
    "    df = pd.read_csv(url, sep=sep, header=None)\n",
    "    print('Loaded data from', url)\n",
    "    Y = df.values[:,0].astype(int)\n",
    "    X = df.values[:,1:]\n",
    "    if truncate_data:\n",
    "        X = X[:,:300]\n",
    "    X = preprocess(X)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def reshape(x, model_type):\n",
    "    ''' Reshape data into input format for the selected DNN '''\n",
    "    if model_type == 'ResNet':\n",
    "        return reshape_2d(x)\n",
    "    elif model_type == 'FCN' or model_type == 'FCN_tuned' or model_type == 'CNN' or model_type == 'ResNet_tuned':\n",
    "        return reshape_1d(x)\n",
    "    elif model_type == 'MLP' or model_type == 'MLP_tuned':\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError('Unrecognised model type')\n",
    "    return x\n",
    "\n",
    "\n",
    "def augment_data(x, y):\n",
    "    ''' Return n times as many data samples, x, and labels, y. The augmented data is generated \n",
    "    by applying a shift to each row of x and appending these new rows to x '''\n",
    "    m = x.shape[1]\n",
    "    x_new = x\n",
    "    y_new = y\n",
    "    for shift in range(-50, 60, 10):\n",
    "        x_aug = np.zeros_like(x)\n",
    "        if shift < 0:\n",
    "            x_aug[:,:m+shift] = x[:,-shift:]\n",
    "        elif shift > 0:\n",
    "            x_aug[:,shift:] = x[:,:m-shift]\n",
    "        elif shift == 0:\n",
    "            continue\n",
    "        x_new = np.concatenate((x_new, x_aug), axis=0)\n",
    "        y_new = np.concatenate((y_new, y), axis=0)\n",
    "    return x_new, y_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_end_test = False    # For each fold, evaluate model on the end_test set too.\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    url_data_dir = 'https://raw.githubusercontent.com/Withington/deepscent/master/data'\n",
    "    root = url_data_dir+'/'+fname+'/'+fname\n",
    "    sep = ' ' if 'DetectionDogMockData' in fname else '  '\n",
    "    x_train, y_train = load_data_from_web(root+'_TRAIN.txt', sep) \n",
    "    x_test, y_test = load_data_from_web(root+'_TEST.txt', sep) \n",
    "    if 'DetectionDogMockData' in fname:\n",
    "        x_other, y_other= load_data_from_web(root+'_END_TEST.txt')\n",
    "        do_end_test = True\n",
    "else:   \n",
    "    if 'private' in fname:\n",
    "        fdir = '../data/private_data/private_events_dev2' \n",
    "    else:\n",
    "        fdir = '../data' \n",
    "    root = fdir+'/'+fname+'/'+fname\n",
    "    x_train, y_train = readucr(root+'_TRAIN.txt')\n",
    "    x_test, y_test = readucr(root+'_TEST.txt')\n",
    "    if 'correct_plus' in fname or 'DetectionDogMockData' in fname:\n",
    "        x_other, y_other = readucr(root+'_END_TEST.txt')\n",
    "        do_end_test = True\n",
    "        \n",
    "print('Data loaded from', root+'_...txt')\n",
    "print('Training dataset size:', x_train.shape[0])\n",
    "print('Test set size:', x_test.shape[0])\n",
    "if do_end_test:\n",
    "    print('Alternative test set size:', x_other.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build DNN\n",
    "Build a binary classifier. Model types: MLP, FCN, ResNet, CNN.\n",
    "## ResNet\n",
    "ResNet with hyperparameters as per Wang et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_2d(x):\n",
    "    ''' Reshape data into input format for ResNet '''\n",
    "    x = x.reshape(x.shape + (1,1,))\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_resnet(input_shape, nb_classes):\n",
    "    ''' Build ResNet DNN and return input and output tensors '''\n",
    "    # Parameters\n",
    "    k = 1 # kernel multiplier\n",
    "    n_feature_maps = 128\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = keras.layers.BatchNormalization()(x)\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps, 8*k, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps, 1*k, 1,padding='same')(x)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8*k, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "         \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "     \n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1*k, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    print ('build conv_x')\n",
    "    x1 = y\n",
    "    conv_x = keras.layers.Conv2D(n_feature_maps*2, 8*k, 1, padding='same')(x1)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv2D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv2D(n_feature_maps*2, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "\n",
    "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
    "    if is_expand_channels:\n",
    "        shortcut_y = keras.layers.Conv2D(n_feature_maps*2, 1*k, 1,padding='same')(x1)\n",
    "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
    "    else:\n",
    "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
    "    print ('Merging skip connection')\n",
    "    y = keras.layers.add([shortcut_y, conv_z])\n",
    "    y = Activation('relu')(y)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling2D()(y)   \n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    print ('        -- model was built.')\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet tuned\n",
    "ResNet with hyperparameters tuned to optimise performance on the ACI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_resnet_tuned(input_shape):\n",
    "    ''' Return ResNet model '''  \n",
    "    # Hyperparameters\n",
    "    num_features0 = 64\n",
    "    num_features1 = 128\n",
    "    filter_size = 4\n",
    "    pooling_size = 8\n",
    "    dropout = 0.5  \n",
    "    \n",
    "    # Preparation block\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(x)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = keras.layers.MaxPooling1D(pooling_size)(conv)\n",
    "    \n",
    "    # First block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Second block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Third block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Output block\n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv)\n",
    "    y = Dropout(dropout, name='Dropout')(full)\n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN\n",
    "With hyperparameters as per Wang et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_1d(x):\n",
    "    ''' Reshape data into input format for FCN or CNN'''\n",
    "    x = x.reshape(x.shape + (1,))\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def build_fcn(input_shape, nb_classes):\n",
    "    ''' Build Fully Convolutional Network (FCN) and return input and output tensors '''\n",
    "    # Parameters\n",
    "    k = 1 # kernel multiplier\n",
    "    n_feature_maps = 128\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    #conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_feature_maps, 8*k, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5*k, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(n_feature_maps, 3*k, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    conv_z = Activation('relu')(conv_z)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv_z)\n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCN tuned\n",
    "With hyperparameters tuned to optimise performance on the ACI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fcn_tuned(input_shape, nb_classes):\n",
    "    ''' Build Fully Convolutional Network (FCN) and return input and output tensors '''\n",
    "    # Parameters\n",
    "    feature_maps_a = 32\n",
    "    feature_maps_b = 64\n",
    "    feature_maps_c = 32\n",
    "    filter_a = 4\n",
    "    filter_b = 4\n",
    "    filter_c = 4\n",
    "    \n",
    "    print ('build conv_x')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    conv_x = keras.layers.Conv1D(feature_maps_a, filter_a, 1, padding='same')(conv_x)\n",
    "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "    conv_x = Activation('relu')(conv_x)\n",
    "     \n",
    "    print ('build conv_y')\n",
    "    conv_y = keras.layers.Conv1D(feature_maps_b, filter_b, 1, padding='same')(conv_x)\n",
    "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
    "    conv_y = Activation('relu')(conv_y)\n",
    "     \n",
    "    print ('build conv_z')\n",
    "    conv_z = keras.layers.Conv1D(feature_maps_c, filter_c, 1, padding='same')(conv_y)\n",
    "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
    "    conv_z = Activation('relu')(conv_z)\n",
    "     \n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv_z)\n",
    "    out = Dense(1, activation='sigmoid')(full)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "With hyperparameters tuned to optimise performance on the ACI dataset.\n",
    "\n",
    "Using the CNN architecture of\n",
    "[Ackermann, Nils, 2018, Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_harus(input_shape, nb_classes):\n",
    "    ''' Build a CNN and return input and output tensors '''\n",
    "    # Parameters\n",
    "    n_features_a = 128 # Ackermann 100 \n",
    "    n_features_b = 128 # Ackermann 160\n",
    "    filter_size = 16   # Ackermann 10\n",
    "    pooling_size = 16   # Ackermann 3\n",
    "    dropout = 0.7      # Ackermann 0.5\n",
    "    has_dense_layer = True\n",
    "    \n",
    "    print ('build CNN HARUS')\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv_x = x\n",
    "    conv_x = keras.layers.Conv1D(n_features_a, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_a, filter_size, activation='relu')(conv_x)\n",
    "    conv_x = keras.layers.MaxPooling1D(pooling_size)(conv_x)\n",
    "    conv_x = keras.layers.Conv1D(n_features_b, filter_size, activation='relu')(conv_x)\n",
    "    \n",
    "    if True:\n",
    "        conv_x = keras.layers.Conv1D(n_features_b, filter_size, activation='relu')(conv_x)\n",
    "    else:\n",
    "        conv_x = keras.layers.Conv1D(n_features_b, filter_size)(conv_x)\n",
    "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
    "        conv_x = Activation('relu')(conv_x)\n",
    "        \n",
    "    if has_dense_layer: # End with a fully connected layer\n",
    "        full = keras.layers.GlobalAveragePooling1D()(conv_x)\n",
    "        full = Dropout(dropout,name='Dropout')(full)\n",
    "        out = Dense(1, activation='sigmoid')(full)\n",
    "    else:\n",
    "        conv_x = keras.layers.Conv1D(16, filter_size, activation='relu')(conv_x)\n",
    "        conv_x = Dropout(dropout,name='Dropout')(conv_x)\n",
    "        conv_x = keras.layers.Conv1D(1, filter_size, activation='relu')(conv_x)\n",
    "        conv_x = keras.layers.GlobalAveragePooling1D()(conv_x)\n",
    "        out = Activation(activation='sigmoid')(conv_x)\n",
    "    return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "With hyperparameters as per Wang et al. (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, nb_classes):\n",
    "    num = 500\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dropout(0.1,name='WDrop010')(x)\n",
    "    y = Dense(num, activation='relu', name='WDense010')(y)\n",
    "    y = Dropout(0.2,name='WDrop020')(y)\n",
    "    y = Dense(num, activation='relu', name='WDense020')(y)\n",
    "    y = Dropout(0.2,name='WDrop021')(y)\n",
    "    y = Dense(num, activation='relu', name='WDense021')(y)\n",
    "    y = Dropout(0.3,name='WDrop031')(y)\n",
    "    out = Dense(1, activation='sigmoid', name='WDense080')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP tuned\n",
    "With hyperparameters tuned to optimise performance on the ACI dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_tuned(input_shape, nb_classes):\n",
    "    ''' Build a Multilayer Perceptron (MLP) and return input and output tensors '''\n",
    "    drop = 0.2\n",
    "    num = 16\n",
    "    l2 = 0.1\n",
    "    x = Input(shape=(input_shape))\n",
    "    y = Dropout(drop,name='DropInput')(x)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense010')(y)\n",
    "    y = Dropout(drop,name='Drop010')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense020')(y)\n",
    "    y = Dropout(drop,name='Drop020')(y)\n",
    "    y = Dense(num, kernel_regularizer=regularizers.l2(l2), activation='relu', name='Dense030')(y)\n",
    "    y = Dropout(drop,name='Drop030')(y)\n",
    "    out = Dense(1, activation='sigmoid', name='DenseOutput')(y)\n",
    "    return x, out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function: train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(fname, x_train, y_train, x_test, y_test, label=\"0\"):\n",
    "    ''' Build and train a DNN. Return summary info and a trained model '''\n",
    "    print('Running dataset', fname)\n",
    "    nb_classes = len(np.unique(y_test))\n",
    "    if nb_classes != 2:\n",
    "        raise 'Number of classes must be 2 to use this binary classifier'\n",
    "    \n",
    "    if data_augmentation:\n",
    "        x_train, y_train = augment_data(x_train, y_train)\n",
    "     \n",
    "    Y_train = (y_train - y_train.min())/(y_train.max()-y_train.min())*(nb_classes-1)\n",
    "    Y_test = (y_test - y_test.min())/(y_test.max()-y_test.min())*(nb_classes-1)\n",
    "     \n",
    "    x_train_mean = x_train.mean()\n",
    "    x_train_std = x_train.std()\n",
    "    x_train = (x_train - x_train_mean)/(x_train_std) \n",
    "    x_test = (x_test - x_train_mean)/(x_train_std)\n",
    "     \n",
    "    x_train = reshape(x_train, model_type)\n",
    "    x_test = reshape(x_test, model_type)\n",
    "    if model_type == 'MLP':\n",
    "        x, y = build_mlp(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'MLP_tuned':\n",
    "        x, y = build_mlp_tuned(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'ResNet':\n",
    "            x, y = build_resnet(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'ResNet_tuned':\n",
    "        x, y = build_resnet_tuned(x_train.shape[1:])\n",
    "    elif model_type == 'FCN':\n",
    "        x, y = build_fcn(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'FCN_tuned':\n",
    "        x, y = build_fcn_tuned(x_train.shape[1:], nb_classes)\n",
    "    elif model_type == 'CNN':\n",
    "        x, y = build_cnn_harus(x_train.shape[1:], nb_classes)\n",
    "    model = Model(x, y)\n",
    "    print(model.summary())\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True) \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,\n",
    "                      patience=50, min_lr=0.0001) \n",
    "    callbacks = [reduce_lr]\n",
    "    if tensorboard:\n",
    "        tb_dir = tensorboard_dir+'/'+fname+'_'+label\n",
    "        Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "        print('Tensorboard logs in', tb_dir)\n",
    "        callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "  \n",
    "    start = time.time()\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "              verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "    end = time.time()\n",
    "    log = pd.DataFrame(hist.history) \n",
    "    \n",
    "    # Print results\n",
    "    duration_seconds = round(end-start)\n",
    "    duration_minutes = str(round((end-start)/60))\n",
    "    print('Training complete on', fname, 'Duration:', duration_seconds, 'secs; about', duration_minutes, 'minutes.')\n",
    "    \n",
    "    # Print and save results. Print the testing results that have the lowest training loss.\n",
    "    print('Selected the test result with the lowest training loss. Loss and validation accuracy are -')\n",
    "    idx = log['loss'].idxmin()\n",
    "    loss = log.loc[idx]['loss']\n",
    "    val_acc = log.loc[idx]['val_acc']\n",
    "    epoch = idx + 1\n",
    "    print(loss, val_acc, 'at index', str(idx), ' (epoch ', str(epoch), ')')\n",
    "    summary = '|' + label + '  |'+str(loss)+'  |'+str(val_acc)+' |'+str(epoch)+' |'+ duration_minutes + 'mins  |'\n",
    "    summary_csv = label+','+str(loss)+','+str(val_acc)+','+str(epoch)+','+ duration_minutes \n",
    "    \n",
    "    # Save summary file and log file.\n",
    "    print('Tensorboard logs in', tb_dir)\n",
    "    history_file = logs_dir+'/'+fname+'/history_'+label+'.csv'\n",
    "    print('Saving logs to', history_file)\n",
    "    log.to_csv(history_file)\n",
    "    \n",
    "    model_params = {'x_train_mean':x_train_mean, 'x_train_std':x_train_std, 'threshold':0.5}\n",
    "    return summary, summary_csv, model, model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Train a model, using repeated k-fold cross validation, if selected '''\n",
    "\n",
    "results_file = logs_dir+'/'+fname+'/deepscent_dev_summary.csv'\n",
    "results = []\n",
    "\n",
    "if k > 1: # k-fold cross validation\n",
    "    x_all = np.concatenate((x_train, x_test), axis=0)\n",
    "    y_all = np.concatenate((y_train, y_test), axis=0)\n",
    "    kfold = RepeatedStratifiedKFold(n_splits=k, n_repeats=m, random_state=k_fold_seed)\n",
    "    count = 0\n",
    "    for train, test in kfold.split(x_all, y_all):\n",
    "        x_train, y_train, x_test, y_test = x_all[train], y_all[train], x_all[test], y_all[test]\n",
    "        summary, summary_csv, model, model_params = train_model(fname, x_train, y_train, x_test, y_test, str(count))\n",
    "        if do_end_test:\n",
    "            x_in = (x_other - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "            x_in = reshape(x_in, model_type)\n",
    "            _, end_test_acc = model.evaluate(x_in, y_other, batch_size=batch_size)\n",
    "            summary = summary + str(end_test_acc) +' |'\n",
    "            summary_csv = summary_csv + ',' + str(end_test_acc)\n",
    "        with open(results_file, 'a+') as f:\n",
    "            f.write(summary_csv)\n",
    "            f.write('\\n')\n",
    "            print('Added summary row to ', results_file)\n",
    "        results.append(summary)\n",
    "        count = count + 1\n",
    "else:\n",
    "    summary, summary_csv, model, model_params = train_model(fname, x_train, y_train, x_test, y_test)\n",
    "    if do_end_test:\n",
    "        x_in = (x_other - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "        x_in = reshape(x_in, model_type)\n",
    "        _, end_test_acc = model.evaluate(x_in, y_other, batch_size=batch_size)\n",
    "        summary = summary + str(end_test_acc) +' |'\n",
    "        summary_csv = summary_csv + ',' + str(end_test_acc)\n",
    "    with open(results_file, 'a+') as f:\n",
    "        f.write(summary_csv)\n",
    "        f.write('\\n')\n",
    "        print('Added summary row to ', results_file)\n",
    "    results.append(summary)\n",
    "        \n",
    "print('DONE')\n",
    "print(fname, timestamp)\n",
    "print('train:test', y_train.shape[0], y_test.shape[0])\n",
    "for each in results:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print when done\n",
    "print('Done at:' , '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "file = results_file\n",
    "if do_end_test:\n",
    "    data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time', 'end_test_acc'])\n",
    "    all_data = [data['val_acc'], data['end_test_acc']]\n",
    "    all_names = ['Validation test set', 'Alternative test set']\n",
    "else:\n",
    "    data = pd.read_csv(file, header=None, names=['run','loss','val_acc','epoch','time'])\n",
    "    all_data = [data['val_acc']]\n",
    "    all_names = ['Validation test set']\n",
    "    \n",
    "# Box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(data=all_data)\n",
    "ax = sns.swarmplot(data=all_data, color='black')\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.suptitle('K-fold cross validation results')\n",
    "ax.yaxis.set_major_formatter(FuncFormatter('{0:.0%}'.format))\n",
    "plt.xticks(np.arange(len(all_names)), all_names)\n",
    "\n",
    "# Print results\n",
    "print('Training dataset size:', x_train.shape[0])\n",
    "print('Test set size:', x_test.shape[0])\n",
    "if do_end_test:\n",
    "    print('Alternative test set size:', x_other.shape[0])\n",
    "accuracy = data['val_acc']\n",
    "print(file, '\\n')\n",
    "print(data)\n",
    "print('\\nValidation accuracy mean average:', data['val_acc'].mean())\n",
    "print('Validation accuracy sample standard deviation:', data['val_acc'].std())\n",
    "if do_end_test:\n",
    "    print('Alternative test results:')\n",
    "    print('Accuracy mean average:', data['end_test_acc'].mean())\n",
    "    print('Accuracy sample standard deviation:', data['end_test_acc'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, model_params, model_type, \n",
    "                x_input, y_input, name):\n",
    "    ''' Use the model to make predictions on x_input data. Plot the confusion matrix and ROC. '''    \n",
    "    y_input = y_input - y_input.min()\n",
    "    x_input = (x_input - model_params['x_train_mean'])/(model_params['x_train_std'])\n",
    "    x_input = reshape(x_input, model_type)\n",
    "    # Class balance\n",
    "    n0 = (y_input == 0).sum()\n",
    "    n1 = (y_input == 1).sum()\n",
    "    \n",
    "    # Calculate model prediction\n",
    "    threshold = model_params['threshold']\n",
    "    y_probs = model.predict_on_batch(x_input)\n",
    "    if threshold == 0.5:\n",
    "        y_pred = np.round(y_probs).flatten()\n",
    "    else:\n",
    "        y_pred = y_probs.flatten()\n",
    "        y_pred[y_pred > threshold] = 1\n",
    "        y_pred[y_pred <= threshold] = 0\n",
    "        \n",
    "    cm = confusion_matrix(y_input, y_pred, labels=[1,0])\n",
    "    acc_calc = (cm[0][0]+cm[1][1])/(cm.sum())\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print('Operating point:',threshold)   \n",
    "    print('Accuracy:',acc_calc)\n",
    "    print('Class balance in test set:', n0/(n0+n1))\n",
    "    title = 'Normalised confusion matrix'\n",
    "    plot_confusion_matrix(cm_norm, title=title, name=name)\n",
    "\n",
    "    # ROC and AUC\n",
    "    auc = plot_roc(y_input, y_probs, name=name)\n",
    "    print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Make predictions using the last model that was trained '''\n",
    "operating_point = 0.5\n",
    "model_params['threshold'] = operating_point\n",
    "if do_end_test:\n",
    "    print('Predictions on the alternative test set')\n",
    "    x_in = x_other\n",
    "    y_in = y_other\n",
    "else:\n",
    "    x_in = x_test\n",
    "    y_in = y_test\n",
    "predictions(model, model_params, model_type, x_in, y_in, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfile = logs_dir+'/'+fname+'/model'\n",
    "model_json = model.to_json()\n",
    "with open(modelfile+'.json', 'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "# Save the model's weights\n",
    "model.save_weights(modelfile+'.h5')\n",
    "print('Model saved to', modelfile)\n",
    "# Save the other model parameters (mean and std dev of training data)\n",
    "model_params          \n",
    "with open(logs_dir+'/'+fname+'/model_params.csv', 'a+') as f:\n",
    "    for key in model_params.keys():\n",
    "        f.write(\"%s,%s\\n\"%(key,model_params[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License to use Zhiguang Wang's software\n",
    "\n",
    "UCR Time Series Classification Deep Learning Baseline \n",
    "\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) [2019] [Zhiguang Wang]\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### Reference\n",
    "Wang, Z., Yan, W. and Oates, T. (2017) ‘Time series classification from scratch with deep neural networks: A strong baseline’, 2017 International Joint Conference on Neural Networks (IJCNN), pp. 1578–1585 Online. Available at https://arxiv.org/abs/1611.06455."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
