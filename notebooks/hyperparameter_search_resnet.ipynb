{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter grid search\n",
    "NB the input data to the DNN is not normalised.\n",
    "\n",
    "Hyperparameter grid search adapted from Machine Learning Mastery\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "Using scikit-learn to grid search the batch size and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dateutil.tz import gettz\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.random.seed(757566)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'private_dog0_correct_plus' # private_dog0_correct_plus\n",
    "\n",
    "logs_dir = '../logs'\n",
    "timestamp = '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\")))\n",
    "logs_dir = logs_dir +'/' + timestamp\n",
    "if 'private' in fname:\n",
    "    fdir = '../data/private_data/private_events_dev2' \n",
    "else:\n",
    "    fdir = '../data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readucr(filename):\n",
    "    ''' Load a dataset from a file in UCR format\n",
    "    space delimited, class labels in the first column.\n",
    "    Returns\n",
    "    X : DNN input data\n",
    "    Y : class labels\n",
    "    '''\n",
    "    data = np.loadtxt(Path(filename))\n",
    "    Y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def prepare_data(y):\n",
    "    ''' Return y as a categorical array'''\n",
    "    nb_classes = 2\n",
    "    y = (y - y.min())/(y.max()-y.min())*(nb_classes-1)\n",
    "    Y = utils.to_categorical(y, nb_classes)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet(input_shape=(0,0), num_features0=-1, num_features1=-1, filter_size=-1, pooling_size=-1, dropout=-1):\n",
    "    ''' Return ResNet model '''\n",
    "    nb_classes = 2\n",
    "    print(input_shape, num_features0, num_features1, filter_size, pooling_size, dropout)\n",
    "    \n",
    "    # Preparation block\n",
    "    x = Input(shape=(input_shape))\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(x)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = keras.layers.MaxPooling1D(pooling_size)(conv)\n",
    "    \n",
    "    # First block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Second block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Third block\n",
    "    skip = conv\n",
    "    conv = keras.layers.Conv1D(num_features0*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features0*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    conv = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(conv)\n",
    "    conv = keras.layers.BatchNormalization()(conv)\n",
    "    shortcut = keras.layers.Conv1D(num_features1*2, filter_size, padding='same')(skip)\n",
    "    shortcut = keras.layers.BatchNormalization()(shortcut)\n",
    "    conv = keras.layers.add([conv, shortcut])\n",
    "    conv = Activation('relu')(conv)\n",
    "    \n",
    "    # Output block\n",
    "    full = keras.layers.GlobalAveragePooling1D()(conv)\n",
    "    y = Dropout(dropout, name='Dropout')(full)\n",
    "    out = Dense(nb_classes, activation='sigmoid')(full)\n",
    "    model = Model(x, out)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    print('Model compiled')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "x_train, y_train = readucr(fdir+'/'+fname+'/'+fname+'_TRAIN.txt')\n",
    "x_test, y_test = readucr(fdir+'/'+fname+'/'+fname+'_TEST.txt')\n",
    "X = np.concatenate((x_train, x_test), axis=0)\n",
    "Y = np.concatenate((y_train, y_test), axis=0)\n",
    "X = X.reshape(X.shape + (1,))\n",
    "input_shape = X.shape[1:]\n",
    "print(input_shape)\n",
    "Y = prepare_data(Y)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "num_features0 = [64, 128, 256]\n",
    "num_features1 = [64, 128, 256]\n",
    "filter_size = [2, 8, 32]\n",
    "pooling_size = [4, 8, 16]\n",
    "dropout = [0.2, 0.5]\n",
    "param_grid = dict(num_features0=num_features0, num_features1=num_features1, \n",
    "                  filter_size=filter_size, pooling_size=pooling_size, \n",
    "                  dropout=dropout,)\n",
    "\n",
    "# Create model and run the grid search\n",
    "if True:\n",
    "    model = KerasClassifier(build_fn=create_resnet, \n",
    "                            input_shape=input_shape,\n",
    "                            batch_size=batch_size, epochs=epochs,\n",
    "                            verbose=1)\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, error_score=0, return_train_score=True)\n",
    "    grid_result = grid.fit(X, Y)\n",
    "\n",
    "    # Summarise results\n",
    "    print('Best score:', grid_result.best_score_, 'using: ', grid_result.best_params_)\n",
    "    cv = pd.DataFrame(grid_result.cv_results_)\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    cv[['mean_test_score', 'std_test_score', 'params']]\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape + (1,))\n",
    "    x_test = x_test.reshape(x_test.shape + (1,))\n",
    "    Y_train = prepare_data(y_train)\n",
    "    Y_test = prepare_data(y_test)\n",
    "\n",
    "    callbacks = []\n",
    "    tensorboard_dir = '../logs/tensorboard'\n",
    "    tensorboard_dir = tensorboard_dir +'/' + timestamp\n",
    "    tb_dir = tensorboard_dir+'/'+fname\n",
    "    Path(tb_dir).mkdir(parents=True, exist_ok=True) \n",
    "    callbacks.append(keras.callbacks.TensorBoard(log_dir=tb_dir, histogram_freq=0))\n",
    "    model = create_resnet(input_shape, num_features0[0], num_features1, filter_size, pooling_size, dropout)\n",
    "    hist = model.fit(x_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                      verbose=1, validation_data=(x_test, Y_test), callbacks=callbacks)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Completed at', '{:%Y-%m-%dT%H:%M}'.format(datetime.now(gettz(\"Europe/London\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(logs_dir+'/'+fname).mkdir(parents=True, exist_ok=True)\n",
    "filename = Path(logs_dir+'/'+fname+'/resnet_grid_search_summary.csv')\n",
    "cv.to_csv(filename)\n",
    "print('Results saved to', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
